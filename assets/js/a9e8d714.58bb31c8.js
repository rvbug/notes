"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[3454],{4547:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var s=n(8355),i=n(4848),r=n(8453);const o={slug:"LLMs",title:"LLM Overview",authors:["rakesh"],tags:["ML"],side_position:1},a=void 0,l={authorsImageUrls:[void 0]},c=[{value:"Large Language Models",id:"large-language-models",level:2},{value:"Background",id:"background",level:2},{value:"Impact",id:"impact",level:2},{value:"My Motivation",id:"my-motivation",level:2},{value:"LLM Basics",id:"llm-basics",level:2},{value:"LLM Stages",id:"llm-stages",level:3},{value:"Simple Transformers and Attention",id:"simple-transformers-and-attention",level:3},{value:"Autoregression Models",id:"autoregression-models",level:3},{value:"DeepSeek&#39;s Archictecure",id:"deepseeks-archictecure",level:2},{value:"References",id:"references",level:2}];function h(e){const t={a:"a",blockquote:"blockquote",br:"br",em:"em",h1:"h1",h2:"h2",h3:"h3",hr:"hr",img:"img",p:"p",strong:"strong",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h2,{id:"large-language-models",children:"Large Language Models"}),"\n",(0,i.jsx)(t.p,{children:"OpenAI has been at the forefront of developing sophisticated LLMs, often perceived as black boxes. Until DeepSeek's open-source release, that is. Now, we have a unique opportunity to peek behind the curtain. In this series of articles, I will delve into the inner workings of DeepSeek's LLMs, starting with the basics before moving on to more advanced topics, covering their architecture and optimization techniques. Here are some of my notes exploring their research papers."}),"\n",(0,i.jsx)(t.hr,{}),"\n",(0,i.jsx)(t.h2,{id:"background",children:"Background"}),"\n",(0,i.jsx)(t.p,{children:"DeepSeek has emerged as a significant disruptor in the AI industry, particularly in the realm of large language models (LLMs). It gained recognition for achieving high-level AI performance while utilizing significantly fewer computational resources compared to industry giants like OpenAI."}),"\n",(0,i.jsx)(t.p,{children:'DeepSeek leverages techniques like "Mixture of Experts (MoE)"  to optimize resource allocation,  innovations like Multi-Head Latent Attention (MLA) etc among many others. But the most important breakthrough was their commitment to open-source principles contributing to its rapid growth and influence.'}),"\n",(0,i.jsx)(t.h2,{id:"impact",children:"Impact"}),"\n",(0,i.jsx)(t.p,{children:"DeepSeek's success has put pressure on established AI companies to reconsider their development strategies and by democratizing their AI technology, it has made these models more accessible to smaller organizations and developing nations."}),"\n",(0,i.jsx)(t.p,{children:"They developed a model much more powerful (or similar) than OpenAI with significantly less resource. DeepSeek's rise signifies a shift towards more efficient and accessible AI, challenging the dominance of OpenAI."}),"\n",(0,i.jsx)(t.h2,{id:"my-motivation",children:"My Motivation"}),"\n",(0,i.jsx)(t.p,{children:"DeepSeek's achievement, building a powerful model with optimized resources, resonates deeply. It's a testament to human ingenuity, a reminder that constraints often spark the most innovative solutions. When faced with limitations, we have an incredible ability to find efficient, impactful pathways forward."}),"\n",(0,i.jsxs)(t.blockquote,{children:["\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.em,{children:"We've always defined ourselves by the ability to overcome the impossible. And we count these moments. These moments when we dare to aim higher, to break barriers, to reach for the stars, to make the unknown known. We count these moments as our proudest achievements. But we lost all that. Or perhaps we've just forgotten that we are still pioneers. And we've barely begun. And that our greatest accomplishments cannot be behind us, because our destiny lies above us."}),"\n",(0,i.jsx)(t.strong,{children:"- Cooper, (Movie - Interstellar)"})]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"llm-basics",children:"LLM Basics"}),"\n",(0,i.jsx)(t.p,{children:"Here's the general overview of LLMs"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"Journey",src:n(6071).A+"",width:"2136",height:"1374"})}),"\n",(0,i.jsx)(t.h3,{id:"llm-stages",children:"LLM Stages"}),"\n",(0,i.jsx)(t.p,{children:"It is important for us to understand the LLM stages, I will go through each one in detail."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"alt text",src:n(3190).A+"",width:"2176",height:"801"})}),"\n",(0,i.jsx)(t.h3,{id:"simple-transformers-and-attention",children:"Simple Transformers and Attention"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"alt text",src:n(5278).A+"",width:"1744",height:"1274"})}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"alt text",src:n(9160).A+"",width:"2018",height:"654"})}),"\n",(0,i.jsx)(t.h3,{id:"autoregression-models",children:"Autoregression Models"}),"\n",(0,i.jsx)(t.p,{children:"The pretraining stage uses autoregressive model. The output of the model is fed as the input to the model. More on this later."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"alt text",src:n(8719).A+"",width:"1587",height:"721"})}),"\n",(0,i.jsx)(t.h2,{id:"deepseeks-archictecure",children:"DeepSeek's Archictecure"}),"\n",(0,i.jsx)(t.p,{children:"Here are the some of the components used by DeepSeek"}),"\n",(0,i.jsxs)(t.blockquote,{children:["\n",(0,i.jsx)(t.p,{children:"Multi-Head Latent Attention"}),"\n"]}),"\n",(0,i.jsxs)(t.blockquote,{children:["\n",(0,i.jsx)(t.p,{children:"Mixture of Experts (MoE)"}),"\n"]}),"\n",(0,i.jsxs)(t.blockquote,{children:["\n",(0,i.jsx)(t.p,{children:"Multi-Token Prediction"}),"\n"]}),"\n",(0,i.jsxs)(t.blockquote,{children:["\n",(0,i.jsx)(t.p,{children:"Reinforcement Learning using GRPO"}),"\n"]}),"\n",(0,i.jsxs)(t.blockquote,{children:["\n",(0,i.jsx)(t.p,{children:"GPU Optimization via PTX"}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"Overview",src:n(441).A+"",width:"1374",height:"474"})}),"\n",(0,i.jsx)(t.h1,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsxs)(t.p,{children:["This blog series ",(0,i.jsx)(t.strong,{children:"(converted from my research notes)"})," will provide a glimpse into the architecture and optimization of foundational LLMs, culminating in a deeper understanding of DeepSeek's architecture, equipping you with a solid foundation to embark on your own LLM research. The open sourcing of LLMs, and DeepSeek in particular, is a major step in the democratization of AI. The rapid evolution of these models promises to reshape how we interact with technology."]}),"\n",(0,i.jsx)(t.p,{children:"Enjoy your reading!"}),"\n",(0,i.jsx)(t.h2,{id:"references",children:"References"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.a,{href:"https://arxiv.org/pdf/2501.12948",children:"Reinforcement Learning"}),(0,i.jsx)(t.br,{}),"\n",(0,i.jsx)(t.a,{href:"https://github.com/rvbug/NLP",children:"Roadmap for LLMs"})]})]})}function d(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},8719:(e,t,n)=>{n.d(t,{A:()=>s});const s=n.p+"assets/images/AutoregressiveModel-f0dbb84cb046fff007f57d2e6244cc46.png"},3190:(e,t,n)=>{n.d(t,{A:()=>s});const s=n.p+"assets/images/LLMStages-06471d6394274a0d19817631655f824d.png"},441:(e,t,n)=>{n.d(t,{A:()=>s});const s=n.p+"assets/images/Llmdetails-e30fa8dc772175b7f06236e7c803e567.png"},6071:(e,t,n)=>{n.d(t,{A:()=>s});const s=n.p+"assets/images/Llmjourney-1bc92415fe64fc2bce5bb6336923aae2.png"},9160:(e,t,n)=>{n.d(t,{A:()=>s});const s=n.p+"assets/images/selfAttention -cc2f12195e657760a022455bbee60e92.png"},5278:(e,t,n)=>{n.d(t,{A:()=>s});const s=n.p+"assets/images/simpleTransormers-1174510bc910d964e168a971afec2da6.png"},8453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>a});var s=n(6540);const i={},r=s.createContext(i);function o(e){const t=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(r.Provider,{value:t},e.children)}},8355:e=>{e.exports=JSON.parse('{"permalink":"/notes/blog/LLMs","source":"@site/blog/2025-03-01/0-Introduction.md","title":"LLM Overview","description":"Large Language Models","date":"2025-03-01T00:00:00.000Z","tags":[{"inline":false,"label":"ML","permalink":"/notes/blog/tags/ML","description":"Machine Learning"}],"readingTime":2.76,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"LLMs","title":"LLM Overview","authors":["rakesh"],"tags":["ML"],"side_position":1},"unlisted":false,"nextItem":{"title":"Graphics Processing Unit (GPU)","permalink":"/notes/blog/GPU"}}')}}]);