"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[5766],{3574:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>o,toc:()=>l});var o=n(9256),i=n(4848),s=n(8453);const r={slug:"Tokenizer",title:"Input Block - Tokenization",description:"What is Tokenizer",authors:["rakesh"],tags:["ML","ML Research"]},a="Introduction",c={authorsImageUrls:[void 0]},l=[];function u(e){const t={p:"p",...(0,s.R)(),...e.components};return(0,i.jsx)(t.p,{children:"In building LLMs, the first block is called input block. In this step, input text passed through the Tokenizer to create a tokenized text. This Token IDs are then passed through Embedding Layer and positional encoding is added before sending it to the transformer block."})}function p(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(u,{...e})}):u(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>a});var o=n(6540);const i={},s=o.createContext(i);function r(e){const t=o.useContext(s);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),o.createElement(s.Provider,{value:t},e.children)}},9256:e=>{e.exports=JSON.parse('{"permalink":"/notes/blog/Tokenizer","source":"@site/blog/2025-03-01/LLMs/4.Tokenization.md","title":"Input Block - Tokenization","description":"What is Tokenizer","date":"2025-03-01T00:00:00.000Z","tags":[{"inline":false,"label":"ML","permalink":"/notes/blog/tags/ML","description":"Machine Learning"},{"inline":false,"label":"ML Reserach","permalink":"/notes/blog/tags/ML-Research","description":"Machine Learning Research"}],"readingTime":2.43,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"Tokenizer","title":"Input Block - Tokenization","description":"What is Tokenizer","authors":["rakesh"],"tags":["ML","ML Research"]},"unlisted":false,"prevItem":{"title":"PTX Optimization","permalink":"/notes/blog/DeepSeekPTX"},"nextItem":{"title":"Transformer Block - Multihead Attention","permalink":"/notes/blog/transformers"}}')}}]);