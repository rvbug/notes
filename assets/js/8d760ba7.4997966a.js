"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[6454],{1034:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>g,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var o=n(722),a=n(4848),r=n(8453);const s={slug:"transformers",title:"Transformer Block - Multihead Attention",description:"Road to MHLA",image:"img/LLMWorkflow.png",authors:["rakesh"],tags:["ML","LLMs","ML Research"]},i="Introduction",l={authorsImageUrls:[void 0]},c=[];function u(e){const t={p:"p",strong:"strong",...(0,r.R)(),...e.components};return(0,a.jsxs)(t.p,{children:["In my previous blog post, I introduced Deepseek LLM's innovative parallel thread execution (PTX) mechanism and how they use it for GPU optimization. Today, we'll cover another foundational topic - ",(0,a.jsx)(t.strong,{children:"Multihead Attention (MHA)"})," before diving into Deepseek's second groundbreaking innovation known as ",(0,a.jsx)(t.strong,{children:"Multihead Latent Attention (MHLA)"}),"."]})}function g(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(u,{...e})}):u(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>i});var o=n(6540);const a={},r=o.createContext(a);function s(e){const t=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),o.createElement(r.Provider,{value:t},e.children)}},722:e=>{e.exports=JSON.parse('{"permalink":"/notes/blog/transformers","source":"@site/blog/2025-03-01/LLMs/5.MHLA.md","title":"Transformer Block - Multihead Attention","description":"Road to MHLA","date":"2025-03-01T00:00:00.000Z","tags":[{"inline":false,"label":"ML","permalink":"/notes/blog/tags/ML","description":"Machine Learning"},{"inline":false,"label":"LLM","permalink":"/notes/blog/tags/LLM","description":"Large Language Models"},{"inline":false,"label":"ML Reserach","permalink":"/notes/blog/tags/ML-Research","description":"Machine Learning Research"}],"readingTime":8.655,"hasTruncateMarker":true,"authors":[{"name":"Rakesh","title":"C | Rust | Quantum Gravity | LLM Researcher","description":"I am a researcher in the field of quantum gravity and LLMs. I have a strong interest in C and Rust programming languages.\\n","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"transformers","title":"Transformer Block - Multihead Attention","description":"Road to MHLA","image":"img/LLMWorkflow.png","authors":["rakesh"],"tags":["ML","LLMs","ML Research"]},"unlisted":false,"prevItem":{"title":"Input Block - Tokenization","permalink":"/notes/blog/Tokenizer"},"nextItem":{"title":"Dotfiles","permalink":"/notes/blog/dotfiles"}}')}}]);