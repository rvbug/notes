"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[4145],{9922:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"LLMs","metadata":{"permalink":"/notes/blog/LLMs","source":"@site/blog/2025-03-01/0-Introduction.md","title":"LLM Overview","description":"Large Language Models","date":"2025-03-01T00:00:00.000Z","tags":[{"inline":false,"label":"ML","permalink":"/notes/blog/tags/ML","description":"Machine Learning"}],"readingTime":2.755,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"LLMs","title":"LLM Overview","authors":["rakesh"],"tags":["ML"],"side_position":1},"unlisted":false,"nextItem":{"title":"Graphics Processing Unit (GPU)","permalink":"/notes/blog/GPU"}},"content":"## Large Language Models\\nOpenAI has been at the forefront of developing sophisticated LLMs, often perceived as black boxes. Until DeepSeek\'s open-source release, that is. Now, we have a unique opportunity to peek behind the curtain. In this series of articles, I will delve into the inner workings of DeepSeek\'s LLMs, starting with the basics before moving on to more advanced topics, covering their architecture and optimization techniques. Here are some of my notes exploring their research papers.\\n\\n\x3c!-- truncate --\x3e\\n---\\n\\n## Background\\nDeepSeek has emerged as a significant disruptor in the AI industry, particularly in the realm of large language models (LLMs). It gained recognition for achieving high-level AI performance while utilizing significantly fewer computational resources compared to industry giants like OpenAI. \\n\\nDeepSeek leverages techniques like \\"Mixture of Experts (MoE)\\"  to optimize resource allocation,  innovations like Multi-Head Latent Attention (MLA) etc among many others. But the most important breakthrough was their commitment to open-source principles contributing to its rapid growth and influence. \\n\\n## Impact\\n\\nDeepSeek\'s success has put pressure on established AI companies to reconsider their development strategies and by democratizing their AI technology, it has made these models more accessible to smaller organizations and developing nations.\\n\\nThey developed a model much more powerful (or similar) than OpenAI with significantly less resource. DeepSeek\'s rise signifies a shift towards more efficient and accessible AI, challenging the dominance of OpenAI.\\n\\n## My Motivation\\n\\nDeepSeek\'s achievement, building a powerful model with optimized resources, resonates deeply. It\'s a testament to human ingenuity, a reminder that constraints often spark the most innovative solutions. When faced with limitations, we have an incredible ability to find efficient, impactful pathways forward.\\n\\n\\n> *We\'ve always defined ourselves by the ability to overcome the impossible. And we count these moments. These moments when we dare to aim higher, to break barriers, to reach for the stars, to make the unknown known. We count these moments as our proudest achievements. But we lost all that. Or perhaps we\'ve just forgotten that we are still pioneers. And we\'ve barely begun. And that our greatest accomplishments cannot be behind us, because our destiny lies above us.*\\n**- Cooper, (Movie - Interstellar)**\\n\\n## LLM Basics\\n\\nHere\'s the general overview of LLMs\\n\\n![Journey](img/Llmjourney.png)\\n\\n### LLM Stages\\nIt is important for us to understand the LLM stages, I will go through each one in detail. \\n\\n![alt text](img/LLMStages.png)\\n\\n### Simple Transformers and Attention\\n\\n![alt text](img/simpleTransormers.png)\\n\\n![alt text](<img/selfAttention.png>)\\n\\n### Autoregression Models\\nThe pretraining stage uses autoregressive model. The output of the model is fed as the input to the model. More on this later.\\n\\n![alt text](img/AutoregressiveModel.png)\\n\\n\\n## DeepSeek\'s Archictecure  \\n\\nHere are the some of the components used by DeepSeek \\n\\n> Multi-Head Latent Attention \\n\\n> Mixture of Experts (MoE)   \\n\\n> Multi-Token Prediction   \\n\\n> Reinforcement Learning using GRPO  \\n\\n> GPU Optimization via PTX  \\n\\n![Overview](img/Llmdetails.png)\\n\\n\\n# Conclusion\\n\\nThis blog series **(converted from my research notes)** will provide a glimpse into the architecture and optimization of foundational LLMs, culminating in a deeper understanding of DeepSeek\'s architecture, equipping you with a solid foundation to embark on your own LLM research. The open sourcing of LLMs, and DeepSeek in particular, is a major step in the democratization of AI. The rapid evolution of these models promises to reshape how we interact with technology.\\n\\nEnjoy your reading!\\n\\n## References\\n\\n[Reinforcement Learning](https://arxiv.org/pdf/2501.12948)  \\n[Roadmap for LLMs](https://github.com/rvbug/NLP)"},{"id":"GPU","metadata":{"permalink":"/notes/blog/GPU","source":"@site/blog/2025-03-01/1-Graphics Processing Unit.md","title":"Graphics Processing Unit (GPU)","description":"In my last blog, I quickly introduced to DeepSeek and some of the components the used. In this blog, we will get down to basics of GPU and specifically NVIDIA GPU architecture, how CUDA programs gets compiled.","date":"2025-03-01T00:00:00.000Z","tags":[{"inline":false,"label":"ML","permalink":"/notes/blog/tags/ML","description":"Machine Learning"},{"inline":true,"label":"LLMs","permalink":"/notes/blog/tags/ll-ms"}],"readingTime":2.69,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"GPU","title":"Graphics Processing Unit (GPU)","authors":["rakesh"],"tags":["ML","LLMs"]},"unlisted":false,"prevItem":{"title":"LLM Overview","permalink":"/notes/blog/LLMs"},"nextItem":{"title":"PTX Basics","permalink":"/notes/blog/PTXIntro"}},"content":"In my last blog, I quickly introduced to DeepSeek and some of the components the used. In this blog, we will get down to basics of GPU and specifically NVIDIA GPU architecture, how CUDA programs gets compiled. \\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\n## *CPU vs GPU*\\nWhat is the difference between a CPU and GPU?\\n\\n**A**> CPUs are for general-purpose computing where as GPUs are optimized for performing same operations on multuple data points simultaneously achieveing in high level of parallelism.  \\n\\n**B**> CPUs are best suited which runnign complex logic where as GPUs are ideally for massively parallel computations like graphics rendering, deep learning and scientific simulations.  \\n\\n**C**> CPU is optimized for low-latency access to relatively small amount of memory. GPU is typically used for high-bandwidth to large amount of dataset in parallel.\\n\\n\\n## *GPU*\\nHere\'s a quick introduction to GPU, it\'s architecture and CUDA programming workflow.\\n\\nThe cuda program workflow is as shown below. It starts off with your program which has extension of \\".cu\\". This code can be written in C, C++ or Fortran.\\n\\n## *Basic Flow*\\n\\nThis is a very basic flow of a CUDA program.\\n\\n![Cuda Program Overview](img/CUDAOverview.png)\\n\\n## *Detailed Workflow*\\n\\n![CUDA Workflow](img/CUDAWorkflow.png)\\n\\nTypically these  programs has both CPU and GPU instructions running on host machine. The CPU piece of code is called the host code and GPU code section is called device code typically which has __global__ and __device__ function. \\n\\nThis code is compiled using NVCC compiler separating both the CPU and GPU code. The CPU code uses CPU complier like `GCC` converting to the object code. \\n\\nAt the same time the 1st pass of the GPU converts the GPU code is converted to PTX code. This is a low level IR (Intermediate Representation) is then compiled to convert it to device specific code known as SASS code. \\n\\nFinal stage is to link the host object code with SASS and run the code to run on host machine.\\n\\n\\nPTX is an abstraction layer helping code portabiloty between NVIDIA decides. This helps tools and libraries to manupulate code code before GPU execution, an approach whichDeepSeek team did. We will have a separate article going over the PTX.\\n\\n## *Structure*\\nBefore we delve deeper into the PTX optimization by DeepSeek, let us first talk about GPU .\\n\\n![GPU Structure](<img/GPU Structure.png>)\\n\\nThere are : \\n1. 7 Graphic Processing Clusters (GPC) \\n2. Each GPC had 12 Streaming Multiprocessors (SM). \\n3. Every SM will have 4 Warps and 1 Ray Tracing core \\n4. A single Warp will have 32 CUDA core and 1 Tensor Code.\\n5. 12 Graphic Memory Controllers\\n6. Two L2 Cache of 6MB SRAM each\\n7. NVLink\\n8. PCIe Interface\\n\\n\\nSo in total in one NVDIA GPU there could be about :\\n\\n> `10752` CUDA cores   \\n\\n> `336` Tensor cores    \\n\\n> `84` Ray tracing cores  \\n\\n## *Core*\\n\\n![CUDA Cores](img/Cores.png)\\n\\nHere are the three cores available in these GPUs\\n\\n- CUDA Core is used for game and game engines. \\n- Tensor Core is exclusively for Matrix Multiplication and Geometric Transformation which is used in AI/ML\\n- Ray Tracing is used ror Ray Tracing algorithms.\\n\\n\\n## Next Steps\\n\\nWith basic introduction to GPU out of the way. let us move to PTX. See you in my next arcticle."},{"id":"PTXIntro","metadata":{"permalink":"/notes/blog/PTXIntro","source":"@site/blog/2025-03-01/2. Intro_to_ptx.md","title":"PTX Basics","description":"Parallel Thread Execution (PTX) is a virtual machine instruction set architecture and can be thought of as the assembly language for NVDIA GPUs. You would need to know few syntax of PTX. This should get you started quickly.","date":"2025-03-01T00:00:00.000Z","tags":[{"inline":false,"label":"ML","permalink":"/notes/blog/tags/ML","description":"Machine Learning"},{"inline":false,"label":"ML Reserach","permalink":"/notes/blog/tags/ML-Research","description":"Machine Learning Research"}],"readingTime":9.425,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"PTXIntro","title":"PTX Basics","authors":["rakesh"],"tags":["ML","ML Research"]},"unlisted":false,"prevItem":{"title":"Graphics Processing Unit (GPU)","permalink":"/notes/blog/GPU"},"nextItem":{"title":"PTX Optimization","permalink":"/notes/blog/DeepSeekPTX"}},"content":"Parallel Thread Execution (PTX) is a virtual machine instruction set architecture and can be thought of as the assembly language for NVDIA GPUs. You would need to know few syntax of PTX. This should get you started quickly.\\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\n# PTX Syntax \\n\\nHere are few syntax of PTX for your references.\\n\\n## **Basic Structure**\\n\\n```ptx\\n.version 7.0                    // PTX version\\n.target sm_70                   // Target architecture\\n.address_size 64                // 64-bit addressing\\n\\n.visible .entry kernel_name(   // Kernel name and declaration\\n    .param .u64 param1,        // Parameters\\n    .param .f32 param2\\n)\\n{\\n    // Kernel body\\n}\\n```\\n\\n## **Registers**\\n\\n```ptx\\n.reg .b32 %r<5>;    // 5 32-bit registers %r0 through %r4\\n.reg .f32 %f<3>;    // 3 single-precision float\\n.reg .b64 %rd<2>;   // 2 64-bit \\n.reg .pred %p<2>;   // 2 predicate registers (used for conditionals)\\n\\n```\\n\\n\\n## **Instructions**\\n\\n```ptx\\nmov.u32 %r1, %tid.x;       // Move thread(ID) to %r1\\nadd.s32 %r3, %r1, %r2;     // Add int\\nmul.f32 %f3, %f1, %f2;     // Mul floats\\nsetp.lt.s32 %p1, %r1, %r2; // Set predicate if r1 < r2\\n@%p1 bra label;            // Conditional branch\\n\\n```\\n\\n## **Memory Operations**\\n\\n```ptx\\nld.param.u64 %rd1, [param1];       // Load param into register\\nld.global.f32 %f1, [%rd1];         // Load from global mem\\nst.global.f32 [%rd2], %f2;         // Store to global mem\\nld.shared.f32 %f3, [%rd3];         // Load from shared mem\\nld.global.v4.f16 {%f1, %f2, %f3, %f4}, [%rd1];  // Vector load\\n\\n```\\n\\n## **Special Registers**\\n\\n```ptx\\n%tid.x, %tid.y, %tid.z     // Thread (ID) within a block\\n%ctaid.x, %ctaid.y, %ctaid.z   // Block (ID) within a grid\\n%ntid.x, %ntid.y, %ntid.z  // Block dimensions (threads per block)\\n\\n```\\n\\n## **Math Operaations**\\n\\n```ptx\\nadd.f32 %f3, %f1, %f2;     // Add\\nsub.f32 %f3, %f1, %f2;     // Sub\\nmul.f32 %f3, %f1, %f2;     // Mul\\ndiv.f32 %f3, %f1, %f2;     // Div\\nmad.f32 %f4, %f1, %f2, %f3;  // Multiply & add: (f4 = f1*f2+f3)\\n```\\n\\n## **Tensor Core Operations**\\n\\n```ptx\\n// Matrix multiply-accumulate using tensor cores\\nmma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 \\n    {%f5, %f6, %f7, %f8},   // Destination registers\\n    {%f1, %f2},             // A matrix registers\\n    {%f3, %f4},             // B matrix registers\\n    {%f5, %f6, %f7, %f8};   // C matrix registers (accumulator)\\n\\n```\\n\\n## **Control Flow**\\n\\n```ptx\\nbra label;           // Unconditional branch\\n@%p1 bra label;      // Conditional branch if predicate = true\\nret;                 // Return from kernel\\n```\\n\\n\\n## Code Sample\\n\\nNow that we know the the basic syntax of PTX, here is one simple C program for vector addition. \\n\\n```C\\n\\n#include <stdio.h>\\n#include <cuda.h>\\n#include <cuda_runtime.h>\\n\\n// Kernel function to add the elements of two arrays\\n__global__ void vectorAdd(int *a, int *b, int *c, int n) {\\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\\n    if (i < n) {\\n        c[i] = a[i] + b[i];\\n    }\\n}\\n\\nint main() {\\n    int n = 1000;\\n    int size = n * sizeof(int);\\n    int *a, *b, *c;\\n    int *d_a, *d_b, *d_c;\\n\\n    // Allocate memory on the host\\n    a = (int *)malloc(size);\\n    b = (int *)malloc(size);\\n    c = (int *)malloc(size);\\n\\n    // Initialize the arrays\\n    for (int i = 0; i < n; i++) {\\n        a[i] = i;\\n        b[i] = i * 2;\\n    }\\n\\n    // Allocate memory on the device\\n    cudaMalloc((void **)&d_a, size);\\n    cudaMalloc((void **)&d_b, size);\\n    cudaMalloc((void **)&d_c, size);\\n\\n    // Copy data from host to device\\n    cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\\n    cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\\n\\n    // Launch the vectorAdd kernel\\n    int threadsPerBlock = 256;\\n    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\\n    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, n);\\n\\n    // Copy the result from device to host\\n    cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\\n\\n    // Free device memory\\n    cudaFree(d_a);\\n    cudaFree(d_b);\\n    cudaFree(d_c);\\n\\n    // Free host memory\\n    free(a);\\n    free(b);\\n    free(c);\\n\\n    return 0;\\n}\\n\\n```\\n### Generating output\\n\\nLet us compile the code using `nvcc` compiler\\n\\n```bash\\n$> nvcc vector.cu -o vector\\n$> ./my_kernel\\n```\\n\\n```bash\\n# output\\nc[0] = 0\\nc[1] = 3\\nc[2] = 6\\nc[3] = 9\\nc[4] = 12\\nc[5] = 15\\nc[6] = 18\\nc[7] = 21\\nc[8] = 24\\nc[9] = 27\\n\\n```\\n\\n\\n\\n### Code Explaination\\n\\n\\n```C\\n// This is for CUDA runtime functions\\n#include <cuda_runtime.h> \\n```\\n\\n#### Kernel Function \\n\\n* *`__global__`* specifices that this is CUDA kernel that runs on GPU    \\n* Three float arrays are pointers along with array size  `a`, `b` and `c`\\n* *`blockIdx.x`* is the block index. \\n* *`blockDim.x`* is number of thread per block\\n* *`threadIdx.x`* is thread index within a block\\n* Calculate unique ID for each thread to process a different array element\\n\\n\\n\\n```C\\n// CUDA kernel for vector addition\\n__global__ void vectorAdd(float *a, float *b, float *c, int n)\\n{\\n    // Calculate global thread ID\\n    int id = blockIdx.x * blockDim.x + threadIdx.x;\\n\\n    // To make sure we don\'t go out of bounds\\n    if (id < n)\\n        c[id] = a[id] + b[id];\\n}\\n```\\n\\n#### Main function\\n\\n```C\\nint main()\\n{\\n    // Vector size\\n    int n = 1000000;               // One million elements\\n    size_t bytes = n * sizeof(float);  // Calculate memory size in bytes\\n    // Allocate host memory\\n    float *h_a = (float*)malloc(bytes);  // Allocate memory for array a\\n    float *h_b = (float*)malloc(bytes);  // Allocate memory for array b\\n    float *h_c = (float*)malloc(bytes);  // Allocate memory for results\\n    // Initialize vectors on host\\n    for (int i = 0; i < n; i++)\\n    {\\n        h_a[i] = 1.0f;  // All elements in a are 1.0\\n        h_b[i] = 2.0f;  // All elements in b are 2.0\\n    }\\n}\\n```\\n\\n#### GPU Memory Allocation\\n\\n```C\\n// Allocate device memory\\n    float *d_a, *d_b, *d_c;            // Declare device pointers\\n    cudaMalloc(&d_a, bytes);           // Allocate memory on GPU for a\\n    cudaMalloc(&d_b, bytes);           // Allocate memory on GPU for b\\n    cudaMalloc(&d_c, bytes);           // Allocate memory on GPU for c\\n\\n```\\n\\n#### GPU Data Transfer\\n\\n```C\\n// Copy data from host to device\\n    cudaMemcpy(d_a, h_a, bytes, cudaMemcpyHostToDevice);  // Copy a to GPU\\n    cudaMemcpy(d_b, h_b, bytes, cudaMemcpyHostToDevice);  // Copy b to GPU\\n\\n```\\n\\n#### Kernel Launch Configuration\\n\\n```C\\n// Set up execution configuration\\n    int blockSize = 256;                         // 256 threads per block\\n    int gridSize = (n + blockSize - 1) / blockSize;  // Calculate grid size\\n// This formula ensures we have enough blocks to cover all elements\\n// Launch kernel\\n    vectorAdd<<<gridSize, blockSize>>>(d_a, d_b, d_c, n);\\n    // <<<>>> is special CUDA syntax for kernel launch configuration\\n    // gridSize = number of blocks, blockSize = threads per block\\n```\\n\\n#### Results and Cleanup\\n\\n```C\\n// Copy result back to host\\ncudaMemcpy(h_c, d_c, bytes, cudaMemcpyDeviceToHost);  // Copy results from GPU to CPU\\n\\n// Free memory\\ncudaFree(d_a);  // Free GPU memory for a\\ncudaFree(d_b);  // Free GPU memory for b\\ncudaFree(d_c);  // Free GPU memory for c\\nfree(h_a);      // Free CPU memory for a\\nfree(h_b);      // Free CPU memory for b\\nfree(h_c);      // Free CPU memory for c\\n\\n```\\n\\n\\n## PTX Code\\n\\nTo extract PTX  from the above code, try this the following command.\\n\\n```bash \\n$> nvcc -ptx vector.cu -o vector.ptx\\n```\\n\\n### PTX file \\n\\n```ptx\\n\\n.visible .entry vectorAdd(\\n    .param .u64 vectorAdd_param_0,  // Pointer to array a\\n    .param .u64 vectorAdd_param_1,  // Pointer to array b\\n    .param .u64 vectorAdd_param_2,  // Pointer to array c\\n    .param .u32 vectorAdd_param_3   // Parameter n (size)\\n)\\n{\\n    .reg .pred  %p<2>;          // Predicate registers\\n    .reg .f32   %f<4>;          // Float registers\\n    .reg .b32   %r<6>;          // 32-bit registers\\n    .reg .b64   %rd<11>;        // 64-bit registers\\n\\n    // Load parameters into registers\\n    ld.param.u64    %rd1, [vectorAdd_param_0];\\n    ld.param.u64    %rd2, [vectorAdd_param_1];\\n    ld.param.u64    %rd3, [vectorAdd_param_2];\\n    ld.param.u32    %r2, [vectorAdd_param_3];\\n    \\n    // Calculate thread ID\\n    mov.u32         %r3, %ctaid.x;    // Get block index\\n    mov.u32         %r4, %ntid.x;     // Get block size\\n    mov.u32         %r5, %tid.x;      // Get thread index within block\\n    mad.lo.s32      %r1, %r3, %r4, %r5;  // Calculate global thread ID: blockIdx * blockDim + threadIdx\\n    \\n    // Check if thread ID is within bounds\\n    setp.ge.s32     %p1, %r1, %r2;    // Set predicate if thread ID >= n\\n    @%p1 bra        BB0_2;            // If true, jump to the end (BB0_2 label)\\n    \\n    // Calculate memory addresses\\n    cvta.to.global.u64  %rd4, %rd1;   // Convert array a pointer to global address\\n    mul.wide.s32    %rd5, %r1, 4;     // Multiply thread ID by 4 (size of float)\\n    add.s64         %rd6, %rd4, %rd5; // Calculate address for a[id]\\n    \\n    cvta.to.global.u64  %rd7, %rd2;   // Convert array b pointer to global address\\n    add.s64         %rd8, %rd7, %rd5; // Calculate address for b[id]\\n    \\n    // Load values, add them, and store result\\n    ld.global.f32   %f1, [%rd6];      // Load a[id]\\n    ld.global.f32   %f2, [%rd8];      // Load b[id]\\n    add.f32         %f3, %f1, %f2;    // Add them: c[id] = a[id] + b[id]\\n    \\n    cvta.to.global.u64  %rd9, %rd3;   // Convert array c pointer to global address\\n    add.s64         %rd10, %rd9, %rd5; // Calculate address for c[id]\\n    st.global.f32   [%rd10], %f3;     // Store the result in c[id]\\n    \\nBB0_2:                                // End label\\n    ret;                              // Return from kernel\\n}\\n\\n```\\n\\n### PTX Code\\n\\nLet us now review the PTX code\\n\\n### Entry Point \\n\\nThis section declares entry point for the kernel followed by 4 paramaters which is a pointer to the variable a, b, c and size n.\\n\\n```ptx\\n\\n.visible .entry vectorAdd(           // Declares entry point for kernel\\n    .param .u64 vectorAdd_param_0,   // First parameter (pointer to array a)\\n    .param .u64 vectorAdd_param_1,   // Second parameter (pointer to array b)\\n    .param .u64 vectorAdd_param_2,   // Third parameter (pointer to array c)\\n    .param .u32 vectorAdd_param_3    // Fourth parameter (size n)\\n)\\n\\n```\\n\\n### Register Declaration\\n\\nDeclating Predicate registers for conditions, float registers, 32 bit int and register for addresses\\n\\n```ptx\\n    .reg .pred  %p<2>;          // Predicate r\\n    .reg .f32   %f<4>;          // Float \\n    .reg .b32   %r<6>;          // 32-bit int\\n    .reg .b64   %rd<11>;        // 64-bit for addresses\\n```\\n\\n### Parameter Loading\\n\\nThis section loads parameters from kernel into registers. `ld` is for load.\\n\\n```ptx\\n\\n    ld.param.u64    %rd1, [vectorAdd_param_0];  // Load pointer to array a\\n    ld.param.u64    %rd2, [vectorAdd_param_1];  // Load pointer to array b\\n    ld.param.u64    %rd3, [vectorAdd_param_2];  // Load pointer to array c\\n    ld.param.u32    %r2, [vectorAdd_param_3];   // Load size n\\n```\\n\\n### Thread ID\\nNow calcuate unique thread Id using built-on registers.   \\n\\nFirst get the current block index into `%r3`, then get number of threads per block into `%r4` and then get thread index within this block into `%r5`. `mad` is multiply and add in a single instruction and calculate ID by using `blockIdx * blockDim + threadIdx`.    \\n\\n\\n```ptx\\n    mov.u32         %r3, %ctaid.x;    \\n    mov.u32         %r4, %ntid.x;     \\n    mov.u32         %r5, %tid.x;      \\n    mad.lo.s32      %r1, %r3, %r4, %r5; \\n```\\n\\n### Bounds Checking\\n\\nHere we check  if thread ID is within bounds of the array and then Set predicate `%p1` if thread ID >= n. If true then jump to return label `BB0_2`\\n\\n```ptx\\n// \\nsetp.ge.s32     %p1, %r1, %r2;    // \\n@%p1 bra        BB0_2;            // If true, jump to the return label (BB0_2)\\n\\n```\\n\\n### Memory Calculations\\n\\nCalculate memory address for array elements. Covert array `a` pointer to global address using `cvta`.  Multiply `mul` thread Id by 4 which is the size of the float followed by adding address for `a`.  \\n\\n```ptx\\n\\n    cvta.to.global.u64  %rd4, %rd1;   \\n    mul.wide.s32    %rd5, %r1, 4;     \\n    add.s64         %rd6, %rd4, %rd5; \\n    \\n    cvta.to.global.u64  %rd7, %rd2;   // Convert array b pointer to global address\\n    add.s64         %rd8, %rd7, %rd5; // Calculate address for b[id]\\n\\n```\\n\\n### Load, Add and Store\\n\\nLoad values from arrays, perform addition, and store result\\n\\n```ptx\\n    ld.global.f32   %f1, [%rd6];      // Load a[id] into register %f1\\n    ld.global.f32   %f2, [%rd8];      // Load b[id] into register %f2\\n    add.f32         %f3, %f1, %f2;    // Add them: %f3 = %f1 + %f2\\n    \\n    cvta.to.global.u64  %rd9, %rd3;   // Convert array c pointer to global address\\n    add.s64         %rd10, %rd9, %rd5; // Calculate address for c[id]\\n    st.global.f32   [%rd10], %f3;     // Store the result in c[id]\\n\\n```\\n\\n### Return from Kernel\\n\\n```ptx\\nBB0_2:  // Label for our return point\\nret;                             \\n```\\n\\n\\n## Conclusion\\nThis article covered the basic syntax of PTX. The next article will focus on how Deepseek could have possibly optimized their PTX code on their H800 NVIDIA GPUs."},{"id":"DeepSeekPTX","metadata":{"permalink":"/notes/blog/DeepSeekPTX","source":"@site/blog/2025-03-01/3.Advanced_ptx copy.md","title":"PTX Optimization","description":"How Deepseek optimized performance of the LLMs","date":"2025-03-01T00:00:00.000Z","tags":[{"inline":false,"label":"ML","permalink":"/notes/blog/tags/ML","description":"Machine Learning"},{"inline":false,"label":"ML Reserach","permalink":"/notes/blog/tags/ML-Research","description":"Machine Learning Research"}],"readingTime":5.04,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"DeepSeekPTX","title":"PTX Optimization","description":"How Deepseek optimized performance of the LLMs","image":"img/LLMWorkflow.png","authors":["rakesh"],"tags":["ML","ML Research"]},"unlisted":false,"prevItem":{"title":"PTX Basics","permalink":"/notes/blog/PTXIntro"},"nextItem":{"title":"Tokenization","permalink":"/notes/blog/Tokenizer"}},"content":"Let us now delve into the details of PTX, the parallel thread execution virtual instruction set, and explore how DeepSeek might have approached optimization for their H800 GPUs. PTX optimization is critical for maximizing performance in large language models. \\n\\n\x3c!-- truncate --\x3e\\n\\nSince DeepSeek\'s specific PTX implementations are proprietary, this article focuses on optimization strategies inferred from their research papers and related discussions. We\'ll explore a few of them within their architecture. For example, **Multi-Head Latent Attention (MHLA)** employs a modified Key and Value cache approach, differing from the standard transformer KV cache concept, to enhance efficiency.\\n\\n----\\n\\n# Overview\\nDeepSeek, particularly with their R1 model, has implemented significant optimizations across both training and inference phases. We will delve into these broader optimizations in a separate, more detailed article. In this one, however, our focus will be exclusively on PTX. \xa0 \\n\\nPTX empowers developers with the ability to perform low-level optimizations, granting fine-grained control over register allocation, thread execution, and memory access patterns.\\n\\n## Register Allocation\\n\\n1. Allowing manually optimizing the register allocation, the latency could have been reduced.\\n\\n2. Fine-tune thread scheduling allowing them to maximize parallelism across the streaming multiprocessors.\\n\\n\\n## Custom Memory Management\\n\\nImplementing custome PTX instructions for accessing memory including global VRAM access by bypassing L1 and L2 cache in a very specific way allowing increasing data transfer pattern thus improving memory bandwidth.\\n\\n:::tip  \\n- **Global VRAM** is largest and slowest memory on the GPU\\n- **Cache** can also introduce overhead and may not always be effective\\n- **Coalesced Access** - Accessing contiguous memory locations in a single transaction significantly improves memory bandwidth. \\n- **Memory Access** - Aligned memory access e.g. to 128-bytes are much more efficient.\\n:::\\n\\n\\n### Cache\\n\\nSince they were dealing with large and streaming datasets, they miht have bypassed **`L1`** or **`L2`** cache. This can be accessible via PTX that allow to control these behaviour\\n\\nBelow is the sample snippet showing the access. Loads from global memory and bypass both L1 and L2 cache.  \\n\\n```asm\\n.reg .u64 %addr;\\n.reg .f32 %data;\\n\\nld.global.nc.f32 %data, [%addr]; \\n```\\n\\n:::info  \\n> **nc** means no cache  \\n> **ld.global.nc.f32** - load 32 bit floating point value from global memory \\n:::\\n\\n### Cache Controls\\n\\n`.volalite` - This modifer tells compiler that memory location can be modified by other threads/devices preventing complier for any optimization to ensure value in the memory remains constant.\\n\\n`.wt` and `.wb` - These are write through and write back modifiers controling the cache write policy. \\n\\n`.wt` writes to both cache and global memory while `.wb` writes only to cache but writes to global memory once cache data is evicted.\\n\\n:::info \\nDeepseek might have used these `write-through` and `write-back` modifiers to further optimize their workload.\\n:::\\n\\n`.relaxed`,`.acquire`,`.release`,`.acquire_release` modifiers are used when dealing with memory coherency between threads i.e. order of memory reads and writes\\n\\n:::info\\nDeepseek most likely used these modifiers when working with shared memory buffers which are accessed by multiple threads.\\n:::\\n\\n### Prefetching\\n\\nFor the predictible memory access, they could have use PTX\'s prefetch instructions to bring load the data in cache before it is needed hiding memory latency thus improving performance\\n\\n```ptx\\nreg .u64 %addr;\\nprefetch.global [%addr];\\n\\n```\\n:::info  \\n> **prefetch.global** Prefetch data into L1 cache.\\n:::\\n\\n\\n#### Prefetch Distance and Hints\\nIt is possible that these parameters are tuned to optimizing prefetching performance.\\n\\n:::info  \\n> **Prefetch Distance** Number of memory location to prefetch ahead.  \\n\\n> **Prefetch Hints** helps to understand tyoe of memory access patterns based on the type of hardware.  \\n:::\\n\\n\\n### Alignment & Coalescing \\nSince PTX allow precise control over memory aligment and access patterns, they could use this to maximize memory bandwidth. Sample code below.\\n\\n```ptx\\n.reg .u64 %base_addr;\\n.reg .u32 %offset;\\n.reg .f32 %data;\\n\\nmad.lo.u64 %addr, %offset, 4, %base_addr; // Assuming 4-byte floats\\n\\n// Load coalesced data\\nld.global.v4.f32 {%data, %data+4, %data+8, %data+12}, [%addr];\\n```\\n\\n:::info  \\n> **ld.global.v4.f32** - Loads vector of 4 32-bit floating values from VRAM ensuring coalesced access.\\n\\n> **mad.lo.u64** - Multiply add lower 64 bits for calculating memory address.\\n:::\\n\\n#### Vectorized loads\\n\\nThey might have used vectorized loads which allow multiple data element to be transferred into a single memory transactions by maximizing memory bandwidth and also ensure these access are coalesced. Sample code below showing loading and storing 4 floats at once.\\n\\n```ptx\\n.reg .u64 %addr;\\n.reg .v4.f32 %data;\\n\\nld.global.v4.f32 %data, [%addr]; \\nst.global.v4.f32 [%addr], %data;\\n\\n```\\n\\n\\n### Shared Memory Optimization\\n\\nShared memory is organized in bands so to avoid conflicts they could have used multiple threads access the same banks simultaneously by arranging data carefully.\\n\\nIt could also be possible that they might have used shared on-chip memory to reduce global access. Below code shows data being moved from global memory to shared memory and then use it.\\n\\n```ptx\\n.shared .f32 shared_data[1024];\\n.reg .u32 %thread_id;\\n.reg .f32 %local_data;\\n\\n// Load data from global memory into shared memory\\nld.global.f32 %local_data, [global_addr + %thread_id*4];\\nst.shared.f32 [shared_data + %thread_id*4], %local_data;\\n\\n// Use data from shared memory\\nld.shared.f32 %local_data, [shared_data + %thread_id*4];\\n```\\n\\n## Inter-GPU communcation\\n\\nAllocate a portion of SM to improve communication by data compression and remove bottlenecks\\n\\n## Warp Level Optimization\\nFine-grain tunining again on warp which contains 32 threads on how they process instructions.\\n\\nNVIDIA GPUs execute threads in groups of 32, called warps. So PTX can allow developers to write warp-synchronous code, to make it more efficient.\\n\\nDeepSeek could have used warp-level primitives to perform warp-wide reductions and scans.\\n\\n#### Warp Shuffle Instructions:\\nPTX also provides shuffle instructions that allow threads within a warp to exchange data. It can be used to implement efficient inter-thread communication.\\nOptimize data layout for shared memory.\\n\\n\\n## Conclusion\\nThis article has outlined potential PTX optimizations employed by DeepSeek. These optimizations highlight DeepSeek\'s impressive ability to leverage fundamental hardware optimization, enabling them to develop models that effectively compete with OpenAI. The difficulty of these low level optimizations cannot be overstated.\\n\\n\\n## Next\\nIn my next article, we will get into the details of how these optimizations happen in various stages of the architecture, from MHLA to Multi-token.\\n\\n\\n## References\\n[DeepSeek R1](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf)  \\n[DeepSeek EP Github](https://github.com/deepseek-ai/DeepEP)"},{"id":"Tokenizer","metadata":{"permalink":"/notes/blog/Tokenizer","source":"@site/blog/2025-03-01/4.Tokenization.md","title":"Tokenization","description":"What is Tokenizer","date":"2025-03-01T00:00:00.000Z","tags":[{"inline":false,"label":"ML","permalink":"/notes/blog/tags/ML","description":"Machine Learning"},{"inline":false,"label":"ML Reserach","permalink":"/notes/blog/tags/ML-Research","description":"Machine Learning Research"}],"readingTime":1.3,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"Tokenizer","title":"Tokenization","description":"What is Tokenizer","authors":["rakesh"],"tags":["ML","ML Research"]},"unlisted":false,"prevItem":{"title":"PTX Optimization","permalink":"/notes/blog/DeepSeekPTX"},"nextItem":{"title":"Dotfiles","permalink":"/notes/blog/dotfiles"}},"content":"In building LLMs, the first step is to obtain the input text and pass it through the Tokenizer\\n\\n\x3c!-- truncate --\x3e\\n\\n# Tokenizer\\n\\nSteps in Tokenization is as follows\\n\\n![Tokenization](img/tokenization.png)\\n\\n\\n## Types of Tokenizer\\n\\nBelow are three type of Tokenizers available. \\n\\n![alt text](img/tokenizer.png)\\n\\n\\n## Byte Pair Encoding (BPE)\\n\\n:::note\\n- Orginally BPE was developed as a compression algorithm.  \\n\\n- Checkout more details and the example used from [**Wiki**](https://en.wikipedia.org/wiki/Byte_pair_encoding)\\n:::\\n\\n![alt text](img/Bpe.png)\\n\\n## Modified BPE\\n\\n:::tip\\nIn LLM, we use something called **`Modified Byte Pair Encoding`**. Used for encoding plain text to tokens\\n:::\\n\\n![alt text](img/ModifiedBPE.png)\\n\\n\\n## BPE Library\\n\\nIn Rust, you can use [Tiktoken-rs](https://docs.rs/crate/tiktoken-rs/latest) for *`BPE`*\\n\\n\\n:::info\\n\\nEquivalent Python library is available \\n\\n> [Tiktoken](https://pypi.org/project/tiktoken/)\\n\\n:::\\n\\n\\n## Rust Code\\n\\nBelow is the rust code using `Tiktoken-rs`\\n\\n```rust\\nuse tiktoken_rs::o200k_base;\\n\\nfn main() {\\n    let bpe = o200k_base().unwrap();\\n    let token: Vec<u32> = bpe.encode_with_special_tokens(\\"This is multi line sentence for BPE with rust and a sentence   with spaces\\");\\n\\n    println!(\\"Token: {:?}\\", token);\\n    println!(\\"Decoding the token {:?}\\", bpe.decode(token));\\n\\n}\\n```\\n\\nOutput of this program is as shown below\\n\\n```bash\\n$> cargo run\\n\\n## OUTPUT\\n\\n#Token: [2500, 382, 12151, 2543, 21872, 395, 418, 3111, 483, 20294, 326, 261, 21872, 256, 483, 18608]\\n#Decoding the token Ok(\\"This is multi line sentence for BPE with rust and a sentence   with spaces\\")\\n\\n```\\n\\n## Python Code\\n\\n```python\\nimport tiktoken\\ntokenizer = tiktoken.get_encoding(\\"o200k_base\\")\\n\\ntext_data = ( \\"Ecode using BPE via Python\\")\\n\\nencoder_output = tokenizer.encode(text_data, allowed_special={\\"<|END|>\\"})\\nprint(encoder_output)\\n\\n\\ndecoder_output = tokenizer.decode(encoder_output)\\n\\nprint(decoder_output)\\n```\\n\\n```bash\\n$> python3 bpe_example.py\\n\\n## OUTPUT\\n\\n# [36, 3056, 2360, 418, 3111, 4493, 26534]\\n# Ecode using BPE via Python\\n\\n```"},{"id":"dotfiles","metadata":{"permalink":"/notes/blog/dotfiles","source":"@site/blog/2024-04-21/Dotfiles.md","title":"Dotfiles","description":"In the first part of my Productivity series, we talked about configuring Neovim as your IDE. You can check out that blog and my configuration here.","date":"2024-04-21T00:00:00.000Z","tags":[{"inline":false,"label":"dotfiles","permalink":"/notes/blog/tags/dotfiles","description":"dotfiles"}],"readingTime":6.025,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"dotfiles","title":"Dotfiles","authors":["rakesh"],"tags":["dotfiles"]},"unlisted":false,"prevItem":{"title":"Tokenization","permalink":"/notes/blog/Tokenizer"},"nextItem":{"title":"Neovim IDE","permalink":"/notes/blog/Neovim"}},"content":"In the first part of my `Productivity` series, we talked about configuring [Neovim](https://neovim.io/) as your IDE. You can check out that [blog](https://rvbug.hashnode.dev/configuring-neovim) and my configuration [here](https://github.com/rvbug/neovim).\\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\nIn this part, let us talk about the magic of **.dotfiles**, how to automate your development workflow, manage repetitive tasks by installing softwares, setting up editor shortcuts, and even configuring the development environment across multiple machines for a unified experience.\\n\\n# dotfiles - What are they?\\n\\ndotfiles are configuration files with a dot(.) leading the filename. They are usually found on all \\\\*nix based systems helping store various settings and configurations.\\n\\nHere\'s how to find them - Open your favourite terminal and try the following command to show all the hidden files/directories.\\n\\n```bash\\n$> ls -a\\n```\\n\\n# OS\\n\\nThis script has been tested on:\\n\\n* Mac Sonoma\\n    \\n* Mac Ventura (Virtual Environment)\\n    \\n* Ubuntu (Virtual Environment)\\n    \\n* Fedora (Virtual Environment)\\n    \\n* Docker - Ubuntu, Fedora and Debian\\n    \\n\\n# Workflow\\n\\nDownload the repo\\n\\n```bash\\n# clone the repo\\n$> git clone https://github.com/rvbug/.dotfiles\\n\\n# check if the .dotfile directory is available\\n$> ls -a \\n\\n# cd into the folder\\n$> cd .dotfiles\\n\\n# run the first script \\n$> ./install.sh\\n\\n# Note: if you want to run it in a debug mode, try\\n# $> sh -x install.sh\\n```\\n\\n`install.sh` is the entry point which checks for OS version\\n\\n```bash\\n#check the OS version\\nos=$(uname -s)\\n\\n# If it is Darwin then should be mac \\nif [ $os == \\"Darwin\\" ]; then \\n#...\\n# run mac script\\n./macos.sh\\n#...\\n# Linux variant \\nelif [ $os == \\"Linux\\" ]; then\\n#...\\n# call linux script\\n./linux.sh\\n```\\n\\n## The List\\n\\nSince these are highly personal configurations, the requirement was to make these scripts flexible and easy to use and maintain . The newer version now uses text files from the previous `YAML` based configuration.\\n\\nBelow are the three text files used in the script:\\n\\n`software_list.txt` - List of all softwares to be installed on your machine.\\n\\n`pip_list.txt` - If you are into Data Science or ML then you can add or remove the libraries. e.g. If you use PyTorch , just swap it with Tensorflow.\\n\\n`config_list.txt` - List of all dotfiles to be maintained.\\n\\n`Note`: YAML is a widely used data serialization language for writing configuration files.\\n\\n## Mac\\n\\nIf mac is detected then it will look if [Homebrew](https://brew.sh/) (package manager) and git is installed already.\\n\\n### Homebrew & Git\\n\\n```bash\\n# check if homebrew is already installed\\nif command -v brew &>/dev/null; then\\n  echo \\"Homebrew already installed\\"\\nelse\\n# if not install \\n  echo \\"##### Installing homebrew...\\"\\n#...\\nfi\\n\\n#...\\n\\n# checking for git on your machine.\\nif command -v git &>/dev/null; then\\n  echo \\"git already installed...\\"\\n#...\\n```\\n\\n### Installing softwares\\n\\nThe next step is to check if `software_list.txt` file is available to start the installation process. For certain softwares like Lua, specific version is required. For OCaml, users have an option to skip the installation.\\n\\n`Note:` Ocaml is a general purpose \\"functional\\" programming language. You can read more about it [here](https://ocaml.org/).\\n\\n```bash\\n # check if lua is already is installed\\nif brew list lua &>/dev/null; then\\n#...\\n# if not then install via brew\\nbrew install lua@5.4\\n\\n#...\\n\\n# check with user for OCaml installation\\nread -p \\"do you want to install ocaml? (y/n): \\" choice\\n#...\\n```\\n\\n### Data Science and ML\\n\\nThen comes installing Data Science tools. The only requirement is to have `pyyaml` installed on your machine.\\n\\n```bash\\n# check for pyyaml installation\\n# install if not available.\\nif brew list pyyaml &>/dev/null; then\\n```\\n\\nI have a [repo](https://github.com/rvbug/cookie-ml) where you get a cookie-cutter template for ML projects. It downloads the repo and sets up your machine to get you started immediately.\\n\\n```bash\\n# checks if cookie-cutter is already installed\\n# in your home directory\\nif [ -d \\"$HOME/ml-cookie-cutter\\" ]; then\\n\\n# if not then download the repo \\n# install and activate virtual env \\necho \\"####### activating the venv...\\"\\nsource venv/bin/activate \\n\\n# ensure pip is installed and then start \\n# installing the ML libraries\\npip3 install $cfg\\n\\n# Once done the cleanup starts...\\n# delete the downloaded repo and \\n# deactivate the virtual env\\n#...\\n```\\n\\n### Final configuration\\n\\nNow on to the final steps of the installation process - Back up the existing configuration files and check for Neovim.\\n\\n```bash\\n# Take the backup of the existing config files\\ncp $HOME/$line $HOME/$line.bak.$(date +%Y-%m-%d-%H:%M:%S)\\n\\n# If Neovim is already installed, need to \\n# backup the existing configuration\\ncp -r $HOME/.config/nvim/ $HOME/.config/nvim.bak.$dt\\n```\\n\\nThe script will check for xcode command-line tools\\n\\n```bash\\n# check for commandline tools\\nxcode-select --version > /dev/null\\n\\n# if not then install them\\nxcode-select --install\\n```\\n\\n## Linux\\n\\nFor Linux, most of the process remains the same with a few subtle differences.\\n\\n### OS and Package Manager\\n\\nThe script first checks the version of Linux and sets the package manager.\\n\\n```bash\\n# if Ubuntu or Debian, then set the package as ap-get\\npkg_mgr=apt-get\\n# If Fedora, then go for dnf \\npkg_mgr=dnf \\n# if dnf is not available go for yum\\n```\\n\\n### Additional checks\\n\\nNever (**ever**) run any script as a root user, the script warns if you do so and also check for sudo access. Few software installations are skipped if it is not supported by that operating system.\\n\\n```bash\\n# caution the user if it is root user\\nuser=$(whoami)\\nif [ \\"$user\\" == \\"root\\" ]; then\\n\\n# Option to run this with sudo access\\nread -p \\"do you want to run this script as sudo (y/n)?\\" choice\\n\\n# check if you want to upgrade your system\\n# before starting the installation\\n$su_user $pkg_mgr update -y\\n```\\n\\n## Docker Support\\n\\nIf you want to try out this script on docker containers then here are some helpful commands. You can download the images and run the container as shown below.\\n\\n```bash\\n# To pull the images\\ndocker pull fedora # debian # or ubuntu\\n\\n# run the container\\n$> docker run --name fedora -d -i -t fedora /bin/bash\\n\\n# check the processes\\n$> docker ps\\n\\n# to drop inside the container\\n# replace the container-id with the number \\n# shown with the above command\\n$> docker exec -it \\"container-id\\" /bin/bash\\n```\\n\\n`Note` If you are using the docker machine, you will be dropped into the container as \\"root\\".\\n\\n## Config files\\n\\nI used the following files for my machine configuration:\\n\\n1. .zshrc - This helps to configure your shell and is known to provide great customization.\\n    \\n2. .tmux.conf - If you are like me managing multiple projects, why not consider a terminal multiplexer like tmux. It helps create sessions with multiple windows and panes which can be attached and detached easily.\\n    \\n3. .wezterm.lua - I migrated from iterm to a rust based terminal called wezterm, which uses lua based configuration.\\n    \\n4. startship.toml - It is used for customizing your shell prompt.\\n    \\n\\n# Conclusion\\n\\nHere\'s how I created a simple script to set the installation and configuration automatically. Use this as an inspiration to build your own personalized configuration.\\n\\n# Reference\\n\\n[Productivity Series I - Blog](https://rvbug.hashnode.dev/configuring-neovim)\\n\\n[My Neovim Configuration](https://github.com/rvbug/neovim)\\n\\n[My Dotfiles - Github](https://github.com/rvbug/.dotfiles)\\n\\n[Terminal Multiplexer](https://github.com/tmux/tmux/wiki)\\n\\n[Wezterm - Rust based Terminal](https://wezfurlong.org/wezterm/index.html)\\n\\n[Starship - Customizable Shell Prompt](https://starship.rs/)\\n\\n[Docker](https://www.docker.com/)\\n\\n[OCaml - Functional Programming Lang](https://ocaml.org/)"},{"id":"Neovim","metadata":{"permalink":"/notes/blog/Neovim","source":"@site/blog/2024-04-21/Neovimide.md","title":"Neovim IDE","description":"An Integrated Development Environment (IDE) provides a comprehensive list of features like code editor, compiler/interpreter, code completion, debugger and much more.","date":"2024-04-21T00:00:00.000Z","tags":[{"inline":false,"label":"IDE","permalink":"/notes/blog/tags/IDE","description":"Integrated Development Environment"},{"inline":false,"label":"Neovim","permalink":"/notes/blog/tags/Neovim","description":"Neovim"}],"readingTime":5.815,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"Neovim","title":"Neovim IDE","authors":["rakesh"],"tags":["IDE","Neovim"]},"unlisted":false,"prevItem":{"title":"Dotfiles","permalink":"/notes/blog/dotfiles"},"nextItem":{"title":"Quantum Machine Learning","permalink":"/notes/blog/QuantumML"}},"content":"An Integrated Development Environment (IDE) provides a comprehensive list of features like code editor, compiler/interpreter, code completion, debugger and much more.\\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\nModern editors work out of the box. Just install, launch and you are ready to go! But sometimes, it is slow and clunky eating up a lot of memory.\\n\\nThen there are terminal based IDEs focusing on pure text manipulation. No fancy interfaces, only powered by keyboard shortcuts and macros. One such IDE is Vim (**Vi** i**M**proved)\\n\\nIn this blog, I have shared my Neovim setup, plugins and keyboard mappings. This configuration supports various programming languages like `Lua`, `Python for Data Science & ML`, `OCaml`, `Rust`, `Web Development`, even Notes and Journaling. Detailed documentation is available on my [github](https://github.com/rvbug/neovim).\\n\\nRead on!\\n\\n# Popular IDEs\\n\\nHere\'s a 2023 stack overflow survey of the most popular IDEs. Vim is still in the top 5 considering that it was first released in 1991. ([Link](https://survey.stackoverflow.co/2023/#most-popular-technologies-new-collab-tools) to the survey)\\n\\n![Stackoverflow Survey]![alt text](img/Survey.png)\\n\\n---\\n\\n## Vi Improved (Vim)\\n\\nVim was created by Bram Moolenaar. A highly customizable modal, screen-based text editor written in C and Vim Script (VimL). It is known to be very fast, extremely efficient, highly configurable, supporting many programming languages and providing extensive plugin support.\\n\\n![VIM](img/Vim.png)\\n\\n## Neovim (Nvim)\\n\\nNeoVim was released in 2014 and is a fork of Vim. It uses a fast, small and lightweight scripting language called [Lua](https://www.lua.org/).\\n\\nLua supports procedural, object-oriented and functional programming. It can be embedded into C/C++ quite easily too.\\n\\nThe goal of this project was not to replace Vim but to extend and incorporate additional features. Almost all of the features of vim are now supported by Neovim.\\n\\n`Note`: I have used Neovim and nvim interchangeably to refer to Neovim.\\n\\n(This splash screen is because of a plugin called [Alpha](https://github.com/goolord/alpha-nvim))\\n\\n![Neovim](img/neovim.png)\\n\\n---\\n\\n# Configuration\\n\\nThe main configuration directory of nvim is `$XDG_CONFIG_HOME/nvim/` pointing to this folder - `~/.config/nvim/`\\n\\n`Note`: Lua is integrated as the first-class language for Neovim v5.1 and above.\\n\\n```bash\\n# nvim looks for this file when it loads\\n~/.config/nvim/init.lua \\n\\n# if this folder and files does not exist, create them\\n> mkdir .config\\n> cd .config\\n> mkdir nvim \\n> cd nvim \\n> touch init.lua\\n```\\n\\n## File structure\\n\\nI have used [VimPlug](https://github.com/junegunn/vim-plug) and [Packer](https://github.com/wbthomason/packer.nvim) but I found [Lazy](https://github.com/folke/lazy.nvim) package manager to be well organized for my needs.\\n\\nHere\'s how my files are structured. Anything inside of plugins folder gets called automatically. Details of these files are available on my [github](https://github.com/rvbug/neovim?tab=readme-ov-file#folder-structure).\\n\\n```bash\\n~/.config/nvim/\\n\u251c\u2500\u2500 LICENSE  \\n\u251c\u2500\u2500 README.md  \\n\u251c\u2500\u2500 init.lua  \\n\u251c\u2500\u2500 lazy-lock.json\\n\u251c\u2500\u2500 lua\\n\u2502\xa0\xa0 \u251c\u2500\u2500 keymaps.lua\\n\u2502\xa0\xa0 \u2514\u2500\u2500 plugins\\n\u2502\xa0\xa0     \u251c\u2500\u2500 autopairs.lua\\n\u2502\xa0\xa0     \u251c\u2500\u2500 comments.lua\\n\u2502\xa0\xa0     \u251c\u2500\u2500 completions.lua\\n\u2502\xa0\xa0     \u251c\u2500\u2500 db.lua\\n\u2502\xa0\xa0     \u251c\u2500\u2500 debug.lua\\n\u2502\xa0\xa0     \u251c\u2500\u2500 floating-help.lua\\n\u2502\xa0\xa0     \u251c\u2500\u2500 gitblame.lua\\n\u2502\xa0\xa0     \u251c\u2500\u2500 gitsigns.lua\\n\u2502\xa0\xa0     \u251c\u2500\u2500 greetings.lua\\n\u2502\xa0\xa0     \u251c\u2500\u2500 iron.lua\\n\u2502\xa0\xa0     \u251c\u2500\u2500 lspconfig.lua\\n\u2502\xa0\xa0     \u251c\u2500\u2500 lualine.lua\\n\u2502\xa0\xa0     \u251c\u2500\u2500 markdown.lua\\n\u2502\xa0\xa0     \u251c\u2500\u2500 neorg.lua\\n\u2502\xa0\xa0     \u251c\u2500\u2500 noice.lua\\n\u2502\xa0\xa0     \u251c\u2500\u2500 none-ls.lua\\n\u2502\xa0\xa0     \u251c\u2500\u2500 nvim_tree.lua\\n\u2502\xa0\xa0     \u251c\u2500\u2500 quarto.lua\\n\u2502\xa0\xa0     \u251c\u2500\u2500 telescope.lua\\n\u2502\xa0\xa0     \u251c\u2500\u2500 themes.lua\\n\u2502\xa0\xa0     \u251c\u2500\u2500 toggleterm.lua\\n\u2502\xa0\xa0     \u251c\u2500\u2500 treesitter.lua\\n\u2502\xa0\xa0     \u2514\u2500\u2500 zen.lua\\n\u2514\u2500\u2500 yarn.lock\\n```\\n\\n# Terminal & Tmux\\n\\nI use [Wezterm](https://wezfurlong.org/wezterm/index.html) terminal. It is written in rust and supports lua based configuration. [`.wezterm.lua`](https://github.com/rvbug/neovim/blob/main/.wezterm.lua) contains my basic setup.\\n\\nUsing Tmux (Terminal Multiplexer) helps me with multiple sessions, each with multiple windows (tabs). It even allows one to save and restore the sessions.\\n\\nYou can find the configuration in this [`.tmux.conf`](https://github.com/rvbug/neovim/blob/main/.tmux.conf) file.\\n\\n![TMUX](img/tmux.png)\\n\\n## Init file\\n\\n`init.lua` is the first file that gets loaded when Neovim starts. I am loading `keymaps.lua` and Lazy package manager via this.\\n\\n## Keymaps\\n\\n`keymaps.lua` is where I have configured my keyboard shortcuts. Look at a few sample keybindings, these are pretty standard ones.\\n\\n```bash\\n# global leader key is <Space> \\nvim.g.mapleader = \\" \\"\\n\\n# in normal mode, use <space>v or <space>sh to run specific commands \\n\\n# to split the screen vertically\\nkeymap.set(\\"n\\", \\"<leader>sv\\" , \\"<C-w>v\\")\\n# splits screen horizontally\\nkeymap.set(\\"n\\", \\"<leader>sh\\" , \\"<C-w>s\\")\\n\\n## Telescope keymapings\\n\\n# find files \\nkeymap.set(\\"n\\", \\"<leader>ff\\", \\"<cmd>Telescope find_files<cr>\\") \\n# live grep \\nkeymap.set(\\"n\\", \\"<leader>fg\\", \\"<cmd>Telescope live_grep<cr>\\") \\n# find string under cursor \\nkeymap.set(\\"n\\", \\"<leader>fc\\", \\"<cmd>Telescope grep_string<cr>\\") \\n# display list of open buffers\\nkeymap.set(\\"n\\", \\"<leader>fb\\", \\"<cmd>Telescope buffers<cr>\\")\\n```\\n\\n## Theme\\n\\nThere are many color schemes to choose from, I have been using [Tokyo Dark](https://github.com/tiagovla/tokyodark.nvim) and the configuration is loaded via `themes.lua` file.\\n\\n![Theme](img/theme.png)\\n\\n## Telescope\\n\\nA fuzzy finder plugin that helps you to find files and supports live grep along with many other features.\\n\\n![telescope](img/telescope.png)\\n\\n# Code Completion\\n\\n`completions.lua` has the following plugins to support code completions and snippets using LSP ([Language Server Protocol](https://microsoft.github.io/language-server-protocol/)).\\n\\n```bash\\n# client for LSP.\\n\\"hrsh7th/cmp-nvim-lsp\\", \\n# snippet engine\\n\\"L3MON4D3/LuaSnip\\",\\n# completion source for nvim-cmp\\n\\"saadparwaiz1/cmp_luasnip\\",\\n# additional snippet support \\n\\"rafamadriz/friendly-snippets\\",\\n```\\n\\n#### ***Here\'s the completion and snippets for Python, Lua and Ocaml***\\n\\n![Python](img/completion.png)\\n![Lua](img/completion2.png)\\n![OCaml](img/completion3.png)\\n\\nThe following plugins help with path and command completion too.\\n\\n```bash\\n# path completion\\n\\"hrsh7th/cmp-path\\",\\n# command line completion\\n\\"hrsh7th/cmp-cmdline\\",\\n```\\n\\n*Path Completion*\\n\\n![PathCompletion](img/path.png)\\n\\n*Command Completion*\\n\\n![Command](img/command.png)\\n\\n## Quarto Support\\n\\nQuarto is heavily used in scientific publishing, my setup supports them.\\n\\n![Quarto](img/Quarto1.png)\\n\\n`Did you know:` Quarto also has great integration with Jupyter Notebook, see the sample below.\\n\\n`The one on the left is the Quarto html format generated from the Jupyter notebook on your right.`\\n\\n![QuartoJupyter](img/QuartoJupyter.png)\\n\\n## Git Integration\\n\\nFor git integration I use\\n\\n#### Lazygit\\n\\n![Lazu](img/Lazygit.png)\\n\\n#### Gitblame\\n\\n![GitBlame](img/Gitblame.png)\\n\\n## DAP (Debug Adapter Protocol)\\n\\nShowing DAP support for Python\\n\\n![DAP](img/dap.png)\\n\\n# Next steps\\n\\nHow do you learn Neovim, I have added the references to some amazing documentation [here](https://github.com/rvbug/neovim?tab=readme-ov-file#references). You can also get extensive help right inside Neovim.\\n\\nCheckout `:Tutor`\\n\\n![Tutor](img/tutor.png)\\n\\nType `:help` to show up entire documentation but you can also go to specific ones by typing `:help usr_0` or use [Neovim Website](https://neovim.io/doc/user/index.html) to read through it.\\n\\n\\n![TutorHelp](img/tutorhelp.png)\\n\\n# Alternatives\\n\\nIf you decide to try Neovim but do not want to go through the hassle of configuring all by yourself then check out these interesting \\"starter kit\\" projects.\\n\\nAll these projects have extensive documentation on their website but before you try them, backup your existing nvim file under `~/.config/` folder if it exists.\\n\\n```bash\\n> cd ~/.config\\n> mv nvim nvim_backup\\n```\\n\\n### [LazyVim](https://www.lazyvim.org/)\\n\\nSupports Mac/Linux based systems.\\n\\n```bash\\ngit clone https://github.com/LazyVim/starter ~/.config/nvim\\n```\\n\\n### [NVChad](https://nvchad.com/)\\n\\nTo install on Mac/Linux based system, use the following command:\\n\\n```bash\\ngit clone https://github.com/NvChad/NvChad ~/.config/nvim --depth 1 && nvim\\n```\\n\\n### [LunarVim](https://www.lunarvim.org/)\\n\\nTo install, use the following command\\n\\n```bash\\nLV_BRANCH=\'release-1.3/neovim-0.9\' bash <(curl -s https://raw.githubusercontent.com/LunarVim/LunarVim/release-1.3/neovim-0.9/utils/installer/install.sh)\\n```\\n\\n# Conclusion\\n\\nYes, it takes time to configure these IDEs but is it really worth the effort? That is the question you need to answer for yourself. If you are unable to decide then I would suggest give it a try.\\n\\nNeovim has helped me to design the workflow that I need to get to a complete \'mouse-less\' environment.\\n\\nCheck out my [configuration](https://github.com/rvbug/neovim) and leave a comment if you have had any similar experiences. Reach out if you have any questions, I\'d be happy to help!\\n\\n***Happy Coding!!***\\n\\n# References\\n\\n[My Configuration (Github)](https://github.com/rvbug/neovim/)\\n\\n[Official VI](https://quarto.org/docs/publishing/rstudio-connect.html)[M](https://quarto.org/docs/publishing/confluence.html)\\n\\n[VIM](https://en.wikipedia.org/wiki/Vim_(text_editor))[Wiki](https://pandoc.org/)\\n\\n[Officia](https://quarto.org/docs/publishing/confluence.html)[l Neovim](https://neovim.io/)\\n\\n[Vim Cheat Sheet](https://linuxhandbook.com/vim-cheat-sheet/)\\n\\n[Lua Cheat Sheet](https://devhints.io/lua)"},{"id":"QuantumML","metadata":{"permalink":"/notes/blog/QuantumML","source":"@site/blog/2024-04-21/QuantumComputing.md","title":"Quantum Machine Learning","description":"My love for Quantum Physics rekindled in 2017 while studying the foundation of mathematics used in Machine Learning and Deep Learning.","date":"2024-04-21T00:00:00.000Z","tags":[{"inline":false,"label":"Quantum Computing","permalink":"/notes/blog/tags/Quantum-Computing","description":"Quantum Computing"},{"inline":false,"label":"ML","permalink":"/notes/blog/tags/ML","description":"Machine Learning"}],"readingTime":5.515,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"QuantumML","title":"Quantum Machine Learning","authors":["rakesh"],"tags":["Quantum","ML"]},"unlisted":false,"prevItem":{"title":"Neovim IDE","permalink":"/notes/blog/Neovim"},"nextItem":{"title":"Cookie ML","permalink":"/notes/blog/CookieCutterML"}},"content":"My love for Quantum Physics rekindled in 2017 while studying the foundation of mathematics used in Machine Learning and Deep Learning. \\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\nAnother interesting branch gaining momentum is Quantum Machine Learning (QML) - an intersection between Quantum Physics and ML. QML can speed up training and evaluation of ML models and, in turn, help develop new quantum algorithms. This blog is an introduction to QML.\\n\\n\\n![alt text](img/intersection.png)\\n\\n\\nSource code provided in this article is available in my [**Github repo**](https://github.com/rvbug/q-gravity/)**.**\\n\\n---\\n\\n# Introduction\\n\\nRapid progress has been made in Quantum Computing to harness the law of [Quantum Mechanics](https://en.wikipedia.org/wiki/Quantum_mechanics) and the problems (NP or NP-Hard) that classical computers alone cannot solve, even with arrays of GPUs and TPUs at our disposal.\\n\\nThat being said, Quantum Computers is not going to replace classical systems any time soon but will coexist (for now). Companies like IBM, Google, Microsoft, Rigetti, D-Wave continue to make rapid progress to achieve quantum supremacy. [IBM Qiskit]((https://www.ibm.com/quantum)) and [AWS](https://aws.amazon.com/braket/) even offer free but limited access to their Quantum devices.\\n\\n---\\n\\n# Motivation\\n\\nWe are still a few years away from making quantum computers commercially available so then why learn it now?\\n\\nUnderstanding Quantum Mechanics will take time; you need to gain knowledge of a lot of complex topics - a few laws of classical physics do not even hold true at the subatomic level.\\n\\nHere are some interesting facts of Quantum Mechanics:\\n\\n1&gt; **\\"Spooky action at a distance\\"** is a famous quote by Albert Einstein when he found that one particle\'s behaviour affects the other even if they are separated by vast distances.\\n\\nImage Credit - [Futurism](https://futurism.com)\\n\\n![spooky action]![alt text](img/spooky.png)\\n\\n2&gt; There is a famous [double slit](https://en.wikipedia.org/wiki/Double-slit_experiment) experiment showing particle-wave duality i.e. light and matter sometime act as particles and at other time, as waves.\\n\\nImage Credit - [Wiki](https://en.wikipedia.org/wiki/Double-slit_experiment)\\n\\n![Double-slit experiment - Wikipedia]![alt text](img/doubleslit.png)\\n\\n3&gt; Quantum Computing could be one of the biggest breakthroughs ever known to mankind :-\\n\\n* Speeding up drug development\\n    \\n* Solving complex optimisation problems\\n    \\n* Forecasting weather conditions with greater accuracy.\\n    \\n\\n4&gt; Just to give you an idea of what it can achieve - it will take millions of years to find the prime factors of a **2048-bit** number using classical computers but with quantum computers, it can be calculated within minutes. There is an active research in the field of [Quantum Cryptography](https://en.wikipedia.org/wiki/Quantum_cryptography) and [Post-Quantum Cryptography](https://en.wikipedia.org/wiki/Post-quantum_cryptography) .\\n\\n### **Still not convinced?**\\n\\nWell, you will get to see our universe through a completely different lens, appreciate nature\'s grand design and our existence - realising how little we know about it!\\n\\n> Beauty of unknown...\\n\\n---\\n\\n# Prerequisites\\n\\nTo get started in QML requires knowledge of (but not limited to) :\\n\\n* Quantum Mechanics\\n    \\n* Python\\n    \\n* Mathematics\\n    \\n* Deep Learning\\n    \\n\\nBut being `Passionate` alone will give you ***superpowers*** to digest these extremely complex topics.\\n\\n---\\n\\n# PennyLane\\n\\nIn this article, we will use Xanadu\'s PennyLane for programming Quantum Computers. It is a cross-platform Python library that enables the training of quantum programs.\\n\\nPennyLane can connect to powerful ML frameworks like JAX, Pytorch and TensorFlow. It manages execution by passing information between classical and quantum computers.\\n\\nHere\'s how you can install PennyLane on a virtual environment:\\n\\n```python\\n# preferrably in a virtual env\\n$> pip install pennylane\\n```\\n\\n---\\n\\n# Basics\\n\\nBefore we get to programming, here are some basics you should know.\\n\\n## Qubits\\n\\nIn classical computers a bit can be either in a 0 or 1 state.\\n\\nBut Qubits can not only be in 0 or 1 but can also be in both states simultaneously - this is known as [***superposition***](https://en.wikipedia.org/wiki/Quantum_superposition)***.***\\n\\n## Bloch Sphere\\n\\nThe state of the qubits is represented using Bloch Sphere and it helps to understand the behaviour of quantum circuits and algorithms.\\n\\nImage Credit - [Wiki](https://en.wikipedia.org/wiki/Bloch_sphere)\\n\\n![Bloch sphere - Wikipedia]![alt text](img/bloch.png)\\n\\n## Gates\\n\\nGates helps us to manipulate the state of the qubits. Some gates can be controlled and adjusted like a tuning nob. The state of the qubits can be changed by rotating parametrised gates around the X, Y, and Z axes. **The rotation is explained using these formulas.**\\n\\n\\n## Ket\\n\\nKet vector represents the state of a quantum particle and is denoted as:\\n\\n\\nProgrammatically , it can be represented as an array as show below:\\n\\n```python\\nimport numpy as np\\n\\nk0 = np.array([1,0]) # ket 0\\nk1 = np.array([0,1]) # ket 1\\n\\nk0, k1\\n\\n# output\\n#(array([1, 0]), array([0, 1]))\\n```\\n\\n## Quantum Neural Network\\n\\nEquivalent to creating a Neural Network for Deep Learning on classical computers, QML uses trainable quantum circuits which should be **differentiable**.\\n\\n![Quantum Circuit] ![alt text](img/QNN.png)\\n\\n> A function f(x) is said to be ***differentiable*** if a **derivate** exists for that function.\\n\\n# The Script\\n\\n#### Libraries\\n\\n```python\\nimport pennylane as qml\\nfrom pennylane import numpy as np\\nimport matplotlib.pyplot as plt\\n```\\n\\n#### Device Definition & Quantum Function\\n\\n```python\\n# defining a vey simple circuit\\ndev = qml.device(\'default.qubit\', wires=1)\\n@qml.qnode(dev)\\ndef circuit(x):\\n    qml.RX(x[0], wires=0)\\n    qml.RY(x[1], wires=0)\\n    qml.PauliX(wires=0)\\n    qml.PauliY(wires=0)\\n    return qml.expval(qml.PauliZ(0))\\n```\\n\\n#### Visualizing Circuit\\n\\n```python\\nfig, ax = qml.draw_mpl(circuit)([0.543, 0.4])\\nplt.show()\\n```\\n\\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1713329626629/ec899477-097f-4068-80ae-592749078b51.png align=\\"left\\")\\n\\n#### Cost & Gradient calculations\\n\\n```python\\n# Cost function\\ndef cost(x):\\n  return my_quantum_function(x)\\n\\n# uses automatic differentiation \\ninit_params = np.array([0.011, 0.012], requires_grad=True)\\nprint(cost(init_params))\\n\\n# initialise the optimizer\\nopt = qml.GradientDescentOptimizer(stepsize=0.4)\\n\\n# ...\\n#...\\n```\\n\\n# Complete Script\\n\\nYou can find the entire script here - [**Github repo**](https://github.com/rvbug/q-gravity/)**.**\\n\\n```python\\nimport pennylane as qml\\nfrom pennylane import numpy as np\\nimport matplotlib.pyplot as plt\\n\\ndev = qml.device(\'default.qubit\', wires=1)\\n\\n@qml.qnode(dev)\\ndef circuit(x):\\n    qml.RX(x[0], wires=0)\\n    qml.RY(x[1], wires=0)\\n    qml.PauliX(wires=0)\\n    qml.PauliY(wires=0)\\n    return qml.expval(qml.PauliZ(0))\\n\\nfig, ax = qml.draw_mpl(circuit)([0.543, 0.4])\\nplt.show()\\n\\nresult = circuit([0.543, 0.4])\\nprint(result)\\n\\nd = qml.grad(circuit, argnum=0)\\nd([0.543, 0.4])\\n\\nfig, ax = qml.draw_mpl(circuit)([0.543, 0.4])\\nplt.show()\\n\\ndef cost(x):\\n  return circuit(x)\\n\\ninit_params = np.array([0.011, 0.012], requires_grad=True)\\nprint(cost(init_params))\\n\\n\\n# initialise the optimizer\\nopt = qml.GradientDescentOptimizer(stepsize=0.4)\\n\\n# set the number of steps\\nsteps = 100\\n# set the initial parameter values\\nparams = init_params\\n\\nfor i in range(steps):\\n    # update the circuit parameters\\n    params = opt.step(cost, params)\\n\\n    if (i + 1) % 20 == 0:\\n        print(\\"Cost after step {:5d}: {: .7f}\\".format(i + 1, cost(params)))\\n\\nprint(\\"Optimized rotation angles: {}\\".format(params))\\n```\\n\\n# Conclusion\\n\\nThis blog gives a very basic information of QML and what the future might look like. If you are interested in delving deeper into this vertical, then I will leave some references below.\\n\\nWhat is the best way to end an article than with a quote from [Niels Bohr](https://en.wikipedia.org/wiki/Niels_Bohr) :-\\n\\n> **If quantum mechanics hasn\'t profoundly shocked you, you haven\'t understood it yet...**\\n\\n# References\\n\\n[Quantum Computing Race](https://www.businessofbusiness.com/articles/whos-winning-the-quantum-computing-race-china-and-the-us-are-neck-and-neck/)\\n\\n[Google Cirq](https://quantumai.google/cirq)\\n\\n[TensorFlow Quantum](https://www.tensorflow.org/quantum)\\n\\n[Quantum Superposition](https://en.wikipedia.org/wiki/Quantum_superposition)\\n\\n[PennyLane](https://pennylane.ai/)\\n\\n[Math is fun](https://www.mathsisfun.com/)\\n\\n[Quantum Cryptography](https://en.wikipedia.org/wiki/Quantum_cryptography)\\n\\n[Post Quantum Cryptography](https://en.wikipedia.org/wiki/Post-quantum_cryptography)\\n\\n[Quantum Gravity - Github Repo](https://github.com/rvbug/q-gravity/)"},{"id":"CookieCutterML","metadata":{"permalink":"/notes/blog/CookieCutterML","source":"@site/blog/2024-04-21/cookiecutterv2.0.md","title":"Cookie ML","description":"I go by one simple principle","date":"2024-04-21T00:00:00.000Z","tags":[{"inline":false,"label":"ML","permalink":"/notes/blog/tags/ML","description":"Machine Learning"}],"readingTime":3.645,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"CookieCutterML","title":"Cookie ML","authors":["rakesh"],"tags":["ML"]},"unlisted":false,"prevItem":{"title":"Quantum Machine Learning","permalink":"/notes/blog/QuantumML"},"nextItem":{"title":"Quantum Gravity - Intestellar","permalink":"/notes/blog/QuantumGravity"}},"content":"I go by one simple principle\\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\n> If you repeat the same steps twice, then automate it the third time.\\n\\nCouple of years back, I wrote blog on how to :\\n\\n1&gt; Structure your Data Science and ML project - [Link here](https://rvbug.hashnode.dev/structuring-data-science-and-ml-projects)\\n\\n2&gt; Automating ML structure using `Make` and a simple `Python Script` - [Link here](https://rvbug.hashnode.dev/ml-cookie-cutter).\\n\\nIn this follow-up article, I will provide an update to the cookie-cutter project which now uses `yaml` making it far more flexible and easy to use.\\n\\n# Project Structure\\n\\nHere\'s the structure I use for all my ML projects.\\n\\nYou can create a different one but these are pretty standard for most of the projects.\\n\\n![](https://private-user-images.githubusercontent.com/10928536/294880896-e0785d48-c21b-42c6-84a7-de211e6687ca.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTI5ODQxNTAsIm5iZiI6MTcxMjk4Mzg1MCwicGF0aCI6Ii8xMDkyODUzNi8yOTQ4ODA4OTYtZTA3ODVkNDgtYzIxYi00MmM2LTg0YTctZGUyMTFlNjY4N2NhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTMlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDEzVDA0NTA1MFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWUxOGU3OTMxMjIzMjJmN2UyMWRiMzBmNDI2YWMwYWQ0ZWU1NGM4YzY0OWQ1ZjY1ZmFjNzNlYzZlOWM3MjgxOGYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.usv2boxCTmYm2_bIrPdwqCONB49vgPjyGkoY2RUN3CE align=\\"left\\")\\n\\n## YAML\\n\\nYAML is **a text based data serialization language for managing your configuration files**. YAML stands for `Yet Another Markup Language` or `YAML ain\'t markup language`. You can read more about it [here](https://en.wikipedia.org/wiki/YAML).\\n\\nHere\'s the YAML format used in my project.\\n\\n```yaml\\n  # support for data version control\\n  - .dvc:\\n\\n  # if you plan to use docker container\\n  - docker:\\n    - Dockerfile\\n\\n  # basic ML development files\\n  - src:\\n    - __init__.py\\n    - data_pipeline.py\\n    - data_processing.py\\n\\n  ####\\n```\\n\\n# Script\\n\\nOne of the main pre-requisites for this script is `pyyaml`. You can find the documentation [here](https://pyyaml.org/wiki/PyYAMLDocumentation). Install them via [pip](https://pip.pypa.io/en/stable/).\\n\\n## Argparse\\n\\nEverything starts with the main function. It uses argparse to parse command line arguments provided to the script. It then loads the yaml config file.\\n\\n```python\\n# parse the arguments provided on the command line\\nargs = parse_args()\\n\\n# Load yaml config\\nconfig = load_config()\\n```\\n\\nThe parser will look for specific flags and take action. More about that in the Help section below.\\n\\n```python\\nparser.add_argument(\\"--n\\", \\"--name\\", #...)\\nparser.add_argument(\\"--p\\", \\"--path\\",  #...) \\nparser.add_argument(\\"--c\\", \\"--config\\",  #...)\\nparser.add_argument(\\"--v\\", \\"--venv\\",  #...)\\n```\\n\\n## File and Directories\\n\\nBased on how YAML is structured, the script will create files, directories and sub-directories via a function `create_directories`\\n\\n```python\\ndef create_directories(project_path, config):\\n\\nif isinstance(config, str):\\n        item_path = os.path.join(project_path, config)\\n        with open(item_path, \\"w\\"):\\n            pass  # empty file\\n\\n#...\\n```\\n\\n## Virtual env\\n\\nIf you provide flag to create a [virtual environment](https://docs.python.org/3/library/venv.html), the script will create one for you. There is a function called `create_virtual_env` to do exactly that.\\n\\n```python\\ndef create_virtual_env(project_path, activate=True):\\n#...\\nif not os.path.exists(venv_path):\\n        venv.create(venv_path, with_pip=True)\\n#...\\n```\\n\\n## Help\\n\\nWhen you use `-h` flag, it will show you how to use the script.\\n\\nIf you want to give a specific name to your project use `--n or N` flag. If not, it will create a default directory called `ml-cookie-cutter.`\\n\\nSpecify the path where the project needs to be created using `--p or P` flag.\\n\\nFinally, if you want the script to create a virtual environment for you, go ahead and use the `--v or --venv` flag. By default, the name of the environment is `venv`.\\n\\n```bash\\n\\n$> python3 main.py --h\\n\\n# usage: ML Cookie Cutter [-h] [--n N] [--p P] [--c C] [--v]\\n\\n# Creates ML project cookie cutter structure\\n\\n#optional arguments:\\n#  -h, --help       show this help message and exit\\n#  --n N, --name N  Name of the directory to be created, default = ml-cookie-cutter\\n#  --p P, --path P    provide the path where, default is $HOME dir\\n#  --v, --venv        create a virtual env. [ignore if you are already on a virtual env]\\n\\n# Enjoy and happy coding\\n```\\n\\n## Final step\\n\\nOnce you have the virtual environment set up, you can activate it as shown below. You should see that it in your command prompt; go ahead and start installing all your Data science and ML libraries.\\n\\nFor deactivating simply type `deactivate`\\n\\n```bash\\n# go to the project folder\\n$> cd ml-cookie-cutter\\n# activate the environment \\n$> source venv/bin/activate \\n\\n# venv is activated \\n(venv) $> pip install numpy pandas pytorch seaborn notebook\\n\\n# To deactive use the following command\\n(venv) $> deactivate\\n$>\\n```\\n\\nOn windows if you use PowerShell activate using `Activate.ps1`\\n\\n# Conclusion\\n\\nThis article shows how you can use this cookie-cutter project and create a ML structure with a simple command-line tool. Enjoy and have fun learning!\\n\\nThis is how I have used the cookie-cutter project to create a ML structure with a simple command-line tool. Enjoy and have fun learning!\\n\\nHappy coding!\\n\\n# References\\n\\n[Github - Complete Code](https://github.com/rvbug/cookie-ml/)\\n\\n[Pyyaml Documentation](https://pypi.org/project/PyYAML/)\\n\\n[Installing Python](https://www.python.org/)\\n\\n[PIP Installation](https://pip.pypa.io/en/stable/)\\n\\n[How to structure your ML projects](https://rvbug.hashnode.dev/ml-cookie-cutter)\\n\\n[Automating ML structure (v 1.0)](https://rvbug.hashnode.dev/structuring-data-science-and-ml-projects)\\n\\n[Documentation on Virtual Env](https://docs.python.org/3/library/venv.html)"},{"id":"QuantumGravity","metadata":{"permalink":"/notes/blog/QuantumGravity","source":"@site/blog/2024-04-21/interstellar-science-blog.md","title":"Quantum Gravity - Intestellar","description":"Introduction","date":"2024-04-21T00:00:00.000Z","tags":[{"inline":true,"label":"Quantum Gravity","permalink":"/notes/blog/tags/quantum-gravity"}],"readingTime":2.985,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"QuantumGravity","title":"Quantum Gravity - Intestellar","authors":["rakesh"],"tags":["Quantum Gravity"]},"unlisted":false,"prevItem":{"title":"Cookie ML","permalink":"/notes/blog/CookieCutterML"},"nextItem":{"title":"Nix Introduction","permalink":"/notes/blog/Nix OS"}},"content":"## Introduction\\n\\nWhen Christopher Nolan\'s film \\"Interstellar\\" hit theaters, it wasn\'t just another sci-fi movie\u2014it was a scientifically rigorous exploration of some of the most mind-bending concepts in modern physics. Behind the film\'s stunning visuals was renowned theoretical physicist Kip Thorne, who ensured that the movie\'s scientific foundations were as accurate as possible.\\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\n## Black Holes: Cosmic Enigmas\\n\\nOne of the most fascinating aspects of the book is its deep dive into black holes. Thorne explains these cosmic phenomena not as simple \\"cosmic vacuum cleaners,\\" but as complex gravitational systems that warp the very fabric of spacetime.\\n\\n### The Mathematics of Black Holes\\n\\nAt the heart of black hole physics is Einstein\'s field equation:\\n\\n*G*\u03bc\u03bd = 8\u03c0G/*c*\u2074 *T\u03bc\u03bd*\\n\\nWhere:\\n- *G*\u03bc\u03bd represents the curvature of spacetime\\n- *T\u03bc\u03bd* describes the distribution of matter and energy\\n- G is the gravitational constant\\n- *c* is the speed of light\\n\\nThis equation essentially tells us that mass and energy bend spacetime, creating what we experience as gravity.\\n\\n## Wormholes: Cosmic Shortcuts\\n\\nThorne explores the theoretical possibility of wormholes\u2014hypothetical tunnels through space-time that could create shortcuts for long journeys across the universe. While purely theoretical, the mathematics suggests they\'re not impossible.\\n\\nA simplified wormhole equation can be represented as:\\n\\n*d* = *c* * *t* / \u221a(1 - *v*\xb2/(*c*\xb2))\\n\\nWhere:\\n- *d* is distance\\n- *c* is the speed of light\\n- *t* is time\\n- *v* is velocity\\n\\n## Time Dilation: Not Just Science Fiction\\n\\nOne of the most mind-bending concepts in the book is time dilation\u2014how time can move differently depending on gravitational forces and velocity. Near a massive object like a black hole, time actually slows down relative to distant observers.\\n\\nThe time dilation factor can be calculated using:\\n\\n*t*\' = *t* / \u221a(1 - *v*\xb2/(*c*\xb2))\\n\\nWhere:\\n- *t*\' is the dilated time\\n- *t* is the proper time\\n- *v* is velocity\\n- *c* is the speed of light\\n\\n## Gravitational Waves: Ripples in Spacetime\\n\\nThorne was instrumental in the scientific understanding of gravitational waves\u2014ripples in the fabric of spacetime caused by massive cosmic events. These waves were theoretically predicted by Einstein and first directly observed in 2015, vindicating decades of scientific prediction.\\n\\nThe amplitude of a gravitational wave can be described by:\\n\\n*h* = 4G*M*/*c*\u2074*r*\\n\\nWhere:\\n- *h* is the wave amplitude\\n- *G* is the gravitational constant\\n- *M* is the mass of the generating object\\n- *c* is the speed of light\\n- *r* is the distance from the source\\n\\n## Beyond the Equations: A Human Story\\n\\nWhat makes Thorne\'s book remarkable is how it bridges pure scientific theory with human imagination. It demonstrates that the most complex scientific concepts can be understood with patience, curiosity, and a sense of wonder.\\n\\n## Conclusion\\n\\n\\"The Science of Interstellar\\" is more than a companion to a movie\u2014it\'s a gateway to understanding some of the most profound mysteries of our universe. It shows us that reality can be stranger and more beautiful than fiction.\\n\\n### Key Takeaways\\n- Black holes are not simple cosmic vacuums but complex spacetime phenomena\\n- Wormholes, while theoretical, are mathematically possible\\n- Time is relative and can be dramatically affected by gravity and velocity\\n- Gravitational waves confirm some of Einstein\'s most radical predictions\\n\\n## About the Author\\n\\nKip Thorne is not just a physicist, but a scientific consultant who ensured that \\"Interstellar\\" pushed the boundaries of scientific accuracy in cinema. His work bridges theoretical physics with popular understanding.\\n\\n*Disclaimer: While these equations are simplified, they provide a glimpse into the mathematical foundations of these complex cosmic phenomena.*"},{"id":"Nix OS","metadata":{"permalink":"/notes/blog/Nix OS","source":"@site/blog/2023-03-05/Nix.md","title":"Nix Introduction","description":"Introduction to Nix Functional programming","date":"2023-03-05T00:00:00.000Z","tags":[{"inline":false,"label":"nix","permalink":"/notes/blog/tags/nix","description":"Nix OS and Functional Package Manager"}],"readingTime":2.245,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"Nix OS","title":"Nix Introduction","authors":["rakesh"],"tags":["nix"]},"unlisted":false,"prevItem":{"title":"Quantum Gravity - Intestellar","permalink":"/notes/blog/QuantumGravity"},"nextItem":{"title":"Nix Pkg Manager","permalink":"/notes/blog/Nix Packages"}},"content":"Introduction to Nix Functional programming\\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\n\\n##  Core Concepts\\n\\n### 1. Immutability\\n\\nIn Nix, variables are immutable once defined:\\n\\n```nix\\nlet\\n  x = 5;\\n  # x = 6;  # This would cause an error\\nin x\\n```\\n\\n### 2. Let Bindings\\nNix uses let for local variable definitions:\\n```nix\\nlet\\n  greeting = \\"Hello\\";\\n  name = \\"World\\";\\nin greeting + \\" \\" + name\\n```\\n\\n### 3. Functions and Lambda Expressions\\nBasic Function Definition\\n\\n```nix\\nlet\\n  greet = name: \\"Hello, \\" + name;\\nin greet \\"Alice\\"  # Returns \\"Hello, Alice\\"\\n```\\n\\n### Multiple Parameter Functions\\n```nix\\nlet\\n  multiply = x: y: x * y;\\n  result = multiply 3 4;  # Returns 12 in result\\n```\\n\\n### 4. Pattern Matching and Attribute Sets\\nAttribute Set Creation\\n\\n```nix\\n{\\n  name = \\"John\\";\\n  age = 30;\\n  skills = [\\"Nix\\" \\"Functional Programming\\"];\\n}\\n```\\n\\n### Accessing Attributes\\n```nix\\nlet\\n  person = { name = \\"Alice\\"; age = 25; };\\nin person.name  # Returns \\"Alice\\"\\n```\\n\\n\\n### 5. Function with Attribute Sets\\n```nix\\nlet\\n  greet = { name, age }: \\"Hello ${name}, you are ${toString age}\\";\\n  \\n  result = greet { \\n    name = \\"Bob\\"; \\n    age = 35; \\n  }\\nin result\\n```\\n\\n### 6. Recursion and Recursive Sets\\n```nix\\nlet\\n  fibonacci = rec {\\n    fib0 = 0;\\n    fib1 = 1;\\n    fib2 = fib0 + fib1;\\n    fib3 = fib1 + fib2;\\n    # Continues recursively\\n  };\\nin fibonacci\\n```\\n\\n\\n### 7. Importing and Modules\\n```nix\\nlet\\n  # Importing another Nix file\\n  utils = import ./utils.nix;\\nin utils.someFunction\\n```\\n\\n### 8. List Comprehensions\\n```nix\\nlet\\n  numbers = [1 2 3 4 5];\\n  squared = map (x: x * x) numbers;\\nin squared  # [1 4 9 16 25]\\n```\\n\\n### 9. Conditional Expressions\\n```nix\\nlet\\n  checkAge = age: \\n    if age < 18 \\n    then \\"Minor\\" \\n    else \\"Adult\\";\\nin checkAge 20  # Returns \\"Adult\\"\\n```\\n\\n\\n##  Advanced Concepts\\nLazy Evaluation - Nix uses lazy evaluation, meaning expressions are only computed when needed:\\n```nix\\nlet\\n  expensiveComputation = \\n    builtins.trace \\"Computing...\\" \\n    (x: x * x);\\n  \\n  # Not evaluated until used\\n  result = expensiveComputation;\\nin result 42\\n```\\n\\nDerivations\\nThe core of Nix\'s package management:\\n\\n```nix\\nderivation {\\n  name = \\"example\\";\\n  builder = \\"/bin/sh\\";\\n  args = [ \\"-c\\" \\"echo hello > $out\\" ];\\n  system = \\"x86_64-linux\\";\\n}\\n```\\n\\n## Best Practices\\n\\n- Keep functions pure\\n- Avoid side effects\\n- Embrace immutability\\n- Use pattern matching\\n- Leverage lazy evaluation\\n\\n## Key Differences from Other Languages\\n\\n- No mutable state\\n- Functions are first-class citizens\\n- Strong emphasis on reproducibility\\n- Declarative package management\\n- Built-in support for functional programming paradigms\\n\\n## Conclusion\\nNix\'s functional programming syntax provides a unique approach to package management and system configuration, emphasizing reproducibility, purity, and declarative design."},{"id":"Nix Packages","metadata":{"permalink":"/notes/blog/Nix Packages","source":"@site/blog/2023-03-05/index.md","title":"Nix Pkg Manager","description":"Nix is a powerful, purely functional package manager that provides a unique approach to software deployment and system configuration.","date":"2023-03-05T00:00:00.000Z","tags":[{"inline":false,"label":"nix","permalink":"/notes/blog/tags/nix","description":"Nix OS and Functional Package Manager"}],"readingTime":4.225,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"Nix Packages","title":"Nix Pkg Manager","authors":["rakesh"],"tags":["nix"]},"unlisted":false,"prevItem":{"title":"Nix Introduction","permalink":"/notes/blog/Nix OS"},"nextItem":{"title":"Emotional Cause Pair Analysis","permalink":"/notes/blog/ecpe"}},"content":"Nix is a powerful, purely functional package manager that provides a unique approach to software deployment and system configuration. \\n\\n\x3c!-- truncate --\x3e\\n---\\n\\n\\n\\nUnlike traditional package managers, Nix focuses on:\\n\\n>-  Reproducibility  \\n>-  Atomic upgrades and rollbacks  \\n>-  Consistent development environments  \\n>-  Cross-platform compatibility  \\n\\n# Installation\\n\\n## MacOS\\n\\n```bash\\n# Install Nix using the official installer\\nsh <(curl -L https://nixos.org/nix/install)\\n\\n# For Apple Silicon (M1/M2) machines, use:\\nsh <(curl -L https://nixos.org/nix/install) --darwin-use-unencrypted-nix-store\\n```\\n\\n## Linux\\n```bash\\n# For most Linux distributions\\ncurl -L https://nixos.org/nix/install | sh\\n\\n# Multi-user installation (recommended)\\ncurl -L https://nixos.org/nix/install | sh -s -- --daemon\\n```\\n\\n\\n## Post Installation\\nSource the Nix profile. I prefer using zshrc.\\n\\n```bash\\n# Add to your shell configuration (.zshrc, .bashrc)\\nsource ~/.nix-profile/etc/profile.d/nix.sh\\n```\\n\\n## Nix Flakes\\nFlakes are a new feature in Nix that provide better reproducibility and composability. Create a `flake.nix` file in your project:\\n\\n```nix\\n{\\n  description = \\"My Dev Environment\\";\\n\\n  inputs = {\\n    nixpkgs.url = \\"github:NixOS/nixpkgs/nixos-unstable\\";\\n    home-manager = {\\n      url = \\"github:nix-community/home-manager\\";\\n      inputs.nixpkgs.follows = \\"nixpkgs\\";\\n    };\\n\\n    # Add inputs for your dotfile repositories\\n    nvim-config = {\\n      url = \\"github:yourusername/nvim-config\\";\\n      flake = false;  # Just fetch the source\\n    };\\n\\n    tmux-config = {\\n      url = \\"github:yourusername/tmux-config\\";\\n      flake = false;\\n    };\\n\\n    i3-config = {\\n      url = \\"github:yourusername/i3-config\\";\\n      flake = false;\\n    };\\n\\n  };\\n\\n  outputs = { self, nixpkgs, home-manager, nvim-config, tmux-config, i3-config }:\\n    let\\n      # This needs to be adjusted based on your system\\n      system = \\"x86_64-linux\\";  \\n      pkgs = nixpkgs.legacyPackages.${system};\\n    in {\\n      homeConfigurations.mydevenv = home-manager.lib.homeManagerConfiguration {\\n        inherit pkgs;\\n        modules = [ \\n          ./home.nix \\n          {\\n            # Pass the inputs to home-manager configuration\\n            home.file = {\\n              \\".config/nvim\\".source = nvim-config;\\n              \\".config/tmux\\".source = tmux-config;\\n              \\".config/i3\\".source = i3-config;\\n            };\\n          }\\n        ];\\n      };\\n    };\\n}\\n\\n```\\n\\n## Development Environment Setup\\n\\nCreate a `home.nix` file for comprehensive development environment configuration.  \\nHere\'s how my configuration looks like\\n\\n```nix\\n{ config, pkgs, ... }:\\n\\n{\\n  home = {\\n    username = \\"yourusername\\";\\n    homeDirectory = \\"/home/yourusername\\";\\n    stateVersion = \\"23.11\\";\\n\\n    packages = with pkgs; [\\n      # Development tools\\n      git\\n      neovim\\n      wezterm\\n      starship\\n      curl\\n      tmux\\n      rofi\\n      dunst\\n      feh\\n\\n      tmux\\n      \\n      # Language-specific tools\\n      python3\\n      rustup\\n      nodejs\\n      rust-analyzer\\n      \\n      # Utility tools\\n      ripgrep\\n      fd\\n      npm\\n      yarn\\n      lazygit\\n      tree\\n      tree-sitter\\n    ];\\n  };\\n\\n  # Neovim configuration from GitHub\\n  programs.neovim = {\\n    enable = true;\\n    extraConfig = \'\'\\n      lua << EOF\\n      -- Load configuration from GitHub repo\\n      vim.cmd(\'source ~/.config/nvim/init.lua\')\\n      EOF\\n    \'\';\\n  };\\n\\n  # Starship prompt configuration\\n  programs.starship = {\\n    enable = true;\\n    settings = {\\n      add_newline = false;\\n      character = {\\n        success_symbol = \\"[\u279c](bold green)\\";\\n        error_symbol = \\"[\u279c](bold red)\\";\\n      };\\n    };\\n  };\\n\\n  # WezTerm configuration\\n  programs.wezterm = {\\n    enable = true;\\n    extraConfig = \'\'\\n      return {\\n        font = wezterm.font(\\"JetBrains Mono\\"),\\n        color_scheme = \\"Dracula\\",\\n      }\\n    \'\';\\n  };\\n\\n  # Tmux configuration\\n    programs.tmux = {\\n        enable = true;\\n        shell = \\"${pkgs.zsh}/bin/zsh\\";\\n        terminal = \\"screen-256color\\";\\n        plugins = with pkgs.tmuxPlugins; [\\n            resurrect\\n            continuum\\n            # Add your preferred tmux plugins\\n        ];\\n        extraConfig = \'\'\\n            source-file ${../dotfiles/tmux/tmux.conf}\\n        \'\';\\n        };\\n}\\n\\n```\\n\\n\\n## Managing Dotfiles with Nix\\nTo manage dotfiles, create a separate repository with your configuration files and reference them in your Nix configuration:\\n\\n```nix\\n{\\n  home.file = {\\n    \\".config/nvim\\" = {\\n      source = fetchFromGitHub {\\n        owner = \\"yourusername\\";\\n        repo = \\"nvim-config\\";\\n        rev = \\"main\\";  # Or specific commit\\n        sha256 = \\"xxx\\";\\n      };\\n      recursive = true;\\n    };\\n  };\\n}\\n```\\n\\n## Applying the Configuration\\n\\n```nix\\n# Initialize and update flakes\\nnix flake update\\n\\n# Apply the home-manager configuration\\nhome-manager switch --flake .#mydevenv\\n```\\n\\n## Benefits of this Approach\\n\\n- Reproducibility: Exact same environment across machines\\n- Declarative Configuration: Define your entire setup in code\\n- Easy Rollbacks: Simple to revert to previous configurations\\n- Isolated Environments: No dependency conflicts\\n\\n## My Nix Configuration \\n\\n```markdown\\nnix-config/\\n\u2502\\n\u251c\u2500\u2500 flake.nix           # Main Nix flake configuration\\n\u251c\u2500\u2500 flake.lock          # Locked dependencies\\n\u2502\\n\u251c\u2500\u2500 home.nix            # Home Manager configuration\\n\u2502\\n\u251c\u2500\u2500 hosts/              # Machine-specific configurations\\n\u2502   \u251c\u2500\u2500 macbook/\\n\u2502   \u2502   \u2514\u2500\u2500 default.nix\\n\u2502   \u2514\u2500\u2500 workstation/\\n\u2502       \u2514\u2500\u2500 default.nix\\n\u2502\\n\u251c\u2500\u2500 modules/            # Reusable configuration modules\\n\u2502   \u251c\u2500\u2500 neovim/\\n\u2502   \u2502   \u251c\u2500\u2500 default.nix\\n\u2502   \u2502   \u2514\u2500\u2500 plugins.nix\\n\u2502   \u251c\u2500\u2500 starship.nix\\n\u2502   \u251c\u2500\u2500 wezterm.nix\\n\u2502   \u2514\u2500\u2500 git.nix\\n\u2502   \u251c\u2500\u2500 tmux/\\n\u2502   \u2502   \u251c\u2500\u2500 default.nix\\n\u2502   \u2502   \u2514\u2500\u2500 tmp.nix\\n\u2502   \u251c\u2500\u2500 i3/\\n\u2502   \u2502   \u2514\u2500\u2500 default.nix\\n\u2502   \\n\u2502   \\n\u251c\u2500\u2500 dotfiles/           # Actual dotfile configurations\\n\u2502   \u251c\u2500\u2500 nvim/\\n\u2502   \u2502   \u251c\u2500\u2500 init.lua\\n\u2502   \u2502   \u2514\u2500\u2500 lua/\\n\u2502   \u251c\u2500\u2500 starship.toml\\n\u2502   \u2514\u2500\u2500 wezterm.lua\\n\u2502   \u251c\u2500\u2500 tmux/\\n\u2502   \u2502   \u251c\u2500\u2500 tmux.conf   # Main tmux configuration\\n\u2502   \u2502   \u2514\u2500\u2500 plugins/   \\n\u2502   \u2502       \u251c\u2500\u2500 tpm/    # Tmux Plugin Manager (TPM) plugins\\n\u2502   \u2502       \u2514\u2500\u2500 other-plugins/\\n\u2502   \u251c\u2500\u2500 i3/\\n\u2502   \u2502   \u251c\u2500\u2500 config\\n\u2502   \\n\u2514\u2500\u2500 scripts/            # Utility scripts\\n    \u251c\u2500\u2500 setup.sh\\n    \u2514\u2500\u2500 update.sh\\n    \u2514\u2500\u2500 tmux-setup.sh  # Optional setup script for tmux\\n```\\n\\n## Troubleshooting\\n\\n- Ensure Nix daemon is running\\n- Check `~/.config/nixpkgs/config.nix` for global configurations\\n- Use nix-shell for temporary environments\\n\\n## Conclusion\\nNix provides a powerful, reproducible way to manage your development environment. By leveraging Nix flakes and home-manager, you can create consistent, version-controlled setups across multiple machines. If you want to know more about the nix functional package manager. Check out my quick guide [here](http://quibitai.in/notes/)\\n\\n## Additional Resources\\n\\n- Nix Official Documentation\\n- Home Manager GitHub\\n- Nix Flakes Guide\\n- [Nix Functional Package](http://quibitai.in/notes/)"},{"id":"ecpe","metadata":{"permalink":"/notes/blog/ecpe","source":"@site/blog/2022-09-10/ECPE.md","title":"Emotional Cause Pair Analysis","description":"As you move on from working on production grade machine learning projects to the field of research, you get a glimpse of what companies or team of brilliant minds work on.","date":"2022-09-10T00:00:00.000Z","tags":[{"inline":false,"label":"ML Reserach","permalink":"/notes/blog/tags/ML-Research","description":"Machine Learning Research"}],"readingTime":3.71,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"ecpe","title":"Emotional Cause Pair Analysis","authors":["rakesh"],"tags":["ML Research"]},"unlisted":false,"prevItem":{"title":"Nix Pkg Manager","permalink":"/notes/blog/Nix Packages"},"nextItem":{"title":"Recommendation System","permalink":"/notes/blog/Recsys"}},"content":"As you move on from working on production grade machine learning projects to the field of research, you get a glimpse of what companies or team of brilliant minds work on. \\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\nAfter wrapping up our NLP project as part of regular weekend meet, my friends and I decided it was time to take the plunge into state-of-the-art AI/ML topics and look at some advancement in this field.\\n\\nIdea was to choose one research paper , understand the scope, problem it is trying to solve and look at proposed architecture for our implementation. Our objective was to **LEARN** and have **FUN** along the way.\\n\\nPaper published in arxiv.org was selected called as [Emotional Cause Pair Extraction (ECPE)](https://arxiv.org/pdf/1906.01267.pdf). \\n\\n`Note: Images used are from this research paper, and credit goes to the authors. I have just added few annotation on top of it for better understanding.`\\n\\n# Prerequisites\\n\\nThe prerequisites for understanding the research paper is to have excellent knowledge of the following architecture and why they took the decision to go with certain implementation.\\n \\n- RNN (Recurrent Neural Network)\\n- LSTM (Long Short Term Memory)\\n- BI-LSTM (Bi-directional LSTM)\\n- Attention Model\\n\\nAPIs are provided by Tensorflow & Pytorch to implement these in just few lines of code but our suggestion is to try to understand through [1st principles](https://en.wikipedia.org/wiki/First_principle#:~:text=In%20philosophy%2C%20a%20first%20principle,to%20as%20postulates%20by%20Kantians) and then build some of them from scratch (using python). This will give in-depth understanding and intuition of how and why the authors have developed the system the way it is.\\n\\n`Pro tip : You will quickly start going down the rabbit hole as each topic is pretty big in itself. But if you have the passion, you will get through it. Remember - it is no magic but brilliant use of mathematics`\\n\\n\\n# Overview\\n\\nEmotional Cause Pair Extraction (ECPE) - Objective is to extract potential causes that lead to emotional expression in a sentence \\n\\n**Input** - Sentence with 5 clauses :-\\n![ecpe 1](img/ecpe-1.png)\\n\\n**Output** - Emotions and Cause pair\\n\\n![ecpe 2](img/ecpe-2.png)\\n\\n# High Level \\nThe paper proposes to extract all potential pairs of emotions and and corresponding causes in a document via two step process.\\n\\n`Tip: AI and ML are math heavy. Treat it like any other programming language. It takes time to learn. Math is the language of the universe and that was my motivation to learn it. What\'s yours? `\\n\\n\\n### **Step 1** - Extraction \\n\\nExtraction of individual Emotion \\\\\\\\(E\\\\\\\\) and Cause \\\\\\\\(C\\\\\\\\) from a document can be achieved by either of the two approaches :-\\n\\n1.1> Independent Multi-task learning network\\n\\n1.2> Interactive Multi-task learning network (enhanced version)\\n\\n`Tip - Learn LaTeX, your friendly neighbourhood high quality typesetting system for writing Technical or Scientific documents`\\n\\nFinally, the o/p of step 1 is   \\n\\n\\n![Step 1 output](img/step1output.png)\\n\\n\\n### **Step 2** - Pairing and Filtering \\n\\nThere are 4 sub tasks under this :-    \\n\\n2.1 Apply cartesian production on all possible pairs - \\\\\\\\( E \\\\times C\\\\\\\\)   \\n2.2 Represent each pair  by a feature vector   \\n2.3 Build a logistic regression model on each pairs  \\n2.4 Remove 0s from the previous steps to get the final set of \\\\\\\\(e\\\\\\\\) & \\\\\\\\(c\\\\\\\\) pairs   \\n\\n\\n# Details\\n\\nLet us get into a bit more details of ***Step 1***\\n\\n### 1.1>  Independent Multi-task learning network.  \\n\\nBelow diagram shows some of the components. \\n\\nWhere:  \\n\\n![formula](img/formula.png)\\n\\n![alt text](img/architecture.png)\\n\\n\\n\\n### 1.2>Interactive Multi-task learning network  \\n\\nBelow is the enhanced version where the correlation is captured between \\\\\\\\(E\\\\\\\\) & \\\\\\\\(C\\\\\\\\). There are couple of methods here but the architecture remaining the same with slight variations.\\n\\n#### Inter-EC - Using emotion extraction to improve cause Extraction\\n#### Inter-CE - Using cause extraction to improve emotion extraction\\n\\n![alt text](img/formula2.png)\\n\\n\\nMoving on to ***Step 2*** :-  \\n\\n![alt text](img/step2.png)\\n\\n\\n# Settings\\n\\nWe have made some modifications to the settings.  \\n\\n1.  Word2Vec for word embeddings\\n2. Used both uniform distribution and Xavier for initialization\\n3. Mini batch Gradient Decent, Adam optimizer and LR set as 0.0001\\n4. Dropouts with regularization at 20% and 30%\\n\\n\\n# Next steps\\n\\nIn the next article, we will take a look at the implementation using Pytorch.\\n\\n`Pro Tip - Use Pytorch (by FaceBook) if you are into research and use Tensorflow (by Google) if you are in real world implementation - though these differentiation is thinning very quickly`\\n\\n\\n# Conclusion\\nIf you find this article useful or have any inputs, do reach out to me at [Linkedln](https://linkedin.com/in/idinc) and follow me on [Twitter](https://twitter.com/rvbugged). Thank you, keep learning!\\n\\n\\n# References\\n\\nOriginal Paper - (https://arxiv.org/abs/2103.01544)"},{"id":"Recsys","metadata":{"permalink":"/notes/blog/Recsys","source":"@site/blog/2022-09-10/RecommendationSystem.md","title":"Recommendation System","description":"---","date":"2022-09-10T00:00:00.000Z","tags":[{"inline":false,"label":"ML","permalink":"/notes/blog/tags/ML","description":"Machine Learning"}],"readingTime":8.265,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"Recsys","title":"Recommendation System","authors":["rakesh"],"tags":["ML"]},"unlisted":false,"prevItem":{"title":"Emotional Cause Pair Analysis","permalink":"/notes/blog/ecpe"},"nextItem":{"title":"Cookie Cutter Version 1.0","permalink":"/notes/blog/Cookiever1"}},"content":"---\\n# Introduction\\n\\n\\nThis is the first article of a multi-part series on building Recommendation Systems (RS). \\n\\nIn Part 1, we will cover the two types - Popularity Based and Content Based recommenders. Objective of the article is to share my learnings and experiences using a public dataset.\\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n# What are Recommendation Systems\\n---\\nIt\'s a **Information Filtering** system for providing relevant suggestions to users based on their interactions.  It aids our decision making process, things to buy, movies to watch, out of billions of items out there.  \\n\\nIt is perfected by Google, Amazon, Netflix and many other companies. They use various user features (location, demographics, etc) and item features (video length, type etc) to understand user preferences and made billions of dollars perfecting this system.\\n\\nFor example, you open your music app like Spotify and listen to a song  *`Wasted Years`* from **`Iron Maiden`**. It understands your taste in heavy metal music, uses NLP to process lyrics, calculates song\'s acoustics and assesses your mood to recommend similar songs. The more you interact, the perfect it becomes!\\n\\nI have often heard of people spending hours watching funny or cat videos , recommendation is to blame isn\'t it :) .\\n\\n---\\n# Prerequisites\\n---\\n- Dataset used - [TMDB 5000](https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata?select=tmdb_5000_movies.csv) because it is easier to relate. You can choose any dataset to implement these systems.\\n- It is the almost perfect publicly available dataset as using the real world dataset is not possible.\\n- In real world, the data needs to go through lot of clean-up process (preprocessing stage). \\n- Great deal of time needs to be spent on understanding the data via Exploratory Data Analysis (EDA) & Feature Engineering (FE)  \\n- If you do not want to install any tools on your machine, you can use either :\\n    - [Kaggle](https://kaggle.com) - This is where most action happens in the world of  ML\\n    - [Google Colab](https://colab.research.google.com/)  - Google gives you free access to CPU and GPU\\n- If you are building a real world application - using an IDE (VS Code or Pycharm) is a good option.   \\n`Tip: Here\'s link to my blog on structuring real world ML projects` - [link](https://rvbug.hashnode.dev/ml-cookie-cutter)\\n\\n\\n\\n--- \\n# Dataset \\n\\nThis dataset contains two files which are merged into one DataFrame (Pandas data structure).\\n\\n- Movies - Details of movies like title, id etc\\n- Credits - Details of Cast, Crew and other metadata info\\n\\nThe below image shows the columns available and its data types -`df` is a dataframe\\n\\n![dataset](img/dataset.png)\\n\\n`Tip: EDA and FE takes up around 80% of your time, this is to understand the data and what story is it trying to tell`\\n> Become one with the data, my friend... \\n\\n---\\n# Types \\n\\n![overview](img/overview.png)\\n\\n---\\n## A>  Popularity Based\\n\\nThis is a formula based recommendation system and implementation varies from company to company. Few articles classify this type as part of Collaborative filtering but I like to keep it separate.\\n\\nIn [IMDB](https://www.imdb.com/chart/top/), the movie ratings are not just simple average ratings given by casual users, but it takes into account `weights`  i.e. users who are industry professionals, or who has provided more than 50+ ratings. \\n\\n`Tip - Data Science and ML is more of an art than science. The implementation can vary depending on what problem are you trying to solve`\\n\\nThe formula used for calculating the ratings \\\\\\\\(W\\\\\\\\) is as follows:-\\n\\n![weights](img/weight.png)\\n\\n\\n Where:  \\n\\\\\\\\(R\\\\\\\\) :- Average rating of a movie (Ranges from 1 to 10)  \\n\\\\\\\\(v\\\\\\\\) :- Number of votes  i.e. how many users have given rating for a movie  \\n\\\\\\\\(m\\\\\\\\) :- Minimum votes required to be listed (more than 70 Percentile)  \\n\\\\\\\\(C\\\\\\\\) :- Mean votes for entire dataset    \\n\\n![SampleData](img/weightcalcpy.png)\\n\\nThis column is now available  showing weighted average ratings.\\n\\n![output](img/weightedoutput.png)\\n\\n\\nFor YouTube, Twitter and other similar platforms, the implementation might be quite different - it could be based on likes, views, time spent and many other factors.\\n\\n`Pro Tip: In one of the project I had to derive a method to calculate popularity.`\\n\\n\\nIf you sort this weighted average in descending order and pick the first 20, here\'s what you will get.\\n\\n![sorted](img/sort.png)\\n\\n\\n\\n`Note: In this dataset, there is an additional column called popularity, you could also sort the ratings according to this field and have a mix of both weighted average and popularity score. In real world, this field might not be available so you will have to come up with your own calculations.`\\n\\nImage below shows the popularity which is not sorted yet\\n\\n![Notsorted](img/notsorted.png)\\n\\n`Pro tip: The scale of weighted_avg and popularity column is  different, so transformation is applied to bring them down to same scale using - ` [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\\n\\nMinMaxScalar normalizes the features and transforms the numbers between \\\\\\\\([0, 1]\\\\\\\\) using the formula:-\\n\\n![minmax](img/minmax.png)\\n\\nWhere :  \\n\\\\\\\\( x_n\\\\\\\\) = Minimum value of x    \\n\\\\\\\\( x_m\\\\\\\\) = Max value of x    \\n\\nThe last two columns are now scaled. \\n\\n![scaled](img/scaled.png)\\n\\n\\nThis scaled columns can then multiplied by a factor depending on your requirements. \\n\\nFor example, multiply a factor of 0.4 (40%) on popularity and 0.6 (60%) on weighted_avg to extract the top 20 list again. You can even keep it equal on 0.5 (50%) each.\\n\\n`Note: Depending on your application and requirements, you can show top five or ten items too`\\n\\n\\nWhen a new user logs in to your application, their preferences are unknown - this is called `Cold Start` problem so showing most popular items in your application the very first time they visit will get the interaction started.\\n\\n---\\n\\n## B> Content Based Filtering\\n\\nIt is a form of filtering for recommending items to a user based on their past interaction with similar items. We will talk about calculating `similarity` shortly!\\n\\nLet us take our movie dataset  - if the user has watched `Inception` then based on the data available about the movie e.g. director, cast, plot, keywords and crew, you can filter similar movies and recommend them. i.e. similarity is based on metadata of the item. \\n\\n\\nHere is the what the data shows for Inception :-\\n\\n![inception](img/inception.png)\\n\\n---\\n### Features\\n\\nWe will be selecting only few columns. You can always add other features like budget, release_date, runtime, tagline etc to see how your model is performing.  \\n\\n![Features](img/features.png)\\n\\n---\\n### Data Review\\nLet us review the data in each of the columns like below.\\n\\n1st movie in this dataset is `Avatar`\\n\\n![Avatar](img/avatar.png)\\n\\nGenres :-  \\n\\n![Genres](img/generes.png)\\n\\nKeywords :-  \\n\\n![Keywords](img/keywordoutput.png)\\n\\nCrew details are as shown below but we will extract only Director from this list\\n\\n![crew](img/crew.png)\\n\\n`Pro Tip: What you select as a feature depends on the problem and the data available, you might have to create a new feature based on the existing ones or transform certain features too`\\n\\n---\\n### Processing\\nNext step is to process the data and create a list. We will create a function to process genres, keywords, cast and crew. For crew column, extract only job role as director. \\n\\n![Processing](img/Processing.png)\\n\\nThere are few additional steps to be done :-\\n\\n- Drop and keep only relevant columns \\n- Remove spaces from cast and crew\\n- Concatenate all fields except cast and crew\\n- Convert to lowercase\\n- Finally, remove stopwords \\n\\n\\nAfter processing the dataframe will look something like this.\\n[text](img/head.png)\\n\\nSample data :-\\n\\n![SampleData](img/sampledata.png)\\n\\n\\nThis was the easiest part :)\\n\\n\\n---\\n### Text Vectorizer\\n\\nWe went through lot of preprocessing steps in the last section to extract strings.  Strings will be converted into vectors - a step known as `Text Vectorizer`. Closer these vectors are to each other in a multidimensional space, the more similar they are. Some of the techniques to convert are :-\\n\\n1. Bag of Words(BoW)\\n2. TF-IDF\\n3. Word2Vec\\n\\nWe will use Tf-IDF in this article.\\n\\n### TF-IDF\\n\\nTerm Frequency Inverse Document Frequency or TF-IDF is a statistical method widely used in Information Retrieval systems and also to rank the relevance of documents in search engines.\\n\\nIt gives importance to a word which occurs most frequently in a document and frequency of this same word rarely occurring in an entire corpus - A kind of a push-pull mechanism. You can read more about it [here](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) \\n\\n---\\n### Similarity\\n\\nOnce you have the data in a vectorized format, next step is to calculate the distance of every movie with each other. This can be achieved either by  \\n\\n- Euclidian Distance  - It fails in a very high dimensional space. \\n- Cosine Distance - Calculate angle between the movie vectors. Movies will be similar if the angle between them is similar.\\n\\n\\nUse sklearn\'s [cosine distance]![Cosine](img/cosine.png)\\n\\n![Shape](img/shape.png)\\n\\n\\n`Pro-tip: These libraries are designed to make our life easy - but understanding how and why it works at an intuitive level will help you pick up more complex and state-of-the-art implementations as you move forward in your ML jouney`\\n\\n\\nThis is how the data looks :-\\n\\n- Shape is 4800 x 4800 - calculate the similarity of every movie with every other movie\\n- Sample record - Shows 10th Record which has shape of 1 row with 4800 columns-\\n\\n![Screenshot 2022-09-09 at 3.01.41 PM.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1662716299360/HK073c-cJ.png align=\\"left\\")\\n\\n---\\n### Recommendations\\nHere\'s the function to recommend movies. \\n\\n![alt text](img/recommend.png)\\n\\nAll set! Let\'s see how the model performs. \\n\\nSearching for Batman\\n\\n![alt text](img/batman.png)\\n\\nJames Bond\\n\\n![die](img/die.png)\\n\\nTesting on couple of my favorite movies : \\n\\n![spotlight](img/spotlight.png)\\n\\n![Clayton](img/Clayton.png)\\n\\n\\nSeems to be pretty decent. You can also use additional metadata or try Word2Vec to see how accurately this model performs. \\n\\n---\\n# Conclusion\\n  \\nIn this article we covered what are Recommendation Systems, their types, looked at Popularity and Content based systems. In the next article we will take a look at most widely used Collaborative filtering. Stay tuned!!\\n\\nCredit goes to amazing people who have been patiently answering my questions and publishing some great content out there from where I learnt so many things. \\n\\nYou can download the dataset [here](https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata)\\n\\nCode is available at :\\n  \\n[Github](https://github.com/rvbug/RecommendationSystems)       \\n[Kaggle - Content Based](https://kaggle.com/rvbugged/content-based)   \\n[Kaggle - Popularity Based ](https://kaggle.com/rvbugged/popularity-based)     \\n\\n\\nYou can find me on [Linkedln](https://www.linkedin.com/in/idinc/) and [Twitter](https://twitter.com/rvbugged)."},{"id":"Cookiever1","metadata":{"permalink":"/notes/blog/Cookiever1","source":"@site/blog/2022-09-10/cookiev10.md","title":"Cookie Cutter Version 1.0","description":"I go by one simple principle","date":"2022-09-10T00:00:00.000Z","tags":[{"inline":false,"label":"ML","permalink":"/notes/blog/tags/ML","description":"Machine Learning"}],"readingTime":3.64,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"Cookiever1","title":"Cookie Cutter Version 1.0","authors":["rakesh"],"tags":["ML"]},"unlisted":false,"prevItem":{"title":"Recommendation System","permalink":"/notes/blog/Recsys"},"nextItem":{"title":"Data Science and ML Structure","permalink":"/notes/blog/dsml"}},"content":"I go by one simple principle\\n\\n> If you repeat the same steps twice, then automate it the third time.\\n\\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\n\\n\\nCouple of years back, I wrote blog on how to :\\n\\n1&gt; Structure your Data Science and ML project - [Link here](https://rvbug.hashnode.dev/structuring-data-science-and-ml-projects)\\n\\n2&gt; Automating ML structure using `Make` and a simple `Python Script` - [Link here](https://rvbug.hashnode.dev/ml-cookie-cutter).\\n\\nIn this follow-up article, I will provide an update to the cookie-cutter project which now uses `yaml` making it far more flexible and easy to use.\\n\\n# Project Structure\\n\\nHere\'s the structure I use for all my ML projects.\\n\\nYou can create a different one but these are pretty standard for most of the projects.\\n\\n![folder](img/folder.png)\\n\\n## YAML\\n\\nYAML is **a text based data serialization language for managing your configuration files**. YAML stands for `Yet Another Markup Language` or `YAML ain\'t markup language`. You can read more about it [here](https://en.wikipedia.org/wiki/YAML).\\n\\nHere\'s the YAML format used in my project.\\n\\n```yaml\\n  # support for data version control\\n  - .dvc:\\n\\n  # if you plan to use docker container\\n  - docker:\\n    - Dockerfile\\n\\n  # basic ML development files\\n  - src:\\n    - __init__.py\\n    - data_pipeline.py\\n    - data_processing.py\\n\\n  ####\\n```\\n\\n# Script\\n\\nOne of the main pre-requisites for this script is `pyyaml`. You can find the documentation [here](https://pyyaml.org/wiki/PyYAMLDocumentation). Install them via [pip](https://pip.pypa.io/en/stable/).\\n\\n## Argparse\\n\\nEverything starts with the main function. It uses argparse to parse command line arguments provided to the script. It then loads the yaml config file.\\n\\n```python\\n# parse the arguments provided on the command line\\nargs = parse_args()\\n\\n# Load yaml config\\nconfig = load_config()\\n```\\n\\nThe parser will look for specific flags and take action. More about that in the Help section below.\\n\\n```python\\nparser.add_argument(\\"--n\\", \\"--name\\", #...)\\nparser.add_argument(\\"--p\\", \\"--path\\",  #...) \\nparser.add_argument(\\"--c\\", \\"--config\\",  #...)\\nparser.add_argument(\\"--v\\", \\"--venv\\",  #...)\\n```\\n\\n## File and Directories\\n\\nBased on how YAML is structured, the script will create files, directories and sub-directories via a function `create_directories`\\n\\n```python\\ndef create_directories(project_path, config):\\n\\nif isinstance(config, str):\\n        item_path = os.path.join(project_path, config)\\n        with open(item_path, \\"w\\"):\\n            pass  # empty file\\n\\n#...\\n```\\n\\n## Virtual env\\n\\nIf you provide flag to create a [virtual environment](https://docs.python.org/3/library/venv.html), the script will create one for you. There is a function called `create_virtual_env` to do exactly that.\\n\\n```python\\ndef create_virtual_env(project_path, activate=True):\\n#...\\nif not os.path.exists(venv_path):\\n        venv.create(venv_path, with_pip=True)\\n#...\\n```\\n\\n## Help\\n\\nWhen you use `-h` flag, it will show you how to use the script.\\n\\nIf you want to give a specific name to your project use `--n or N` flag. If not, it will create a default directory called `ml-cookie-cutter.`\\n\\nSpecify the path where the project needs to be created using `--p or P` flag.\\n\\nFinally, if you want the script to create a virtual environment for you, go ahead and use the `--v or --venv` flag. By default, the name of the environment is `venv`.\\n\\n```bash\\n\\n$> python3 main.py --h\\n\\n# usage: ML Cookie Cutter [-h] [--n N] [--p P] [--c C] [--v]\\n\\n# Creates ML project cookie cutter structure\\n\\n#optional arguments:\\n#  -h, --help       show this help message and exit\\n#  --n N, --name N  Name of the directory to be created, default = ml-cookie-cutter\\n#  --p P, --path P    provide the path where, default is $HOME dir\\n#  --v, --venv        create a virtual env. [ignore if you are already on a virtual env]\\n\\n# Enjoy and happy coding\\n```\\n\\n## Final step\\n\\nOnce you have the virtual environment set up, you can activate it as shown below. You should see that it in your command prompt; go ahead and start installing all your Data science and ML libraries.\\n\\nFor deactivating simply type `deactivate`\\n\\n```bash\\n# go to the project folder\\n$> cd ml-cookie-cutter\\n# activate the environment \\n$> source venv/bin/activate \\n\\n# venv is activated \\n(venv) $> pip install numpy pandas pytorch seaborn notebook\\n\\n# To deactive use the following command\\n(venv) $> deactivate\\n$>\\n```\\n\\nOn windows if you use PowerShell activate using `Activate.ps1`\\n\\n# Conclusion\\n\\nThis article shows how you can use this cookie-cutter project and create a ML structure with a simple command-line tool. Enjoy and have fun learning!\\n\\nThis is how I have used the cookie-cutter project to create a ML structure with a simple command-line tool. Enjoy and have fun learning!\\n\\nHappy coding!\\n\\n# References\\n\\n[Github - Complete Code](https://github.com/rvbug/cookie-ml/)\\n\\n[Pyyaml Documentation](https://pypi.org/project/PyYAML/)\\n\\n[Installing Python](https://www.python.org/)\\n\\n[PIP Installation](https://pip.pypa.io/en/stable/)\\n\\n[How to structure your ML projects](https://rvbug.hashnode.dev/ml-cookie-cutter)\\n\\n[Automating ML structure (v 1.0)](https://rvbug.hashnode.dev/structuring-data-science-and-ml-projects)\\n\\n[Documentation on Virtual Env](https://docs.python.org/3/library/venv.html)"},{"id":"dsml","metadata":{"permalink":"/notes/blog/dsml","source":"@site/blog/2022-09-10/dsml.md","title":"Data Science and ML Structure","description":"In this article, I will explain how I set up my Data Science (DS) or Machine Learning (ML) projects and the tools I use to make the process as effective as possible. This is purely based on my experience and might help someone who is new to DS or ML. There might be a better option out there but this works extremely very well for me, especially when working on multiple projects simultaneously.","date":"2022-09-10T00:00:00.000Z","tags":[{"inline":false,"label":"ML","permalink":"/notes/blog/tags/ML","description":"Machine Learning"}],"readingTime":3.385,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"dsml","title":"Data Science and ML Structure","authors":["rakesh"],"tags":["ML"]},"unlisted":false,"prevItem":{"title":"Cookie Cutter Version 1.0","permalink":"/notes/blog/Cookiever1"},"nextItem":{"title":"Notion Introduction","permalink":"/notes/blog/Notion"}},"content":"In this article, I will explain how I set up my Data Science (DS) or Machine Learning (ML) projects and the tools I use to make the process as effective as possible. This is purely based on my experience and might help someone who is new to DS or ML. There might be a better option out there but this works extremely very well for me, especially when working on multiple projects simultaneously.\\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\n\\n# Tools\\n\\nYou can choose from a variety of IDE/tools out there but it all comes down to your preference and taste. Some of them are\u2026\\n\\n- **PyCharm**\\n- **Spyder**\\n- **Jupyter Notebook**\\n\\nIf it is your personal project, you can use Google  [Colab](https://colab.research.google.com)  \u2014 This gives you free access to GPUs and TPUs (Tensor Processing Units). Jupyter even works well for learning programming language like Julia.\\n\\nTo track your tasks, you can use any project management or Kanban tool like  [Trello](https://trello.com/)  (easy to set up) or  [Notion](https://www.notion.so/)  (needs time to set it up but extremely effective).\\n\\n# Structure\\n\\nYou need to first install Python and Jupyter notebook via `pip install jupyter`. You can skip this step if you have already installed them.\\n\\nOnce installed, you can add additional configuration by installing NBExtensions using `pip install jupyter_contrib_nbextensions`.\\n\\nNow, when you start Jupyter Notebook, you should see nbextensions Tab like below.\\n\\nThese are the ones which I use \u2014 Collapsible Headings, Table of Contents and Variable inspector but feel free to add additional extensions.\\n![Jupyter 1](img/jupyter1.png)\\n\\n# Sample Jupyter Notebook\\n\\nThis is what the NBExtension selection will help you do. Create a new section using markdown and Contents gets filled in automatically. This also helps you to jump between the section while keeping the notebook clean.\\n\\n![Jupyter 2](img/jupyter2.png)\\n\\n# Setting up Folder Structures\\n\\nTypically, my folder structure is as below \u2014 You can add more folders/sub-folders or modify it according to your requirements.\\n\\n**`data`** - Stores all the data is in various formats (.json, .csv, .xlsx etc).\\n\\n**`reports`** - All the reports are stored in here.\\n\\n**`mode`** - If you are working on ML models, you can use this to store all your model checkpoints.\\n\\n**`technicalPaper`** - Finally, technical paper (.ipynb or a word doc) to be presented to your team.\\n\\n\\n# Naming Jupyter Notebooks\\n\\nI use the following names to differentiate between experiments and final code\\n\\n`**projectName_EDA_ML_Experiements.ipnyb**` - All your analysis will be here. You will start exploring, understanding, cleaning up data, running your models, metrics and performance. This is your playground. During the exploratory phase, this file will be filled with lots of code, data type conversions, distributions, data transformations, plots, sample code etc. This is a necessary step to understand the data.\\n\\n`**projectName.ipynb**` - Where the final code lives all cleaned up. If you want to work on additional data points, go back to the experiment notebook so you can keep this neat and clean.\\n\\n`**projectName_ResearchPaper.ipynb**` - This is where I usually capture my notes, references, website and Math formulas etc. This then becomes your research/white paper to be published.\\n\\n\\n# Project Tracking\\n\\nUsually enterprise level project takes much longer, it has multiple tasks which includes gathering additional inputs from Product Owners/ Business/Sales team and other stakeholders \u2014 not to mention additional rush of ideas when you are in \u201cThe Zone\u201d\\n\\nFor tracking, I have experimented with couple of tools -\\n\\n1> The old fashioned but effective way \u2014 Pen and Paper\\n\\n2> To use some kind of Kanban tool like Trello.\\n\\nBut the one which is been very effective for me over the past year is an app called Notion. This helps me to view the tasks for the day, plan for the week and hit the quarterly goals.\\n\\nAt the end of the day when you see the status of your task as completed like below, you\u2019d know you had a great day!!!\\n\\n\\n![Notion](img/notion.png)\\n\\n# Conclusion\\n\\nI had done a great deal of experiments before narrowing it down to these processes, so if this article helps you in any way do let me know.\\n\\nHappy Learning!!!"},{"id":"Notion","metadata":{"permalink":"/notes/blog/Notion","source":"@site/blog/2021-08-26/notion-productivity-guide.md","title":"Notion Introduction","description":"---","date":"2021-08-26T00:00:00.000Z","tags":[{"inline":false,"label":"Notion","permalink":"/notes/blog/tags/Notion","description":"Notion"}],"readingTime":2.35,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"Notion","title":"Notion Introduction","authors":["rakesh"],"tags":["Notion"]},"unlisted":false,"prevItem":{"title":"Data Science and ML Structure","permalink":"/notes/blog/dsml"},"nextItem":{"title":"Introduction to Docusaurus","permalink":"/notes/blog/Intro to Docusaurus"}},"content":"\x3c!-- truncate --\x3e\\n\\n---\\n\\n## The Professional\'s Productivity Challenge\\n\\nAs a full-time software engineer balancing family life, personal projects, and professional responsibilities, I\'ve spent years searching for the ultimate productivity solution. In 2017, I discovered Notion \u2013 a game-changing platform that revolutionized how I manage my life, track projects, and transform ideas into actionable plans.\\n\\n## Notion: A Powerful Productivity Ecosystem\\n\\nNotion is far more than a simple productivity app. It\'s a versatile workspace that adapts to your unique workflow, combining note-taking, project management, databases, and collaboration tools into a single, customizable platform.\\n\\n## Deep Dive: Notion\'s Powerful Features\\n\\n### 1. Databases: The Backbone of Intelligent Organization\\n\\nNotion\'s databases are game-changers for personal productivity:\\n\\n#### Relational Databases\\n- **Link information across different pages**\\n- **Create complex, interconnected knowledge systems**\\n- **Track relationships between projects, goals, and tasks**\\n\\n#### Multiple View Types\\n- Table view for detailed tracking\\n- Kanban boards for visual progress\\n- Calendar view for time-based organization\\n- Gallery view for visual projects\\n\\n### 2. Dynamic Pages and Nested Information\\n\\n- Hierarchical page structure\\n- Unlimited nesting of pages\\n- Seamless information organization\\n- Context-rich documentation\\n\\n### 3. Powerful Formulas and Rollups\\n\\n- Spreadsheet-like calculations\\n- Aggregate data across databases\\n- Create dynamic, self-updating dashboards\\n- Automate tracking and reporting\\n\\n### 4. Linked Databases: Breaking Information Silos\\n\\n- Reference same database in multiple views\\n- Maintain data consistency\\n- Create project-specific perspectives\\n- Reduce redundant data entry\\n\\n### 5. Rich Embedding Capabilities\\n\\n- Embed images, videos, and external content\\n- Integrate with multiple platforms\\n- Create comprehensive, multimedia workspaces\\n- Centralize information from various sources\\n\\n## My Notion Ecosystem: Practical Implementation\\n\\n### Specialized Trackers\\n\\n1. **Travel Tracker**\\n   - Trip logging\\n   - Expense management\\n   - Future planning\\n   - Memory collection\\n\\n2. **Habit Tracker**\\n   - Daily/weekly habit monitoring\\n   - Progress visualization\\n   - Accountability mechanisms\\n\\n3. **Meal Planner**\\n   - Recipe management\\n   - Grocery lists\\n   - Nutrition tracking\\n\\n4. **Project Management**\\n   - Milestone tracking\\n   - Resource allocation\\n   - Progress monitoring\\n\\n## Integration Strategy\\n\\nNotion works best as part of a broader productivity ecosystem:\\n\\n- Apple Notes for quick captures\\n- Apple Reminders for time-sensitive tasks\\n- Apple Calendar for scheduling\\n\\n## Weekly Review Process\\n\\n- Review captured ideas\\n- Update project trackers\\n- Adjust goals\\n- Reflect on achievements\\n\\n## Key Advantages\\n\\n- **Flexibility**: Fully customizable workflows\\n- **Integration**: Connects life and work domains\\n- **Visualization**: Intuitive data representation\\n- **Accessibility**: Cross-device availability\\n- **Collaboration**: Easy sharing and teamwork\\n\\n## Technical Insights for Engineers\\n\\nThink of Notion like code:\\n- Templates = Modular code\\n- Databases = Structured data models\\n- Views = Different data rendering methods\\n\\n## Getting Started: Your Notion Journey\\n\\n1. Download Notion\\n2. Start with basic templates\\n3. Experiment gradually\\n4. Develop your unique system\\n\\nThe perfect productivity system evolves with you.\\n\\n*Disclaimer: Personal experience. Results may vary.*"},{"id":"Intro to Docusaurus","metadata":{"permalink":"/notes/blog/Intro to Docusaurus","source":"@site/blog/2020-03-05/Docusourus.md","title":"Introduction to Docusaurus","description":"Docusaurus is an open-source static site generator specifically designed for creating documentation websites. Developed by Facebook (Meta), it provides a powerful and intuitive platform for building, deploying, and maintaining documentation sites with ease.","date":"2020-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"docusaurus","permalink":"/notes/blog/tags/docusaurus"}],"readingTime":2.3,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"Intro to Docusaurus","title":"Introduction to Docusaurus","authors":["rakesh"],"tags":["docusaurus"]},"unlisted":false,"prevItem":{"title":"Notion Introduction","permalink":"/notes/blog/Notion"},"nextItem":{"title":"Welcome","permalink":"/notes/blog/welcome"}},"content":"# Introduction\\n\\nDocusaurus is an open-source static site generator specifically designed for creating documentation websites. Developed by Facebook (Meta), it provides a powerful and intuitive platform for building, deploying, and maintaining documentation sites with ease.\\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\n# What is Docusaurus?\\nDocusaurus is a modern static website generator that helps developers and teams create, build, and publish documentation websites quickly and efficiently. It leverages React and provides a clean, customizable interface with built-in features that make documentation management seamless.\\n\\n# Key Features and Advantages\\n### Easy Setup and Configuration\\n\\n- Quick initialization with minimal configuration\\n- Built-in best practices for documentation sites\\n- Supports both single and multi-version documentation\\n\\n### Powerful Customization\\n\\n- Fully customizable React-based themes\\n- Support for custom layouts and components\\n- Flexible styling options\\n\\n### Developer-Friendly Features\\n\\n- Markdown support with enhanced features\\n-  Built-in search functionality\\n- Internationalization (i18n) support\\n- Versioning for documentation\\n\\n### Performance and SEO\\n- Static site generation for fast loading\\n- Optimized for search engines\\n- Responsive design out of the box\\n\\n# Getting Started with Docusaurus\\n## Installation\\nYou can create a new Docusaurus project using either `yarn` or `npm`:\\n\\n### Using Yarn\\n\\n```bash\\nyarn create docusaurus my-website classic\\n```\\n\\n### Using npm\\n```bash\\nnpx create-docusaurus@latest my-website classic\\n```\\n\\n\\n### project Structure\\n```markdown\\nmy-website/\\n\u2502\\n\u251c\u2500\u2500 docs/                  # Markdown documentation files\\n\u251c\u2500\u2500 blog/                  # Blog posts\\n\u251c\u2500\u2500 src/                   # Custom React components\\n\u2502   \u251c\u2500\u2500 components/\\n\u2502   \u2514\u2500\u2500 pages/\\n\u251c\u2500\u2500 static/                # Static assets\\n\u251c\u2500\u2500 docusaurus.config.js   # Configuration file\\n\u251c\u2500\u2500 sidebars.js            # Sidebar configuration\\n\u2514\u2500\u2500 package.json           # Project dependencies\\n\\n```\\n\\n\\n\\n### Confguration\\n\\nThe primary configuration file is `docusaurus.config.js`. Here\'s a basic example:\\n\\n```js\\nmodule.exports = {\\n  title: \'My Documentation Site\',\\n  tagline: \'Awesome Documentation\',\\n  url: \'https://your-website.com\',\\n  baseUrl: \'/\',\\n  theme: \'@docusaurus/theme-classic\',\\n  presets: [\\n    [\\n      \'@docusaurus/preset-classic\',\\n      {\\n        docs: {\\n          sidebarPath: require.resolve(\'./sidebars.js\'),\\n        },\\n        blog: {\\n          showReadingTime: true,\\n        },\\n        theme: {\\n          customCss: require.resolve(\'./src/css/custom.css\'),\\n        },\\n      },\\n    ],\\n  ],\\n};\\n```\\n\\n### Bulding the site\\n\\nStart the local development server\\n```bash\\nyarn start\\n# or\\nnpm run start\\n```\\n\\n### Publishing the site\\n\\nDocusaurus provides built-in support for GitHub Pages:\\n\\n```bash\\nGIT_USER=<GITHUB_USERNAME> yarn deploy\\n# or\\nUSE_SSH=true yarn deploy\\n```\\n\\n# Best Practices\\n\\n1. Keep documentation organized\\n2. Use clear, concise language\\n3. Implement versioning for stable releases\\n4. Utilize built-in search and navigation features\\n5. Regularly update and maintain documentation\\n\\n\\n# Markdown\\n```markdown\\n---\\nid: getting-started\\ntitle: Getting Started\\nsidebar_label: Introduction\\n---\\n\\n# Welcome to Our Documentation\\n\\nThis is a sample documentation page with some information.\\n\\n## Key Concepts\\n\\n- Simple markdown writing\\n- Easy configuration\\n- Powerful features\\n```\\n\\n# Conclusion\\nDocusaurus provides a modern, efficient solution for creating and maintaining documentation websites. Its combination of ease of use, powerful features, and flexibility makes it an excellent choice for projects of all sizes."},{"id":"welcome","metadata":{"permalink":"/notes/blog/welcome","source":"@site/blog/2020-03-05/index.md","title":"Welcome","description":"Welcome to my blog, a space dedicated to exploring the frontiers of technology and science.","date":"2020-03-05T00:00:00.000Z","tags":[{"inline":false,"label":"Hello","permalink":"/notes/blog/tags/hello","description":"Hello tag description"}],"readingTime":0.255,"hasTruncateMarker":false,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"welcome","title":"Welcome","authors":["rakesh"],"tags":["hello"]},"unlisted":false,"prevItem":{"title":"Introduction to Docusaurus","permalink":"/notes/blog/Intro to Docusaurus"}},"content":"Welcome to my blog, a space dedicated to exploring the frontiers of technology and science. \\n\\nDive into various in-depth articles on **`Machine Learning`**, **`Quantum Computing`**, fascinating architecture of **DeepSeek** and much more. \\n\\nPowered by [QuBiTAi](https://qubitai.in) and built with Docusaurus, this platform is your gateway to understanding complex concepts and cutting-edge research."}]}}')}}]);