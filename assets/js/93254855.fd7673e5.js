"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[2302],{4894:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>c});var t=i(1666),a=i(4848),o=i(8453);const s={slug:"DeepSeekPTX",title:"PTX Optimization",description:"How Deepseek optimized performance of the LLMs",image:"img/LLMWorkflow.png",authors:["rakesh"],tags:["ML","ML Research"]},r="Introduction",l={authorsImageUrls:[void 0]},c=[{value:"Register Allocation",id:"register-allocation",level:2},{value:"Custom Memory Management",id:"custom-memory-management",level:2},{value:"Cache",id:"cache",level:3},{value:"Cache Controls",id:"cache-controls",level:3},{value:"Prefetching",id:"prefetching",level:3},{value:"Prefetch Distance and Hints",id:"prefetch-distance-and-hints",level:4},{value:"Alignment &amp; Coalescing",id:"alignment--coalescing",level:3},{value:"Vectorized loads",id:"vectorized-loads",level:4},{value:"Shared Memory Optimization",id:"shared-memory-optimization",level:3},{value:"Inter-GPU communcation",id:"inter-gpu-communcation",level:2},{value:"Warp Level Optimization",id:"warp-level-optimization",level:2},{value:"Warp Shuffle Instructions:",id:"warp-shuffle-instructions",level:4},{value:"Conclusion",id:"conclusion",level:2},{value:"Next",id:"next",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.p,{children:"Let us now delve into the details of PTX, the parallel thread execution virtual instruction set, and explore how DeepSeek might have approached optimization for their H800 GPUs. PTX optimization is critical for maximizing performance in large language models."}),"\n",(0,a.jsxs)(n.p,{children:["Since DeepSeek's specific PTX implementations are proprietary, this article focuses on optimization strategies inferred from their research papers and related discussions. We'll explore a few of them within their architecture. For example, ",(0,a.jsx)(n.strong,{children:"Multi-Head Latent Attention (MHLA)"})," employs a modified Key and Value cache approach, differing from the standard transformer KV cache concept, to enhance efficiency."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h1,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"DeepSeek, particularly with their R1 model, has implemented significant optimizations across both training and inference phases. We will delve into these broader optimizations in a separate, more detailed article. In this one, however, our focus will be exclusively on PTX. \xa0"}),"\n",(0,a.jsx)(n.p,{children:"PTX empowers developers with the ability to perform low-level optimizations, granting fine-grained control over register allocation, thread execution, and memory access patterns."}),"\n",(0,a.jsx)(n.h2,{id:"register-allocation",children:"Register Allocation"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Allowing manually optimizing the register allocation, the latency could have been reduced."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Fine-tune thread scheduling allowing them to maximize parallelism across the streaming multiprocessors."}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"custom-memory-management",children:"Custom Memory Management"}),"\n",(0,a.jsx)(n.p,{children:"Implementing custome PTX instructions for accessing memory including global VRAM access by bypassing L1 and L2 cache in a very specific way allowing increasing data transfer pattern thus improving memory bandwidth."}),"\n",(0,a.jsxs)(n.admonition,{type:"tip",children:[(0,a.jsx)(n.mdxAdmonitionTitle,{}),(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Global VRAM"})," is largest and slowest memory on the GPU"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cache"})," can also introduce overhead and may not always be effective"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Coalesced Access"})," - Accessing contiguous memory locations in a single transaction significantly improves memory bandwidth."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Memory Access"})," - Aligned memory access e.g. to 128-bytes are much more efficient."]}),"\n"]})]}),"\n",(0,a.jsx)(n.h3,{id:"cache",children:"Cache"}),"\n",(0,a.jsxs)(n.p,{children:["Since they were dealing with large and streaming datasets, they miht have bypassed ",(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"L1"})})," or ",(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"L2"})})," cache. This can be accessible via PTX that allow to control these behaviour"]}),"\n",(0,a.jsx)(n.p,{children:"Below is the sample snippet showing the access. Loads from global memory and bypass both L1 and L2 cache."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-asm",children:".reg .u64 %addr;\n.reg .f32 %data;\n\nld.global.nc.f32 %data, [%addr]; \n"})}),"\n",(0,a.jsxs)(n.admonition,{type:"info",children:[(0,a.jsx)(n.mdxAdmonitionTitle,{}),(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"nc"})," means no cache",(0,a.jsx)(n.br,{}),"\n",(0,a.jsx)(n.strong,{children:"ld.global.nc.f32"})," - load 32 bit floating point value from global memory"]}),"\n"]})]}),"\n",(0,a.jsx)(n.h3,{id:"cache-controls",children:"Cache Controls"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:".volalite"})," - This modifer tells compiler that memory location can be modified by other threads/devices preventing complier for any optimization to ensure value in the memory remains constant."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:".wt"})," and ",(0,a.jsx)(n.code,{children:".wb"})," - These are write through and write back modifiers controling the cache write policy."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:".wt"})," writes to both cache and global memory while ",(0,a.jsx)(n.code,{children:".wb"})," writes only to cache but writes to global memory once cache data is evicted."]}),"\n",(0,a.jsxs)(n.admonition,{type:"info",children:[(0,a.jsx)(n.mdxAdmonitionTitle,{}),(0,a.jsxs)(n.p,{children:["Deepseek might have used these ",(0,a.jsx)(n.code,{children:"write-through"})," and ",(0,a.jsx)(n.code,{children:"write-back"})," modifiers to further optimize their workload."]})]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:".relaxed"}),",",(0,a.jsx)(n.code,{children:".acquire"}),",",(0,a.jsx)(n.code,{children:".release"}),",",(0,a.jsx)(n.code,{children:".acquire_release"})," modifiers are used when dealing with memory coherency between threads i.e. order of memory reads and writes"]}),"\n",(0,a.jsx)(n.admonition,{type:"info",children:(0,a.jsx)(n.p,{children:"Deepseek most likely used these modifiers when working with shared memory buffers which are accessed by multiple threads."})}),"\n",(0,a.jsx)(n.h3,{id:"prefetching",children:"Prefetching"}),"\n",(0,a.jsx)(n.p,{children:"For the predictible memory access, they could have use PTX's prefetch instructions to bring load the data in cache before it is needed hiding memory latency thus improving performance"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-ptx",children:"reg .u64 %addr;\nprefetch.global [%addr];\n\n"})}),"\n",(0,a.jsxs)(n.admonition,{type:"info",children:[(0,a.jsx)(n.mdxAdmonitionTitle,{}),(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"prefetch.global"})," Prefetch data into L1 cache."]}),"\n"]})]}),"\n",(0,a.jsx)(n.h4,{id:"prefetch-distance-and-hints",children:"Prefetch Distance and Hints"}),"\n",(0,a.jsx)(n.p,{children:"It is possible that these parameters are tuned to optimizing prefetching performance."}),"\n",(0,a.jsxs)(n.admonition,{type:"info",children:[(0,a.jsx)(n.mdxAdmonitionTitle,{}),(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Prefetch Distance"})," Number of memory location to prefetch ahead."]}),"\n"]}),(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Prefetch Hints"})," helps to understand tyoe of memory access patterns based on the type of hardware."]}),"\n"]})]}),"\n",(0,a.jsx)(n.h3,{id:"alignment--coalescing",children:"Alignment & Coalescing"}),"\n",(0,a.jsx)(n.p,{children:"Since PTX allow precise control over memory aligment and access patterns, they could use this to maximize memory bandwidth. Sample code below."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-ptx",children:".reg .u64 %base_addr;\n.reg .u32 %offset;\n.reg .f32 %data;\n\nmad.lo.u64 %addr, %offset, 4, %base_addr; // Assuming 4-byte floats\n\n// Load coalesced data\nld.global.v4.f32 {%data, %data+4, %data+8, %data+12}, [%addr];\n"})}),"\n",(0,a.jsxs)(n.admonition,{type:"info",children:[(0,a.jsx)(n.mdxAdmonitionTitle,{}),(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"ld.global.v4.f32"})," - Loads vector of 4 32-bit floating values from VRAM ensuring coalesced access."]}),"\n"]}),(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"mad.lo.u64"})," - Multiply add lower 64 bits for calculating memory address."]}),"\n"]})]}),"\n",(0,a.jsx)(n.h4,{id:"vectorized-loads",children:"Vectorized loads"}),"\n",(0,a.jsx)(n.p,{children:"They might have used vectorized loads which allow multiple data element to be transferred into a single memory transactions by maximizing memory bandwidth and also ensure these access are coalesced. Sample code below showing loading and storing 4 floats at once."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-ptx",children:".reg .u64 %addr;\n.reg .v4.f32 %data;\n\nld.global.v4.f32 %data, [%addr]; \nst.global.v4.f32 [%addr], %data;\n\n"})}),"\n",(0,a.jsx)(n.h3,{id:"shared-memory-optimization",children:"Shared Memory Optimization"}),"\n",(0,a.jsx)(n.p,{children:"Shared memory is organized in bands so to avoid conflicts they could have used multiple threads access the same banks simultaneously by arranging data carefully."}),"\n",(0,a.jsx)(n.p,{children:"It could also be possible that they might have used shared on-chip memory to reduce global access. Below code shows data being moved from global memory to shared memory and then use it."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-ptx",children:".shared .f32 shared_data[1024];\n.reg .u32 %thread_id;\n.reg .f32 %local_data;\n\n// Load data from global memory into shared memory\nld.global.f32 %local_data, [global_addr + %thread_id*4];\nst.shared.f32 [shared_data + %thread_id*4], %local_data;\n\n// Use data from shared memory\nld.shared.f32 %local_data, [shared_data + %thread_id*4];\n"})}),"\n",(0,a.jsx)(n.h2,{id:"inter-gpu-communcation",children:"Inter-GPU communcation"}),"\n",(0,a.jsx)(n.p,{children:"Allocate a portion of SM to improve communication by data compression and remove bottlenecks"}),"\n",(0,a.jsx)(n.h2,{id:"warp-level-optimization",children:"Warp Level Optimization"}),"\n",(0,a.jsx)(n.p,{children:"Fine-grain tunining again on warp which contains 32 threads on how they process instructions."}),"\n",(0,a.jsx)(n.p,{children:"NVIDIA GPUs execute threads in groups of 32, called warps. So PTX can allow developers to write warp-synchronous code, to make it more efficient."}),"\n",(0,a.jsx)(n.p,{children:"DeepSeek could have used warp-level primitives to perform warp-wide reductions and scans."}),"\n",(0,a.jsx)(n.h4,{id:"warp-shuffle-instructions",children:"Warp Shuffle Instructions:"}),"\n",(0,a.jsx)(n.p,{children:"PTX also provides shuffle instructions that allow threads within a warp to exchange data. It can be used to implement efficient inter-thread communication.\nOptimize data layout for shared memory."}),"\n",(0,a.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsx)(n.p,{children:"This article has outlined potential PTX optimizations employed by DeepSeek. These optimizations highlight DeepSeek's impressive ability to leverage fundamental hardware optimization, enabling them to develop models that effectively compete with OpenAI. The difficulty of these low level optimizations cannot be overstated."}),"\n",(0,a.jsx)(n.h2,{id:"next",children:"Next"}),"\n",(0,a.jsx)(n.p,{children:"In my next article, we will get into the details of how these optimizations happen in various stages of the architecture, from MHLA to Multi-token."}),"\n",(0,a.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf",children:"DeepSeek R1"}),(0,a.jsx)(n.br,{}),"\n",(0,a.jsx)(n.a,{href:"https://github.com/deepseek-ai/DeepEP",children:"DeepSeek EP Github"})]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>r});var t=i(6540);const a={},o=t.createContext(a);function s(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),t.createElement(o.Provider,{value:n},e.children)}},1666:e=>{e.exports=JSON.parse('{"permalink":"/notes/blog/DeepSeekPTX","source":"@site/blog/2025-03-01/LLMs/3.Advanced_ptx copy.md","title":"PTX Optimization","description":"How Deepseek optimized performance of the LLMs","date":"2025-03-01T00:00:00.000Z","tags":[{"inline":false,"label":"ML","permalink":"/notes/blog/tags/ML","description":"Machine Learning"},{"inline":false,"label":"ML Reserach","permalink":"/notes/blog/tags/ML-Research","description":"Machine Learning Research"}],"readingTime":5.04,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"C | Rust | Quantum Gravity | LLM Researcher","description":"I am a researcher in the field of quantum gravity and LLMs. I have a strong interest in C and Rust programming languages.\\n","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"DeepSeekPTX","title":"PTX Optimization","description":"How Deepseek optimized performance of the LLMs","image":"img/LLMWorkflow.png","authors":["rakesh"],"tags":["ML","ML Research"]},"unlisted":false,"prevItem":{"title":"PTX Basics","permalink":"/notes/blog/PTXIntro"},"nextItem":{"title":"Input Block - Tokenization","permalink":"/notes/blog/Tokenizer"}}')}}]);