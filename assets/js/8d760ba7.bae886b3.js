"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[6454],{1034:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>p,frontMatter:()=>a,metadata:()=>o,toc:()=>c});var o=n(722),r=n(4848),s=n(8453);const a={slug:"transformers",title:"Transformer Block - Multihead Attention",description:"Road to MHLA",image:"img/LLMWorkflow.png",authors:["rakesh"],tags:["ML","LLMs","ML Research"]},i="Introduction",l={authorsImageUrls:[void 0]},c=[];function u(e){const t={p:"p",strong:"strong",...(0,s.R)(),...e.components};return(0,r.jsxs)(t.p,{children:["In my previous blog post, I introduced Deepseek LLM's innovative parallel thread execution (PTX) mechanism and how they use it for GPU optimization. Today, we'll cover another foundational topic - ",(0,r.jsx)(t.strong,{children:"Multihead Attention (MHA)"})," before diving into Deepseek's second groundbreaking innovation known as ",(0,r.jsx)(t.strong,{children:"Multihead Latent Attention (MHLA)"}),"."]})}function p(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>i});var o=n(6540);const r={},s=o.createContext(r);function a(e){const t=o.useContext(s);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),o.createElement(s.Provider,{value:t},e.children)}},722:e=>{e.exports=JSON.parse('{"permalink":"/notes/blog/transformers","source":"@site/blog/2025-03-01/LLMs/5.MHLA.md","title":"Transformer Block - Multihead Attention","description":"Road to MHLA","date":"2025-03-01T00:00:00.000Z","tags":[{"inline":false,"label":"ML","permalink":"/notes/blog/tags/ML","description":"Machine Learning"},{"inline":false,"label":"LLM","permalink":"/notes/blog/tags/LLM","description":"Large Language Models"},{"inline":false,"label":"ML Reserach","permalink":"/notes/blog/tags/ML-Research","description":"Machine Learning Research"}],"readingTime":8.655,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"transformers","title":"Transformer Block - Multihead Attention","description":"Road to MHLA","image":"img/LLMWorkflow.png","authors":["rakesh"],"tags":["ML","LLMs","ML Research"]},"unlisted":false,"prevItem":{"title":"Input Block - Tokenization","permalink":"/notes/blog/Tokenizer"},"nextItem":{"title":"Dotfiles","permalink":"/notes/blog/dotfiles"}}')}}]);