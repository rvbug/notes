"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[4145],{8332:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"LLMs","metadata":{"permalink":"/notes/blog/LLMs","source":"@site/blog/2025-03-01/0-Introduction.md","title":"LLM Overview","description":"Large Language Models","date":"2025-03-01T00:00:00.000Z","tags":[{"inline":true,"label":"ML","permalink":"/notes/blog/tags/ml"}],"readingTime":1.87,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"LLMs","title":"LLM Overview","authors":["rakesh"],"tags":["ML"],"side_position":1},"unlisted":false,"nextItem":{"title":"Graphics Processing Unit (GPU)","permalink":"/notes/blog/GPU"}},"content":"## Large Language Models\\nOpenAI has been in the forefront of developing a sophisticated LLMs which was a blockbox until DeepSeek came along and they open sourced their LLMs, give us a peek into what happens behind the scenes.\\nThis is a series of blogs covering all the in components of DeepSeek including their architecture, optimization and my experiences in going through their research papers.\\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\n\\n## Background\\nDeepSeek has emerged as a significant disruptor in the AI industry, particularly in the realm of large language models (LLMs). It gained recognition for achieving high-level AI performance while utilizing significantly fewer computational resources compared to industry giants like OpenAI. \\n\\nDeepSeek leverages techniques like \\"Mixture of Experts (MoE)\\"  to optimize resource allocation,  innovations like Multi-Head Latent Attention (MLA) etc among many others. But the most important breakthrough was their commitment to open-source principles contributing to its rapid growth and influence. \\n\\n## Impact\\n\\nDeepSeek\'s success has put pressure on established AI companies to reconsider their development strategies and by democratizing their AI technology, it has made these models making it more accessible to smaller organizations and developing nations.\\n\\nThey developed a model much more (or similar) powerful than OpenAI with significantly less resource. DeepSeek\'s rise signifies a shift towards more efficient and accessible AI, challenging the dominance of OpenAI.\\n\\n## My Motivation\\n\\nDeepSeek\'s achievement, building a powerful model with optimized resources, resonates deeply. It\'s a testament to human ingenuity, a reminder that constraints often spark the most innovative solutions. When faced with limitations, we have an incredible ability to find efficient, impactful pathways forward.\\n\\n\\n## Components\\n\\nHere are the some of the components used by DeepSeek \\n\\n> Multi-Head Latent Attention \\n\\n> Mixture of Experts (MoE)   \\n\\n> Multi-Token Prediction   \\n\\n> Reinforcement Learning using GRPO  \\n\\n> GPU Optimization via PTX  \\n\\n![Overview](./img/LLMWorkflow.png)\\n\\n\\n# Blog Series\\nThis is a series of blog which gives you enough to start on the research journey of LLMs. We will start with quick introduction and overview of major components and writing equivalent rust code since my mission is to work on an unified programming language be it  Web Development, CLI programs, Systems Programming, Embedded systems and Robotics. Feel free to use Python. \\n\\nEnjoy your reading!\\n\\n## References\\n\\n[Reinforcement Learning](https://arxiv.org/pdf/2501.12948)  \\n[Roadmap for LLMs](https://github.com/rvbug/NLP)"},{"id":"GPU","metadata":{"permalink":"/notes/blog/GPU","source":"@site/blog/2025-03-01/1-Graphics Processing Unit.md","title":"Graphics Processing Unit (GPU)","description":"In my last blog, I introduced to DeepSeek and some of the components they use for R1 which is a reasoning model.","date":"2025-03-01T00:00:00.000Z","tags":[{"inline":true,"label":"ML","permalink":"/notes/blog/tags/ml"},{"inline":true,"label":"LLMs","permalink":"/notes/blog/tags/ll-ms"}],"readingTime":2.78,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"GPU","title":"Graphics Processing Unit (GPU)","authors":["rakesh"],"tags":["ML","LLMs"]},"unlisted":false,"prevItem":{"title":"LLM Overview","permalink":"/notes/blog/LLMs"},"nextItem":{"title":"PTX Basics","permalink":"/notes/blog/PTXIntro"}},"content":"In my last blog, I introduced to DeepSeek and some of the components they use for R1 which is a reasoning model.\\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\n## *CPU vs GPU*\\nBut what is the difference between a CPU and GPU? Here are some of them...\\n\\n**A**> CPUs are for general-purpose computing where as GPUs are optimized for performing same operations on multuple data points simultaneously achieveing in high level of parallelism.  \\n\\n**B**> CPUs are best suited which runnign complex logic where as GPUs are ideally for massively parallel computations like graphics rendering, deep learning and scientific simulations.  \\n\\n**C**> CPU is optimized for low-latency access to relatively small amount of memory. GPU is typically used for high-bandwidth to large amount of dataset in parallel.\\n\\n\\n## *GPU*\\nBefore we get into PTX, let me first give you a quick introduction to GPU, it\'s architecture, CUDA programming.\\n\\nThe cuda program workflow is as shown below. It starts off with your program which has extension of \\".cu\\", this code can be written in C, C++ or Fortran but then there are other languages like rust which has cuda crates too.\\n\\n## *Basic Flow*\\n\\nThis is a very basic flow of a CUDA program.\\n\\n![Cuda Program Overview](img/CUDAOverview.png)\\n\\n## *Detailed Workflow*\\n\\n![CUDA Workflow](img/CUDAWorkflow.png)\\n\\nTypically these  programs has both CPU and GPU instructions running on host machine. The CPU piece of code is called the host code and GPU code section is called device code typically which has __global__ and __device__ function. \\n\\nThis code is compiled using NVCC which separates both the CPU and GPU code. The CPU code uses CPU complier like GCC or MSVC which convert it to the object code. \\n\\nAt the same time the 1st pass of the GPU converts the GPU code is converted to PTX code. This is a low level IR (Intermediate Representation) which is then compiled to convert it to device specific code known as  SASS code. \\n\\nFinal stage is to link the host object code with SASS and run the code to run on host machine.\\n\\n\\nPTX is an abstraction layer helping code portabiloty between NVIDIA decides. This helps tools and libraries to manupulate code code before GPU execution. This is what DeepSeek team did whcih we will come back to later.\\n\\n## *Structure*\\nBefore we delve deeper into the PTX optimization by DeepSeek, let us first talk about GPU .\\n\\n![GPU Structure](<img/GPU Structure.png>)\\n\\nThere are : \\n1. 7 Graphic Processing Clusters (GPC) \\n2. Each GPC had 12 Streaming Multiprocessors (SM). \\n3. Every SM will have 4 Warps and 1 Ray Tracing core \\n4. A single Warp will have 32 CUDA core and 1 Tensor Code.\\n5. 12 Graphic Memory Controllers\\n6. Two L2 Cache of 6MB SRAM each\\n7. NVLink\\n8. PCIe Interface\\n\\n\\nSo in total in one NVDIA GPU there are about :\\n\\n>  `10752` CUDA cores   \\n\\n> `336` Tensor cores    \\n\\n>  `84` Ray tracing cores  \\n\\n## *Core*\\n\\n![CUDA Cores](img/Cores.png)\\n\\nHere are the three cores available in these GPUs\\n\\n- CUDA Core is used for game and game engines. \\n- Tensor Core is exclusively for Matrix Multiplication and Geometric Transformation which is used in AI/ML\\n- Ray Tracing is used ror Ray Tracing algorithms.\\n\\n\\n## Next Steps\\n\\nWith basic introduction to GPU and a workflow how a cuda program gets executed, let us move to PTX. See you in my next arcticle."},{"id":"PTXIntro","metadata":{"permalink":"/notes/blog/PTXIntro","source":"@site/blog/2025-03-01/2. Intro_to_ptx.md","title":"PTX Basics","description":"Parallel Thread Execution (PTX) is a virtual machine instruction set architecture and can be thought of as the assembly language for NVDIA GPUs","date":"2025-03-01T00:00:00.000Z","tags":[{"inline":true,"label":"ML","permalink":"/notes/blog/tags/ml"},{"inline":true,"label":"ML Research","permalink":"/notes/blog/tags/ml-research"}],"readingTime":9.19,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"PTXIntro","title":"PTX Basics","authors":["rakesh"],"tags":["ML","ML Research"]},"unlisted":false,"prevItem":{"title":"Graphics Processing Unit (GPU)","permalink":"/notes/blog/GPU"},"nextItem":{"title":"PTX Optimization","permalink":"/notes/blog/DeepSeekPTX"}},"content":"Parallel Thread Execution (PTX) is a virtual machine instruction set architecture and can be thought of as the assembly language for NVDIA GPUs\\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\n# PTX Syntax \\n\\nHere are few syntax of PTX for your references.\\n\\n## **Basic Structure**\\n\\n```ptx\\n.version 7.0                    // PTX version\\n.target sm_70                   // Target architecture\\n.address_size 64                // 64-bit addressing\\n\\n.visible .entry kernel_name(   // Kernel name and declaration\\n    .param .u64 param1,        // Parameters\\n    .param .f32 param2\\n)\\n{\\n    // Kernel body\\n}\\n```\\n\\n## **Registers**\\n\\n```ptx\\n.reg .b32 %r<5>;    // 5 32-bit registers %r0 through %r4\\n.reg .f32 %f<3>;    // 3 single-precision float\\n.reg .b64 %rd<2>;   // 2 64-bit \\n.reg .pred %p<2>;   // 2 predicate registers (conditionals)\\n\\n```\\n\\n\\n## **Instructions**\\n\\n```ptx\\nmov.u32 %r1, %tid.x;       // Move thread(ID) to %r1\\nadd.s32 %r3, %r1, %r2;     // Add int\\nmul.f32 %f3, %f1, %f2;     // Mul floats\\nsetp.lt.s32 %p1, %r1, %r2; // Set predicate if r1 < r2\\n@%p1 bra label;            // Conditional branch\\n\\n```\\n\\n## **Memory Operations**\\n\\n```ptx\\nld.param.u64 %rd1, [param1];       // Load param into register\\nld.global.f32 %f1, [%rd1];         // Load from global mem\\nst.global.f32 [%rd2], %f2;         // Store to global mem\\nld.shared.f32 %f3, [%rd3];         // Load from shared mem\\nld.global.v4.f16 {%f1, %f2, %f3, %f4}, [%rd1];  // Vector load\\n\\n```\\n\\n## **Special Registers**\\n\\n```ptx\\n%tid.x, %tid.y, %tid.z     // Thread (ID) within a block\\n%ctaid.x, %ctaid.y, %ctaid.z   // Block (ID) within a grid\\n%ntid.x, %ntid.y, %ntid.z  // Block dimensions (threads per block)\\n\\n```\\n\\n## **Math Operaations**\\n\\n```ptx\\nadd.f32 %f3, %f1, %f2;     // Add\\nsub.f32 %f3, %f1, %f2;     // Sub\\nmul.f32 %f3, %f1, %f2;     // Mul\\ndiv.f32 %f3, %f1, %f2;     // Div\\nmad.f32 %f4, %f1, %f2, %f3;  // Multiply & add: (f4 = f1*f2+f3)\\n```\\n\\n## **Tensor Core Operations**\\n\\n```ptx\\n// Matrix multiply-accumulate using tensor cores\\nmma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 \\n    {%f5, %f6, %f7, %f8},   // Destination registers\\n    {%f1, %f2},             // A matrix registers\\n    {%f3, %f4},             // B matrix registers\\n    {%f5, %f6, %f7, %f8};   // C matrix registers (accumulator)\\n\\n```\\n\\n## **Control Flow**\\n\\n```ptx\\nbra label;           // Unconditional branch\\n@%p1 bra label;      // Conditional branch if predicate = true\\nret;                 // Return from kernel\\n```\\n\\n\\n## Code Sample\\n\\nNow that we know the the basic syntax of PTX, here is one simple rust program for vector addition. \\n\\n```C\\n\\n#include <stdio.h>\\n#include <cuda.h>\\n#include <cuda_runtime.h>\\n\\n// Kernel function to add the elements of two arrays\\n__global__ void vectorAdd(int *a, int *b, int *c, int n) {\\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\\n    if (i < n) {\\n        c[i] = a[i] + b[i];\\n    }\\n}\\n\\nint main() {\\n    int n = 1000;\\n    int size = n * sizeof(int);\\n    int *a, *b, *c;\\n    int *d_a, *d_b, *d_c;\\n\\n    // Allocate memory on the host\\n    a = (int *)malloc(size);\\n    b = (int *)malloc(size);\\n    c = (int *)malloc(size);\\n\\n    // Initialize the arrays\\n    for (int i = 0; i < n; i++) {\\n        a[i] = i;\\n        b[i] = i * 2;\\n    }\\n\\n    // Allocate memory on the device\\n    cudaMalloc((void **)&d_a, size);\\n    cudaMalloc((void **)&d_b, size);\\n    cudaMalloc((void **)&d_c, size);\\n\\n    // Copy data from host to device\\n    cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\\n    cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\\n\\n    // Launch the vectorAdd kernel\\n    int threadsPerBlock = 256;\\n    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\\n    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, n);\\n\\n    // Copy the result from device to host\\n    cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\\n\\n    // Free device memory\\n    cudaFree(d_a);\\n    cudaFree(d_b);\\n    cudaFree(d_c);\\n\\n    // Free host memory\\n    free(a);\\n    free(b);\\n    free(c);\\n\\n    return 0;\\n}\\n\\n```\\n### Generating output\\n\\nLet us compile the code using `nvcc` compiler\\n\\n```bash\\n$> nvcc vector.cu -o vector\\n$> ./my_kernel\\n```\\n\\n```bash\\n# output\\nc[0] = 0\\nc[1] = 3\\nc[2] = 6\\nc[3] = 9\\nc[4] = 12\\nc[5] = 15\\nc[6] = 18\\nc[7] = 21\\nc[8] = 24\\nc[9] = 27\\n\\n```\\n\\n\\n\\n### Code Explaination\\n\\n\\n```C\\n// This is for CUDA runtime functions\\n#include <cuda_runtime.h> \\n```\\n\\n#### Kernel Function \\n\\n* *`__global__`* specifices that this is CUDA kernel that runs on GPU    \\n* Three float arrays are pointers along with array size  `a`, `b` and `c`\\n* *`blockIdx.x`* is the block index. \\n* *`blockDim.x`* is number of thread per block\\n* *`threadIdx.x`* is thread index within a block\\n* Calculate unique ID for each thread to process a different array element\\n\\n\\n\\n```C\\n// CUDA kernel for vector addition\\n__global__ void vectorAdd(float *a, float *b, float *c, int n)\\n{\\n    // Calculate global thread ID\\n    int id = blockIdx.x * blockDim.x + threadIdx.x;\\n\\n    // To make sure we don\'t go out of bounds\\n    if (id < n)\\n        c[id] = a[id] + b[id];\\n}\\n```\\n\\n#### Main function\\n\\n```C\\nint main()\\n{\\n    // Vector size\\n    int n = 1000000;               // One million elements\\n    size_t bytes = n * sizeof(float);  // Calculate memory size in bytes\\n    // Allocate host memory\\n    float *h_a = (float*)malloc(bytes);  // Allocate memory for array a\\n    float *h_b = (float*)malloc(bytes);  // Allocate memory for array b\\n    float *h_c = (float*)malloc(bytes);  // Allocate memory for results\\n    // Initialize vectors on host\\n    for (int i = 0; i < n; i++)\\n    {\\n        h_a[i] = 1.0f;  // All elements in a are 1.0\\n        h_b[i] = 2.0f;  // All elements in b are 2.0\\n    }\\n}\\n```\\n\\n#### GPU Memory Allocation\\n\\n```C\\n// Allocate device memory\\n    float *d_a, *d_b, *d_c;            // Declare device pointers\\n    cudaMalloc(&d_a, bytes);           // Allocate memory on GPU for a\\n    cudaMalloc(&d_b, bytes);           // Allocate memory on GPU for b\\n    cudaMalloc(&d_c, bytes);           // Allocate memory on GPU for c\\n\\n```\\n\\n#### GPU Data Transfer\\n\\n```C\\n// Copy data from host to device\\n    cudaMemcpy(d_a, h_a, bytes, cudaMemcpyHostToDevice);  // Copy a to GPU\\n    cudaMemcpy(d_b, h_b, bytes, cudaMemcpyHostToDevice);  // Copy b to GPU\\n\\n```\\n\\n#### Kernel Launch Configuration\\n\\n```C\\n// Set up execution configuration\\n    int blockSize = 256;                         // 256 threads per block\\n    int gridSize = (n + blockSize - 1) / blockSize;  // Calculate grid size\\n// This formula ensures we have enough blocks to cover all elements\\n// Launch kernel\\n    vectorAdd<<<gridSize, blockSize>>>(d_a, d_b, d_c, n);\\n    // <<<>>> is special CUDA syntax for kernel launch configuration\\n    // gridSize = number of blocks, blockSize = threads per block\\n```\\n\\n#### Results and Cleanup\\n\\n```C\\n// Copy result back to host\\ncudaMemcpy(h_c, d_c, bytes, cudaMemcpyDeviceToHost);  // Copy results from GPU to CPU\\n\\n// Free memory\\ncudaFree(d_a);  // Free GPU memory for a\\ncudaFree(d_b);  // Free GPU memory for b\\ncudaFree(d_c);  // Free GPU memory for c\\nfree(h_a);      // Free CPU memory for a\\nfree(h_b);      // Free CPU memory for b\\nfree(h_c);      // Free CPU memory for c\\n\\n```\\n\\n\\n## PTX Code\\n\\nTo extract PTX  from the above code, try this the following command.\\n\\n```bash \\n$> nvcc -ptx vector.cu -o vector.ptx\\n```\\n\\n### PTX file \\n\\n```ptx\\n\\n.visible .entry vectorAdd(\\n    .param .u64 vectorAdd_param_0,  // Pointer to array a\\n    .param .u64 vectorAdd_param_1,  // Pointer to array b\\n    .param .u64 vectorAdd_param_2,  // Pointer to array c\\n    .param .u32 vectorAdd_param_3   // Parameter n (size)\\n)\\n{\\n    .reg .pred  %p<2>;          // Predicate registers\\n    .reg .f32   %f<4>;          // Float registers\\n    .reg .b32   %r<6>;          // 32-bit registers\\n    .reg .b64   %rd<11>;        // 64-bit registers\\n\\n    // Load parameters into registers\\n    ld.param.u64    %rd1, [vectorAdd_param_0];\\n    ld.param.u64    %rd2, [vectorAdd_param_1];\\n    ld.param.u64    %rd3, [vectorAdd_param_2];\\n    ld.param.u32    %r2, [vectorAdd_param_3];\\n    \\n    // Calculate thread ID\\n    mov.u32         %r3, %ctaid.x;    // Get block index\\n    mov.u32         %r4, %ntid.x;     // Get block size\\n    mov.u32         %r5, %tid.x;      // Get thread index within block\\n    mad.lo.s32      %r1, %r3, %r4, %r5;  // Calculate global thread ID: blockIdx * blockDim + threadIdx\\n    \\n    // Check if thread ID is within bounds\\n    setp.ge.s32     %p1, %r1, %r2;    // Set predicate if thread ID >= n\\n    @%p1 bra        BB0_2;            // If true, jump to the end (BB0_2 label)\\n    \\n    // Calculate memory addresses\\n    cvta.to.global.u64  %rd4, %rd1;   // Convert array a pointer to global address\\n    mul.wide.s32    %rd5, %r1, 4;     // Multiply thread ID by 4 (size of float)\\n    add.s64         %rd6, %rd4, %rd5; // Calculate address for a[id]\\n    \\n    cvta.to.global.u64  %rd7, %rd2;   // Convert array b pointer to global address\\n    add.s64         %rd8, %rd7, %rd5; // Calculate address for b[id]\\n    \\n    // Load values, add them, and store result\\n    ld.global.f32   %f1, [%rd6];      // Load a[id]\\n    ld.global.f32   %f2, [%rd8];      // Load b[id]\\n    add.f32         %f3, %f1, %f2;    // Add them: c[id] = a[id] + b[id]\\n    \\n    cvta.to.global.u64  %rd9, %rd3;   // Convert array c pointer to global address\\n    add.s64         %rd10, %rd9, %rd5; // Calculate address for c[id]\\n    st.global.f32   [%rd10], %f3;     // Store the result in c[id]\\n    \\nBB0_2:                                // End label\\n    ret;                              // Return from kernel\\n}\\n\\n```\\n\\n### PTX Code\\n\\nLet us now review the PTX code\\n\\n### Entry Point \\n\\nThis section declares entry point for the kernel followed by 4 paramaters which is a pointer to the variable a, b, c and size n.\\n\\n```ptx\\n\\n.visible .entry vectorAdd(           // Declares entry point for kernel\\n    .param .u64 vectorAdd_param_0,   // First parameter (pointer to array a)\\n    .param .u64 vectorAdd_param_1,   // Second parameter (pointer to array b)\\n    .param .u64 vectorAdd_param_2,   // Third parameter (pointer to array c)\\n    .param .u32 vectorAdd_param_3    // Fourth parameter (size n)\\n)\\n\\n```\\n\\n### Register Declaration\\n\\nDeclating Predicate registers for conditions, float registers, 32 bit int and register for addresses\\n\\n```ptx\\n    .reg .pred  %p<2>;          // Predicate r\\n    .reg .f32   %f<4>;          // Float \\n    .reg .b32   %r<6>;          // 32-bit int\\n    .reg .b64   %rd<11>;        // 64-bit for addresses\\n```\\n\\n### Parameter Loading\\n\\nThis section loads parameters from kernel into registers. `ld` is for load.\\n\\n```ptx\\n\\n    ld.param.u64    %rd1, [vectorAdd_param_0];  // Load pointer to array a\\n    ld.param.u64    %rd2, [vectorAdd_param_1];  // Load pointer to array b\\n    ld.param.u64    %rd3, [vectorAdd_param_2];  // Load pointer to array c\\n    ld.param.u32    %r2, [vectorAdd_param_3];   // Load size n\\n```\\n\\n### Thread ID\\nNow calcuate unique thread Id using built-on registers.   \\n\\nFirst get the current block index into `%r3`, then get number of threads per block into `%r4` and then get thread index within this block into `%r5`. `mad` is multiply and add in a single instruction and calculate ID by using `blockIdx * blockDim + threadIdx`.    \\n\\n\\n```ptx\\n    mov.u32         %r3, %ctaid.x;    \\n    mov.u32         %r4, %ntid.x;     \\n    mov.u32         %r5, %tid.x;      \\n    mad.lo.s32      %r1, %r3, %r4, %r5; \\n```\\n\\n### Bounds Checking\\n\\nHere we check  if thread ID is within bounds of the array and then Set predicate `%p1` if thread ID >= n. If true then jump to return label `BB0_2`\\n\\n```ptx\\n// \\nsetp.ge.s32     %p1, %r1, %r2;    // \\n@%p1 bra        BB0_2;            // If true, jump to the return label (BB0_2)\\n\\n```\\n\\n### Memory Calculations\\n\\nCalculate memory address for array elements. Covert array `a` pointer to global address using `cvta`.  Multiply `mul` thread Id by 4 which is the size of the float followed by adding address for `a`.  \\n\\n```ptx\\n\\n    cvta.to.global.u64  %rd4, %rd1;   \\n    mul.wide.s32    %rd5, %r1, 4;     \\n    add.s64         %rd6, %rd4, %rd5; \\n    \\n    cvta.to.global.u64  %rd7, %rd2;   // Convert array b pointer to global address\\n    add.s64         %rd8, %rd7, %rd5; // Calculate address for b[id]\\n\\n```\\n\\n### Load, Add and Store\\n\\nLoad values from arrays, perform addition, and store result\\n\\n```ptx\\n    ld.global.f32   %f1, [%rd6];      // Load a[id] into register %f1\\n    ld.global.f32   %f2, [%rd8];      // Load b[id] into register %f2\\n    add.f32         %f3, %f1, %f2;    // Add them: %f3 = %f1 + %f2\\n    \\n    cvta.to.global.u64  %rd9, %rd3;   // Convert array c pointer to global address\\n    add.s64         %rd10, %rd9, %rd5; // Calculate address for c[id]\\n    st.global.f32   [%rd10], %f3;     // Store the result in c[id]\\n\\n```\\n\\n### Return from Kernel\\n\\n```ptx\\nBB0_2:  // Label for our return point\\nret;                             \\n```"},{"id":"DeepSeekPTX","metadata":{"permalink":"/notes/blog/DeepSeekPTX","source":"@site/blog/2025-03-01/3.Advanced_ptx copy.md","title":"PTX Optimization","description":"Let us now get into the meat of the PTX and how Deepseek approached optimization for their H800 GPUs.","date":"2025-03-01T00:00:00.000Z","tags":[{"inline":true,"label":"ML","permalink":"/notes/blog/tags/ml"},{"inline":true,"label":"ML Research","permalink":"/notes/blog/tags/ml-research"}],"readingTime":4.71,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"DeepSeekPTX","title":"PTX Optimization","authors":["rakesh"],"tags":["ML","ML Research"]},"unlisted":false,"prevItem":{"title":"PTX Basics","permalink":"/notes/blog/PTXIntro"},"nextItem":{"title":"Quantum Gravity - Intestellar","permalink":"/notes/blog/Introduction"}},"content":"Let us now get into the meat of the PTX and how Deepseek approached optimization for their H800 GPUs. \\n\\nSince DeepSeek\'s specific PTX implementations are proprietary, this article focuses on the optimization strategies inferred from their research papers and related discussions. We\'ll explore few of them within their architecture. For example **Multi-head Latent Attention** (MHLA) employs a modified Key and Value cache approach, differing from the standard transformer KV cache concept, to enhance efficiency.\\n\\n\x3c!-- truncate --\x3e\\n\\n----\\n\\n# Overview\\nDeepSeek, particularly with their R1 model, has implemented significant optimizations across both training and inference phases. We will delve into these broader optimizations in a separate, more detailed article. In this one, however, our focus will be exclusively on PTX. \xa0 \\n\\nPTX empowers developers with the ability to perform low-level optimizations, granting fine-grained control over register allocation, thread execution, and memory access patterns.\\n\\n## Register Allocation\\n\\n1. Allowing manually optimizing the register allocation, the latency could have been reduced.\\n\\n2. Fine-tune thread scheduling allowing them to maximize parallelism across the streaming multiprocessors.\\n\\n\\n## Custom Memory Management\\n\\nImplementing custome PTX instructions for accessing memory including global VRAM access by bypassing L1 and L2 cache in a very specific way allowing increasing data transfer pattern thus improving memory bandwidth.\\n\\n:::tip  \\n- **Global VRAM** is largest and slowest memory on the GPU\\n- **Cache** can also introduce overhead and may not always be effective\\n- **Coalesced Access** - Accessing contiguous memory locations in a single transaction significantly improves memory bandwidth. \\n- **Memory Access** - Aligned memory access e.g. to 128-bytes are much more efficient.\\n:::\\n\\n\\n### Cache\\n\\nSince they were dealing with large and streaming datasets, they miht have bypassed **`L1`** or **`L2`** cache. This can be accessible via PTX that allow to control these behaviour\\n\\nBelow is the sample snippet showing the access. \\nload  from global memory and bypass both L1 and L2 cache.  \\n\\n```asm\\n.reg .u64 %addr;\\n.reg .f32 %data;\\n\\nld.global.nc.f32 %data, [%addr]; \\n```\\n\\n:::info  \\n> **nc** means no cache  \\n> **ld.global.nc.f32** - load 32 bit floating point value from global memory \\n:::\\n\\n### Cache Controls\\n\\n`.volalite` - This modifer tells compiler that memory location can be modified by other threads/devices preventing complier for any optimization to ensure value in the memory remains constant.\\n\\n`.wt` and `.wb` - These are write through and write back modifiers controling the cache write policy. \\n\\n`.wt` writes to both cache and global memory while `.wb` writes only to cache but writes to global memory once cache data is evicted.\\n\\n:::info \\nDeepseek might have used these `write-through` and `write-back` modifiers to further optimize their workload.\\n:::\\n\\n`.relaxed`,`.acquire`,`.release`,`.acquire_release` modifiers are used when dealing with memory coherency between threads i.e. order of memory reads and writes\\n\\n:::info\\nDeepseek most likely used these modifiers when working with shared memory buffers which are accessed by multiple threads.\\n:::\\n\\n### Prefetching\\n\\nFor the predictivble memory access, they could have use PTX\'s prefetch instructions to bring load the data in cache before it is needed hiding memory latency thus improving performance\\n\\n```ptx\\nreg .u64 %addr;\\nprefetch.global [%addr];\\n\\n```\\n:::info  \\n> **prefetch.global** Prefetch data into L1 cache.\\n:::\\n\\n\\n#### Prefetch Distance and Hints\\nIt is possible that these parameters are tuned to optimizing prefetching performance.\\n\\n:::info  \\n> **Prefetch Distance** Number of memory location to prefetch ahead.  \\n\\n> **Prefetch Hints** helps to understand tyoe of memory access patterns based on the type of hardware.  \\n:::\\n\\n\\n### Alignment & Coalescing \\nSince PTX allow precisee control over memory aligbment and access paters, they could use this to maximize memoryt bandwidth. Sample code below.\\n\\n```ptx\\n.reg .u64 %base_addr;\\n.reg .u32 %offset;\\n.reg .f32 %data;\\n\\nmad.lo.u64 %addr, %offset, 4, %base_addr; // Assuming 4-byte floats\\n\\n// Load coalesced data\\nld.global.v4.f32 {%data, %data+4, %data+8, %data+12}, [%addr];\\n```\\n\\n:::info  \\n> **ld.global.v4.f32** - Loads vector of 4 32-bit floating values from VRAM ensuring coalesced access.\\n\\n> **mad.lo.u64** - Multiply add lower 64 bits for calculating memory address.\\n:::\\n\\n#### Vectorized loads\\n\\nDeepseek might have used vectorized loads which allow multiple data element to be transferred into a single memory transactions by maximizing memory bandwidth and also ensure these access are coalesced. Sample code below showing loading and storing 4 floats at once.\\n\\n```ptx\\n.reg .u64 %addr;\\n.reg .v4.f32 %data;\\n\\nld.global.v4.f32 %data, [%addr]; \\nst.global.v4.f32 [%addr], %data;\\n\\n```\\n\\n\\n### Shared Memory Optimization\\n\\nShared memory is organized in bands so to avoid conflicts they could have used multiple threads access the same banks simultaneously by arranging data carefully.\\n\\nIt could also be possible that they might have used shared on-chip memory to reduce global access. Below code shows data being moved from global memory to shared memory and then use it.\\n\\n```ptx\\n.shared .f32 shared_data[1024];\\n.reg .u32 %thread_id;\\n.reg .f32 %local_data;\\n\\n// Load data from global memory into shared memory\\nld.global.f32 %local_data, [global_addr + %thread_id*4];\\nst.shared.f32 [shared_data + %thread_id*4], %local_data;\\n\\n// Use data from shared memory\\nld.shared.f32 %local_data, [shared_data + %thread_id*4];\\n```\\n\\n## Inter-GPU communcation\\n\\nAllocate a portion of SM to improve communication by data compression and remove bottlenecks\\n\\n## Warp Level Optimization\\nFine-grain tunining again on warp which contains 32 threads on how they process instructions.\\n\\nNVIDIA GPUs execute threads in groups of 32, called warps. So PTX can allow developers to write warp-synchronous code, to amke it more significantly efficient.\\n\\nDeepSeek might have used warp-level primitives to perform warp-wide reductions and scans.\\n\\n#### Warp Shuffle Instructions:\\nPTX also provides shuffle instructions that allow threads within a warp to exchange data. It can be used to implement efficient inter-thread communication.\\nOptimize data layout for shared memory.\\n\\n\\n## Next\\nIn my next article, we will get into the details of how these optimization happens in various stages of the architecture.\\n\\n\\n## References\\n[DeepSeek R1](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf)\\n[DeepSeek EP Github](https://github.com/deepseek-ai/DeepEP)"},{"id":"Introduction","metadata":{"permalink":"/notes/blog/Introduction","source":"@site/blog/2024-01-01/interstellar-science-blog.md","title":"Quantum Gravity - Intestellar","description":"Introduction","date":"2024-01-01T00:00:00.000Z","tags":[{"inline":true,"label":"Quantum Gravity","permalink":"/notes/blog/tags/quantum-gravity"}],"readingTime":2.985,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"Introduction","title":"Quantum Gravity - Intestellar","authors":["rakesh"],"tags":["Quantum Gravity"]},"unlisted":false,"prevItem":{"title":"PTX Optimization","permalink":"/notes/blog/DeepSeekPTX"},"nextItem":{"title":"Nix Introduction","permalink":"/notes/blog/Nix OS"}},"content":"## Introduction\\n\\nWhen Christopher Nolan\'s film \\"Interstellar\\" hit theaters, it wasn\'t just another sci-fi movie\u2014it was a scientifically rigorous exploration of some of the most mind-bending concepts in modern physics. Behind the film\'s stunning visuals was renowned theoretical physicist Kip Thorne, who ensured that the movie\'s scientific foundations were as accurate as possible.\\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\n## Black Holes: Cosmic Enigmas\\n\\nOne of the most fascinating aspects of the book is its deep dive into black holes. Thorne explains these cosmic phenomena not as simple \\"cosmic vacuum cleaners,\\" but as complex gravitational systems that warp the very fabric of spacetime.\\n\\n### The Mathematics of Black Holes\\n\\nAt the heart of black hole physics is Einstein\'s field equation:\\n\\n*G*\u03bc\u03bd = 8\u03c0G/*c*\u2074 *T\u03bc\u03bd*\\n\\nWhere:\\n- *G*\u03bc\u03bd represents the curvature of spacetime\\n- *T\u03bc\u03bd* describes the distribution of matter and energy\\n- G is the gravitational constant\\n- *c* is the speed of light\\n\\nThis equation essentially tells us that mass and energy bend spacetime, creating what we experience as gravity.\\n\\n## Wormholes: Cosmic Shortcuts\\n\\nThorne explores the theoretical possibility of wormholes\u2014hypothetical tunnels through space-time that could create shortcuts for long journeys across the universe. While purely theoretical, the mathematics suggests they\'re not impossible.\\n\\nA simplified wormhole equation can be represented as:\\n\\n*d* = *c* * *t* / \u221a(1 - *v*\xb2/(*c*\xb2))\\n\\nWhere:\\n- *d* is distance\\n- *c* is the speed of light\\n- *t* is time\\n- *v* is velocity\\n\\n## Time Dilation: Not Just Science Fiction\\n\\nOne of the most mind-bending concepts in the book is time dilation\u2014how time can move differently depending on gravitational forces and velocity. Near a massive object like a black hole, time actually slows down relative to distant observers.\\n\\nThe time dilation factor can be calculated using:\\n\\n*t*\' = *t* / \u221a(1 - *v*\xb2/(*c*\xb2))\\n\\nWhere:\\n- *t*\' is the dilated time\\n- *t* is the proper time\\n- *v* is velocity\\n- *c* is the speed of light\\n\\n## Gravitational Waves: Ripples in Spacetime\\n\\nThorne was instrumental in the scientific understanding of gravitational waves\u2014ripples in the fabric of spacetime caused by massive cosmic events. These waves were theoretically predicted by Einstein and first directly observed in 2015, vindicating decades of scientific prediction.\\n\\nThe amplitude of a gravitational wave can be described by:\\n\\n*h* = 4G*M*/*c*\u2074*r*\\n\\nWhere:\\n- *h* is the wave amplitude\\n- *G* is the gravitational constant\\n- *M* is the mass of the generating object\\n- *c* is the speed of light\\n- *r* is the distance from the source\\n\\n## Beyond the Equations: A Human Story\\n\\nWhat makes Thorne\'s book remarkable is how it bridges pure scientific theory with human imagination. It demonstrates that the most complex scientific concepts can be understood with patience, curiosity, and a sense of wonder.\\n\\n## Conclusion\\n\\n\\"The Science of Interstellar\\" is more than a companion to a movie\u2014it\'s a gateway to understanding some of the most profound mysteries of our universe. It shows us that reality can be stranger and more beautiful than fiction.\\n\\n### Key Takeaways\\n- Black holes are not simple cosmic vacuums but complex spacetime phenomena\\n- Wormholes, while theoretical, are mathematically possible\\n- Time is relative and can be dramatically affected by gravity and velocity\\n- Gravitational waves confirm some of Einstein\'s most radical predictions\\n\\n## About the Author\\n\\nKip Thorne is not just a physicist, but a scientific consultant who ensured that \\"Interstellar\\" pushed the boundaries of scientific accuracy in cinema. His work bridges theoretical physics with popular understanding.\\n\\n*Disclaimer: While these equations are simplified, they provide a glimpse into the mathematical foundations of these complex cosmic phenomena.*"},{"id":"Nix OS","metadata":{"permalink":"/notes/blog/Nix OS","source":"@site/blog/2023-03-05/Nix.md","title":"Nix Introduction","description":"Introduction to Nix Functional programming","date":"2023-03-05T00:00:00.000Z","tags":[{"inline":false,"label":"nix","permalink":"/notes/blog/tags/nix","description":"Nix OS and Functional Package Manager"}],"readingTime":2.245,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"Nix OS","title":"Nix Introduction","authors":["rakesh"],"tags":["nix"]},"unlisted":false,"prevItem":{"title":"Quantum Gravity - Intestellar","permalink":"/notes/blog/Introduction"},"nextItem":{"title":"Nix Pkg Manager","permalink":"/notes/blog/Nix Packages"}},"content":"Introduction to Nix Functional programming\\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\n\\n##  Core Concepts\\n\\n### 1. Immutability\\n\\nIn Nix, variables are immutable once defined:\\n\\n```nix\\nlet\\n  x = 5;\\n  # x = 6;  # This would cause an error\\nin x\\n```\\n\\n### 2. Let Bindings\\nNix uses let for local variable definitions:\\n```nix\\nlet\\n  greeting = \\"Hello\\";\\n  name = \\"World\\";\\nin greeting + \\" \\" + name\\n```\\n\\n### 3. Functions and Lambda Expressions\\nBasic Function Definition\\n\\n```nix\\nlet\\n  greet = name: \\"Hello, \\" + name;\\nin greet \\"Alice\\"  # Returns \\"Hello, Alice\\"\\n```\\n\\n### Multiple Parameter Functions\\n```nix\\nlet\\n  multiply = x: y: x * y;\\n  result = multiply 3 4;  # Returns 12 in result\\n```\\n\\n### 4. Pattern Matching and Attribute Sets\\nAttribute Set Creation\\n\\n```nix\\n{\\n  name = \\"John\\";\\n  age = 30;\\n  skills = [\\"Nix\\" \\"Functional Programming\\"];\\n}\\n```\\n\\n### Accessing Attributes\\n```nix\\nlet\\n  person = { name = \\"Alice\\"; age = 25; };\\nin person.name  # Returns \\"Alice\\"\\n```\\n\\n\\n### 5. Function with Attribute Sets\\n```nix\\nlet\\n  greet = { name, age }: \\"Hello ${name}, you are ${toString age}\\";\\n  \\n  result = greet { \\n    name = \\"Bob\\"; \\n    age = 35; \\n  }\\nin result\\n```\\n\\n### 6. Recursion and Recursive Sets\\n```nix\\nlet\\n  fibonacci = rec {\\n    fib0 = 0;\\n    fib1 = 1;\\n    fib2 = fib0 + fib1;\\n    fib3 = fib1 + fib2;\\n    # Continues recursively\\n  };\\nin fibonacci\\n```\\n\\n\\n### 7. Importing and Modules\\n```nix\\nlet\\n  # Importing another Nix file\\n  utils = import ./utils.nix;\\nin utils.someFunction\\n```\\n\\n### 8. List Comprehensions\\n```nix\\nlet\\n  numbers = [1 2 3 4 5];\\n  squared = map (x: x * x) numbers;\\nin squared  # [1 4 9 16 25]\\n```\\n\\n### 9. Conditional Expressions\\n```nix\\nlet\\n  checkAge = age: \\n    if age < 18 \\n    then \\"Minor\\" \\n    else \\"Adult\\";\\nin checkAge 20  # Returns \\"Adult\\"\\n```\\n\\n\\n##  Advanced Concepts\\nLazy Evaluation - Nix uses lazy evaluation, meaning expressions are only computed when needed:\\n```nix\\nlet\\n  expensiveComputation = \\n    builtins.trace \\"Computing...\\" \\n    (x: x * x);\\n  \\n  # Not evaluated until used\\n  result = expensiveComputation;\\nin result 42\\n```\\n\\nDerivations\\nThe core of Nix\'s package management:\\n\\n```nix\\nderivation {\\n  name = \\"example\\";\\n  builder = \\"/bin/sh\\";\\n  args = [ \\"-c\\" \\"echo hello > $out\\" ];\\n  system = \\"x86_64-linux\\";\\n}\\n```\\n\\n## Best Practices\\n\\n- Keep functions pure\\n- Avoid side effects\\n- Embrace immutability\\n- Use pattern matching\\n- Leverage lazy evaluation\\n\\n## Key Differences from Other Languages\\n\\n- No mutable state\\n- Functions are first-class citizens\\n- Strong emphasis on reproducibility\\n- Declarative package management\\n- Built-in support for functional programming paradigms\\n\\n## Conclusion\\nNix\'s functional programming syntax provides a unique approach to package management and system configuration, emphasizing reproducibility, purity, and declarative design."},{"id":"Nix Packages","metadata":{"permalink":"/notes/blog/Nix Packages","source":"@site/blog/2023-03-05/index.md","title":"Nix Pkg Manager","description":"Nix is a powerful, purely functional package manager that provides a unique approach to software deployment and system configuration.","date":"2023-03-05T00:00:00.000Z","tags":[{"inline":false,"label":"nix","permalink":"/notes/blog/tags/nix","description":"Nix OS and Functional Package Manager"}],"readingTime":4.225,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"Nix Packages","title":"Nix Pkg Manager","authors":["rakesh"],"tags":["nix"]},"unlisted":false,"prevItem":{"title":"Nix Introduction","permalink":"/notes/blog/Nix OS"},"nextItem":{"title":"Notion Introduction","permalink":"/notes/blog/Notion"}},"content":"Nix is a powerful, purely functional package manager that provides a unique approach to software deployment and system configuration. \\n\\n\x3c!-- truncate --\x3e\\n---\\n\\n\\n\\nUnlike traditional package managers, Nix focuses on:\\n\\n>-  Reproducibility  \\n>-  Atomic upgrades and rollbacks  \\n>-  Consistent development environments  \\n>-  Cross-platform compatibility  \\n\\n# Installation\\n\\n## MacOS\\n\\n```bash\\n# Install Nix using the official installer\\nsh <(curl -L https://nixos.org/nix/install)\\n\\n# For Apple Silicon (M1/M2) machines, use:\\nsh <(curl -L https://nixos.org/nix/install) --darwin-use-unencrypted-nix-store\\n```\\n\\n## Linux\\n```bash\\n# For most Linux distributions\\ncurl -L https://nixos.org/nix/install | sh\\n\\n# Multi-user installation (recommended)\\ncurl -L https://nixos.org/nix/install | sh -s -- --daemon\\n```\\n\\n\\n## Post Installation\\nSource the Nix profile. I prefer using zshrc.\\n\\n```bash\\n# Add to your shell configuration (.zshrc, .bashrc)\\nsource ~/.nix-profile/etc/profile.d/nix.sh\\n```\\n\\n## Nix Flakes\\nFlakes are a new feature in Nix that provide better reproducibility and composability. Create a `flake.nix` file in your project:\\n\\n```nix\\n{\\n  description = \\"My Dev Environment\\";\\n\\n  inputs = {\\n    nixpkgs.url = \\"github:NixOS/nixpkgs/nixos-unstable\\";\\n    home-manager = {\\n      url = \\"github:nix-community/home-manager\\";\\n      inputs.nixpkgs.follows = \\"nixpkgs\\";\\n    };\\n\\n    # Add inputs for your dotfile repositories\\n    nvim-config = {\\n      url = \\"github:yourusername/nvim-config\\";\\n      flake = false;  # Just fetch the source\\n    };\\n\\n    tmux-config = {\\n      url = \\"github:yourusername/tmux-config\\";\\n      flake = false;\\n    };\\n\\n    i3-config = {\\n      url = \\"github:yourusername/i3-config\\";\\n      flake = false;\\n    };\\n\\n  };\\n\\n  outputs = { self, nixpkgs, home-manager, nvim-config, tmux-config, i3-config }:\\n    let\\n      # This needs to be adjusted based on your system\\n      system = \\"x86_64-linux\\";  \\n      pkgs = nixpkgs.legacyPackages.${system};\\n    in {\\n      homeConfigurations.mydevenv = home-manager.lib.homeManagerConfiguration {\\n        inherit pkgs;\\n        modules = [ \\n          ./home.nix \\n          {\\n            # Pass the inputs to home-manager configuration\\n            home.file = {\\n              \\".config/nvim\\".source = nvim-config;\\n              \\".config/tmux\\".source = tmux-config;\\n              \\".config/i3\\".source = i3-config;\\n            };\\n          }\\n        ];\\n      };\\n    };\\n}\\n\\n```\\n\\n## Development Environment Setup\\n\\nCreate a `home.nix` file for comprehensive development environment configuration.  \\nHere\'s how my configuration looks like\\n\\n```nix\\n{ config, pkgs, ... }:\\n\\n{\\n  home = {\\n    username = \\"yourusername\\";\\n    homeDirectory = \\"/home/yourusername\\";\\n    stateVersion = \\"23.11\\";\\n\\n    packages = with pkgs; [\\n      # Development tools\\n      git\\n      neovim\\n      wezterm\\n      starship\\n      curl\\n      tmux\\n      rofi\\n      dunst\\n      feh\\n\\n      tmux\\n      \\n      # Language-specific tools\\n      python3\\n      rustup\\n      nodejs\\n      rust-analyzer\\n      \\n      # Utility tools\\n      ripgrep\\n      fd\\n      npm\\n      yarn\\n      lazygit\\n      tree\\n      tree-sitter\\n    ];\\n  };\\n\\n  # Neovim configuration from GitHub\\n  programs.neovim = {\\n    enable = true;\\n    extraConfig = \'\'\\n      lua << EOF\\n      -- Load configuration from GitHub repo\\n      vim.cmd(\'source ~/.config/nvim/init.lua\')\\n      EOF\\n    \'\';\\n  };\\n\\n  # Starship prompt configuration\\n  programs.starship = {\\n    enable = true;\\n    settings = {\\n      add_newline = false;\\n      character = {\\n        success_symbol = \\"[\u279c](bold green)\\";\\n        error_symbol = \\"[\u279c](bold red)\\";\\n      };\\n    };\\n  };\\n\\n  # WezTerm configuration\\n  programs.wezterm = {\\n    enable = true;\\n    extraConfig = \'\'\\n      return {\\n        font = wezterm.font(\\"JetBrains Mono\\"),\\n        color_scheme = \\"Dracula\\",\\n      }\\n    \'\';\\n  };\\n\\n  # Tmux configuration\\n    programs.tmux = {\\n        enable = true;\\n        shell = \\"${pkgs.zsh}/bin/zsh\\";\\n        terminal = \\"screen-256color\\";\\n        plugins = with pkgs.tmuxPlugins; [\\n            resurrect\\n            continuum\\n            # Add your preferred tmux plugins\\n        ];\\n        extraConfig = \'\'\\n            source-file ${../dotfiles/tmux/tmux.conf}\\n        \'\';\\n        };\\n}\\n\\n```\\n\\n\\n## Managing Dotfiles with Nix\\nTo manage dotfiles, create a separate repository with your configuration files and reference them in your Nix configuration:\\n\\n```nix\\n{\\n  home.file = {\\n    \\".config/nvim\\" = {\\n      source = fetchFromGitHub {\\n        owner = \\"yourusername\\";\\n        repo = \\"nvim-config\\";\\n        rev = \\"main\\";  # Or specific commit\\n        sha256 = \\"xxx\\";\\n      };\\n      recursive = true;\\n    };\\n  };\\n}\\n```\\n\\n## Applying the Configuration\\n\\n```nix\\n# Initialize and update flakes\\nnix flake update\\n\\n# Apply the home-manager configuration\\nhome-manager switch --flake .#mydevenv\\n```\\n\\n## Benefits of this Approach\\n\\n- Reproducibility: Exact same environment across machines\\n- Declarative Configuration: Define your entire setup in code\\n- Easy Rollbacks: Simple to revert to previous configurations\\n- Isolated Environments: No dependency conflicts\\n\\n## My Nix Configuration \\n\\n```markdown\\nnix-config/\\n\u2502\\n\u251c\u2500\u2500 flake.nix           # Main Nix flake configuration\\n\u251c\u2500\u2500 flake.lock          # Locked dependencies\\n\u2502\\n\u251c\u2500\u2500 home.nix            # Home Manager configuration\\n\u2502\\n\u251c\u2500\u2500 hosts/              # Machine-specific configurations\\n\u2502   \u251c\u2500\u2500 macbook/\\n\u2502   \u2502   \u2514\u2500\u2500 default.nix\\n\u2502   \u2514\u2500\u2500 workstation/\\n\u2502       \u2514\u2500\u2500 default.nix\\n\u2502\\n\u251c\u2500\u2500 modules/            # Reusable configuration modules\\n\u2502   \u251c\u2500\u2500 neovim/\\n\u2502   \u2502   \u251c\u2500\u2500 default.nix\\n\u2502   \u2502   \u2514\u2500\u2500 plugins.nix\\n\u2502   \u251c\u2500\u2500 starship.nix\\n\u2502   \u251c\u2500\u2500 wezterm.nix\\n\u2502   \u2514\u2500\u2500 git.nix\\n\u2502   \u251c\u2500\u2500 tmux/\\n\u2502   \u2502   \u251c\u2500\u2500 default.nix\\n\u2502   \u2502   \u2514\u2500\u2500 tmp.nix\\n\u2502   \u251c\u2500\u2500 i3/\\n\u2502   \u2502   \u2514\u2500\u2500 default.nix\\n\u2502   \\n\u2502   \\n\u251c\u2500\u2500 dotfiles/           # Actual dotfile configurations\\n\u2502   \u251c\u2500\u2500 nvim/\\n\u2502   \u2502   \u251c\u2500\u2500 init.lua\\n\u2502   \u2502   \u2514\u2500\u2500 lua/\\n\u2502   \u251c\u2500\u2500 starship.toml\\n\u2502   \u2514\u2500\u2500 wezterm.lua\\n\u2502   \u251c\u2500\u2500 tmux/\\n\u2502   \u2502   \u251c\u2500\u2500 tmux.conf   # Main tmux configuration\\n\u2502   \u2502   \u2514\u2500\u2500 plugins/   \\n\u2502   \u2502       \u251c\u2500\u2500 tpm/    # Tmux Plugin Manager (TPM) plugins\\n\u2502   \u2502       \u2514\u2500\u2500 other-plugins/\\n\u2502   \u251c\u2500\u2500 i3/\\n\u2502   \u2502   \u251c\u2500\u2500 config\\n\u2502   \\n\u2514\u2500\u2500 scripts/            # Utility scripts\\n    \u251c\u2500\u2500 setup.sh\\n    \u2514\u2500\u2500 update.sh\\n    \u2514\u2500\u2500 tmux-setup.sh  # Optional setup script for tmux\\n```\\n\\n## Troubleshooting\\n\\n- Ensure Nix daemon is running\\n- Check `~/.config/nixpkgs/config.nix` for global configurations\\n- Use nix-shell for temporary environments\\n\\n## Conclusion\\nNix provides a powerful, reproducible way to manage your development environment. By leveraging Nix flakes and home-manager, you can create consistent, version-controlled setups across multiple machines. If you want to know more about the nix functional package manager. Check out my quick guide [here](http://quibitai.in/notes/)\\n\\n## Additional Resources\\n\\n- Nix Official Documentation\\n- Home Manager GitHub\\n- Nix Flakes Guide\\n- [Nix Functional Package](http://quibitai.in/notes/)"},{"id":"Notion","metadata":{"permalink":"/notes/blog/Notion","source":"@site/blog/2021-08-26/notion-productivity-guide.md","title":"Notion Introduction","description":"---","date":"2021-08-26T00:00:00.000Z","tags":[{"inline":true,"label":"notion","permalink":"/notes/blog/tags/notion"}],"readingTime":2.35,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"Notion","title":"Notion Introduction","authors":["rakesh"],"tags":["notion"]},"unlisted":false,"prevItem":{"title":"Nix Pkg Manager","permalink":"/notes/blog/Nix Packages"},"nextItem":{"title":"Introduction to Docusaurus","permalink":"/notes/blog/Intro to Docusaurus"}},"content":"\x3c!-- truncate --\x3e\\n\\n---\\n\\n## The Professional\'s Productivity Challenge\\n\\nAs a full-time software engineer balancing family life, personal projects, and professional responsibilities, I\'ve spent years searching for the ultimate productivity solution. In 2017, I discovered Notion \u2013 a game-changing platform that revolutionized how I manage my life, track projects, and transform ideas into actionable plans.\\n\\n## Notion: A Powerful Productivity Ecosystem\\n\\nNotion is far more than a simple productivity app. It\'s a versatile workspace that adapts to your unique workflow, combining note-taking, project management, databases, and collaboration tools into a single, customizable platform.\\n\\n## Deep Dive: Notion\'s Powerful Features\\n\\n### 1. Databases: The Backbone of Intelligent Organization\\n\\nNotion\'s databases are game-changers for personal productivity:\\n\\n#### Relational Databases\\n- **Link information across different pages**\\n- **Create complex, interconnected knowledge systems**\\n- **Track relationships between projects, goals, and tasks**\\n\\n#### Multiple View Types\\n- Table view for detailed tracking\\n- Kanban boards for visual progress\\n- Calendar view for time-based organization\\n- Gallery view for visual projects\\n\\n### 2. Dynamic Pages and Nested Information\\n\\n- Hierarchical page structure\\n- Unlimited nesting of pages\\n- Seamless information organization\\n- Context-rich documentation\\n\\n### 3. Powerful Formulas and Rollups\\n\\n- Spreadsheet-like calculations\\n- Aggregate data across databases\\n- Create dynamic, self-updating dashboards\\n- Automate tracking and reporting\\n\\n### 4. Linked Databases: Breaking Information Silos\\n\\n- Reference same database in multiple views\\n- Maintain data consistency\\n- Create project-specific perspectives\\n- Reduce redundant data entry\\n\\n### 5. Rich Embedding Capabilities\\n\\n- Embed images, videos, and external content\\n- Integrate with multiple platforms\\n- Create comprehensive, multimedia workspaces\\n- Centralize information from various sources\\n\\n## My Notion Ecosystem: Practical Implementation\\n\\n### Specialized Trackers\\n\\n1. **Travel Tracker**\\n   - Trip logging\\n   - Expense management\\n   - Future planning\\n   - Memory collection\\n\\n2. **Habit Tracker**\\n   - Daily/weekly habit monitoring\\n   - Progress visualization\\n   - Accountability mechanisms\\n\\n3. **Meal Planner**\\n   - Recipe management\\n   - Grocery lists\\n   - Nutrition tracking\\n\\n4. **Project Management**\\n   - Milestone tracking\\n   - Resource allocation\\n   - Progress monitoring\\n\\n## Integration Strategy\\n\\nNotion works best as part of a broader productivity ecosystem:\\n\\n- Apple Notes for quick captures\\n- Apple Reminders for time-sensitive tasks\\n- Apple Calendar for scheduling\\n\\n## Weekly Review Process\\n\\n- Review captured ideas\\n- Update project trackers\\n- Adjust goals\\n- Reflect on achievements\\n\\n## Key Advantages\\n\\n- **Flexibility**: Fully customizable workflows\\n- **Integration**: Connects life and work domains\\n- **Visualization**: Intuitive data representation\\n- **Accessibility**: Cross-device availability\\n- **Collaboration**: Easy sharing and teamwork\\n\\n## Technical Insights for Engineers\\n\\nThink of Notion like code:\\n- Templates = Modular code\\n- Databases = Structured data models\\n- Views = Different data rendering methods\\n\\n## Getting Started: Your Notion Journey\\n\\n1. Download Notion\\n2. Start with basic templates\\n3. Experiment gradually\\n4. Develop your unique system\\n\\nThe perfect productivity system evolves with you.\\n\\n*Disclaimer: Personal experience. Results may vary.*"},{"id":"Intro to Docusaurus","metadata":{"permalink":"/notes/blog/Intro to Docusaurus","source":"@site/blog/2017-03-05/Docusourus.md","title":"Introduction to Docusaurus","description":"Docusaurus is an open-source static site generator specifically designed for creating documentation websites. Developed by Facebook (Meta), it provides a powerful and intuitive platform for building, deploying, and maintaining documentation sites with ease.","date":"2017-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"docusaurus","permalink":"/notes/blog/tags/docusaurus"}],"readingTime":2.3,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"Intro to Docusaurus","title":"Introduction to Docusaurus","authors":["rakesh"],"tags":["docusaurus"]},"unlisted":false,"prevItem":{"title":"Notion Introduction","permalink":"/notes/blog/Notion"},"nextItem":{"title":"Welcome","permalink":"/notes/blog/welcome"}},"content":"# Introduction\\n\\nDocusaurus is an open-source static site generator specifically designed for creating documentation websites. Developed by Facebook (Meta), it provides a powerful and intuitive platform for building, deploying, and maintaining documentation sites with ease.\\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\n# What is Docusaurus?\\nDocusaurus is a modern static website generator that helps developers and teams create, build, and publish documentation websites quickly and efficiently. It leverages React and provides a clean, customizable interface with built-in features that make documentation management seamless.\\n\\n# Key Features and Advantages\\n### Easy Setup and Configuration\\n\\n- Quick initialization with minimal configuration\\n- Built-in best practices for documentation sites\\n- Supports both single and multi-version documentation\\n\\n### Powerful Customization\\n\\n- Fully customizable React-based themes\\n- Support for custom layouts and components\\n- Flexible styling options\\n\\n### Developer-Friendly Features\\n\\n- Markdown support with enhanced features\\n-  Built-in search functionality\\n- Internationalization (i18n) support\\n- Versioning for documentation\\n\\n### Performance and SEO\\n- Static site generation for fast loading\\n- Optimized for search engines\\n- Responsive design out of the box\\n\\n# Getting Started with Docusaurus\\n## Installation\\nYou can create a new Docusaurus project using either `yarn` or `npm`:\\n\\n### Using Yarn\\n\\n```bash\\nyarn create docusaurus my-website classic\\n```\\n\\n### Using npm\\n```bash\\nnpx create-docusaurus@latest my-website classic\\n```\\n\\n\\n### project Structure\\n```markdown\\nmy-website/\\n\u2502\\n\u251c\u2500\u2500 docs/                  # Markdown documentation files\\n\u251c\u2500\u2500 blog/                  # Blog posts\\n\u251c\u2500\u2500 src/                   # Custom React components\\n\u2502   \u251c\u2500\u2500 components/\\n\u2502   \u2514\u2500\u2500 pages/\\n\u251c\u2500\u2500 static/                # Static assets\\n\u251c\u2500\u2500 docusaurus.config.js   # Configuration file\\n\u251c\u2500\u2500 sidebars.js            # Sidebar configuration\\n\u2514\u2500\u2500 package.json           # Project dependencies\\n\\n```\\n\\n\\n\\n### Confguration\\n\\nThe primary configuration file is `docusaurus.config.js`. Here\'s a basic example:\\n\\n```js\\nmodule.exports = {\\n  title: \'My Documentation Site\',\\n  tagline: \'Awesome Documentation\',\\n  url: \'https://your-website.com\',\\n  baseUrl: \'/\',\\n  theme: \'@docusaurus/theme-classic\',\\n  presets: [\\n    [\\n      \'@docusaurus/preset-classic\',\\n      {\\n        docs: {\\n          sidebarPath: require.resolve(\'./sidebars.js\'),\\n        },\\n        blog: {\\n          showReadingTime: true,\\n        },\\n        theme: {\\n          customCss: require.resolve(\'./src/css/custom.css\'),\\n        },\\n      },\\n    ],\\n  ],\\n};\\n```\\n\\n### Bulding the site\\n\\nStart the local development server\\n```bash\\nyarn start\\n# or\\nnpm run start\\n```\\n\\n### Publishing the site\\n\\nDocusaurus provides built-in support for GitHub Pages:\\n\\n```bash\\nGIT_USER=<GITHUB_USERNAME> yarn deploy\\n# or\\nUSE_SSH=true yarn deploy\\n```\\n\\n# Best Practices\\n\\n1. Keep documentation organized\\n2. Use clear, concise language\\n3. Implement versioning for stable releases\\n4. Utilize built-in search and navigation features\\n5. Regularly update and maintain documentation\\n\\n\\n# Markdown\\n```markdown\\n---\\nid: getting-started\\ntitle: Getting Started\\nsidebar_label: Introduction\\n---\\n\\n# Welcome to Our Documentation\\n\\nThis is a sample documentation page with some information.\\n\\n## Key Concepts\\n\\n- Simple markdown writing\\n- Easy configuration\\n- Powerful features\\n```\\n\\n# Conclusion\\nDocusaurus provides a modern, efficient solution for creating and maintaining documentation websites. Its combination of ease of use, powerful features, and flexibility makes it an excellent choice for projects of all sizes."},{"id":"welcome","metadata":{"permalink":"/notes/blog/welcome","source":"@site/blog/2017-03-05/index.md","title":"Welcome","description":"Welcome to my blog site which is hosted on QuBiTAi using Docusaurus.","date":"2017-03-05T00:00:00.000Z","tags":[{"inline":false,"label":"Hello","permalink":"/notes/blog/tags/hello","description":"Hello tag description"}],"readingTime":0.08,"hasTruncateMarker":true,"authors":[{"name":"rakesh","title":"Sr. Engineering Manager","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"welcome","title":"Welcome","authors":["rakesh"],"tags":["hello"]},"unlisted":false,"prevItem":{"title":"Introduction to Docusaurus","permalink":"/notes/blog/Intro to Docusaurus"}},"content":"Welcome to my blog site which is hosted on [QuBiTAi](https://qubitai.in) using Docusaurus.\\n\\n\\n\x3c!-- truncate --\x3e\\n\\n---"}]}}')}}]);