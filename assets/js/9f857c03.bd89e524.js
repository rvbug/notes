"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[9046],{3202:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>c});var i=t(9256),s=t(4848),o=t(8453);const r={slug:"Tokenizer",title:"Input Block - Tokenization",description:"What is Tokenizer",authors:["rakesh"],tags:["ML","ML Research"]},a="Introduction",d={authorsImageUrls:[void 0]},c=[{value:"Types of Tokenizer",id:"types-of-tokenizer",level:2},{value:"Byte Pair Encoding (BPE)",id:"byte-pair-encoding-bpe",level:2},{value:"Modified BPE",id:"modified-bpe",level:2},{value:"BPE Library",id:"bpe-library",level:2},{value:"Rust Code",id:"rust-code",level:2},{value:"Python Code",id:"python-code",level:2}];function l(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",br:"br",code:"code",em:"em",h1:"h1",h2:"h2",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"In building LLMs, the first block is called input block. In this step, input text passed through the Tokenizer to create a tokenized text. This Token IDs are then passed through Embedding Layer and positional encoding is added before sending it to the transformer block."}),"\n",(0,s.jsxs)(n.p,{children:["I will talk about positional encoding later in the series. For now think of ids (",(0,s.jsx)(n.code,{children:"numerical values"}),") been sent to transformer block."]}),"\n",(0,s.jsx)(n.p,{children:"Here's the architecture of LLMs showing all three blocks."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"LLM Achitecture",src:t(9812).A+"",width:"2623",height:"1316"})}),"\n",(0,s.jsx)(n.h1,{id:"tokenizer",children:"Tokenizer"}),"\n",(0,s.jsx)(n.p,{children:"Input to the transformers are numbers not words, so we need to change the raw text to tokens (usually words, subwords or even characters) and convert it to token ids. This is called Tokenization."}),"\n",(0,s.jsx)(n.p,{children:"The token ids are then passed to a neural network which transforms to continuous vector representations in a very high dimensional space.\nThese dense vector representations capture semantic relationships between tokens, allowing machines to process language more effectively."}),"\n",(0,s.jsxs)(n.p,{children:["Here's how Embedding Matrix are typically created.",(0,s.jsx)(n.br,{}),"\n","If you follow the text ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"This"})})," - If token id is 2 , the embedding matrix could be ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"[0.2, 0.8....]"})})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Embedding Matrix",src:t(1201).A+"",width:"2380",height:"966"})}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"Embedding Matrix"})," - A lookup table where each row corresponds to a specific token in the vocabulary and contains that token's learned vector representation."]})}),"\n",(0,s.jsx)(n.p,{children:"Here's the summary -"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Tokenization",src:t(9664).A+"",width:"1720",height:"644"})}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.p,{children:"Typically these Embedding size (rows) are between 768 to roughly 16K for each column inputs text."})}),"\n",(0,s.jsx)(n.h2,{id:"types-of-tokenizer",children:"Types of Tokenizer"}),"\n",(0,s.jsxs)(n.p,{children:["To tokenize the input text, there are three types which are available. ",(0,s.jsx)(n.code,{children:"Sub-Word"})," based tokenizers are what is typically used in LLMs.",(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)(n.img,{alt:"alt text",src:t(2962).A+"",width:"2398",height:"1131"})]}),"\n",(0,s.jsx)(n.p,{children:"Byte Pair Encoding (BPE) is one such library."}),"\n",(0,s.jsx)(n.h2,{id:"byte-pair-encoding-bpe",children:"Byte Pair Encoding (BPE)"}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Orginally BPE was developed as a compression algorithm."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Checkout more details and the example used from ",(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Byte_pair_encoding",children:(0,s.jsx)(n.strong,{children:"Wiki"})})]}),"\n"]}),"\n"]})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"alt text",src:t(8698).A+"",width:"1902",height:"1414"})}),"\n",(0,s.jsx)(n.h2,{id:"modified-bpe",children:"Modified BPE"}),"\n",(0,s.jsx)(n.admonition,{type:"tip",children:(0,s.jsxs)(n.p,{children:["In LLM, we use something called ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"Modified Byte Pair Encoding"})}),". Used for encoding plain text to tokens"]})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"alt text",src:t(9361).A+"",width:"1949",height:"965"})}),"\n",(0,s.jsx)(n.h2,{id:"bpe-library",children:"BPE Library"}),"\n",(0,s.jsxs)(n.p,{children:["In Rust, you can use ",(0,s.jsx)(n.a,{href:"https://docs.rs/crate/tiktoken-rs/latest",children:"Tiktoken-rs"})," for ",(0,s.jsx)(n.em,{children:(0,s.jsx)(n.code,{children:"BPE"})})]}),"\n",(0,s.jsxs)(n.admonition,{type:"info",children:[(0,s.jsx)(n.p,{children:"Equivalent Python library is available"}),(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://pypi.org/project/tiktoken/",children:"Tiktoken"})}),"\n"]})]}),"\n",(0,s.jsx)(n.h2,{id:"rust-code",children:"Rust Code"}),"\n",(0,s.jsxs)(n.p,{children:["Below is the rust code using ",(0,s.jsx)(n.code,{children:"Tiktoken-rs"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:'use tiktoken_rs::o200k_base;\n\nfn main() {\n    let bpe = o200k_base().unwrap();\n    let token: Vec<u32> = bpe.encode_with_special_tokens("This is multi line sentence for BPE with rust and a sentence   with spaces");\n\n    println!("Token: {:?}", token);\n    println!("Decoding the token {:?}", bpe.decode(token));\n\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:"Output of this program is as shown below"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'$> cargo run\n\n## OUTPUT\n\n#Token: [2500, 382, 12151, 2543, 21872, 395, 418, 3111, 483, 20294, 326, 261, 21872, 256, 483, 18608]\n#Decoding the token Ok("This is multi line sentence for BPE with rust and a sentence   with spaces")\n\n'})}),"\n",(0,s.jsx)(n.h2,{id:"python-code",children:"Python Code"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import tiktoken\ntokenizer = tiktoken.get_encoding("o200k_base")\n\ntext_data = ( "Ecode using BPE via Python")\n\nencoder_output = tokenizer.encode(text_data, allowed_special={"<|END|>"})\nprint(encoder_output)\n\n\ndecoder_output = tokenizer.decode(encoder_output)\n\nprint(decoder_output)\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"$> python3 bpe_example.py\n\n## OUTPUT\n\n# [36, 3056, 2360, 418, 3111, 4493, 26534]\n# Ecode using BPE via Python\n\n"})})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},8698:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/Bpe-87b63a28e01d14a66bd9be25d22fd45f.png"},1201:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/EmbeddingMatrix-c9622976143eed1677236f15f3ff77c1.png"},9812:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/LLMarch-9f6779d06fea40ae08058d069aa55ab1.png"},9361:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/ModifiedBPE-ed3fd7fb8aeeb937be137de1b7a8b469.png"},9664:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/tokenization-84b99bbcfbde166c43fe4ffe46cc2c9a.png"},2962:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/tokenizer-ae00bc46580d95c23e2f3998d6a35dfb.png"},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var i=t(6540);const s={},o=i.createContext(s);function r(e){const n=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(o.Provider,{value:n},e.children)}},9256:e=>{e.exports=JSON.parse('{"permalink":"/notes/blog/Tokenizer","source":"@site/blog/2025-03-01/LLMs/4.Tokenization.md","title":"Input Block - Tokenization","description":"What is Tokenizer","date":"2025-03-01T00:00:00.000Z","tags":[{"inline":false,"label":"ML","permalink":"/notes/blog/tags/ML","description":"Machine Learning"},{"inline":false,"label":"ML Reserach","permalink":"/notes/blog/tags/ML-Research","description":"Machine Learning Research"}],"readingTime":2.43,"hasTruncateMarker":true,"authors":[{"name":"Rakesh","title":"C | Rust | Quantum Gravity | LLM Researcher","description":"I am a researcher in the field of Quantum Gravity and LLMs. I have a strong interest in C and Rust programming languages.\\n","url":"https://qubitai.in","page":{"permalink":"/notes/blog/authors/rakesh"},"socials":{"w":"https://qubitai.in","github":"https://github.com/rvbug"},"imageURL":"https://avatars.githubusercontent.com/u/10928536?v=4","key":"rakesh"}],"frontMatter":{"slug":"Tokenizer","title":"Input Block - Tokenization","description":"What is Tokenizer","authors":["rakesh"],"tags":["ML","ML Research"]},"unlisted":false,"prevItem":{"title":"PTX Optimization","permalink":"/notes/blog/DeepSeekPTX"},"nextItem":{"title":"Transformer Block - Multihead Attention","permalink":"/notes/blog/transformers"}}')}}]);