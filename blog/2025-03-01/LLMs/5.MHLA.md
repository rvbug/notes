---
slug: transformers
title: Transformer Block - Attention
description: Road to MHLA
image: "img/LLMWorkflow.png"
authors: [rakesh]
tags: [ML, LLMs, ML Research]
---


# Introduction

In my previous blog post, I introduced Deepseek LLM's innovative parallel thread execution mechanism and how DeepSeek might have use it for optimizing their GPUs. This article is about another Deepseek's ground breaking innovation **Multihead Latent Attention (MHLA)** . But before we get to the concept of MLHA, we need to know what are Attention and Multihead Attentions are.

<!-- truncate -->


# Multihead Latent Attention Roadmap

Here's is a roadmap to understand MHLA.

![MHLA](<img/MHLA roadmap.png>)

Attention Mechanism was introduced to solve the problem which Encoder-Decoder architecture had.

# Encoder-Decoder  

The limitation of RNN (**`Recurrent Neural Network`**) and LSTM (**`Long Short Term Memory`**) is as shown below. 

- The encoder processes input tokens sequentially through LSTM/RNN cells and the final hidden state (h3) is passed to the decoder  
- So all the information from previous hidden state (h0), (h1), (h2) and (h4 is compressed into a single vector known as contect vector  
- That means to decode, s1 will have access only to the final hidden state  
- So the conext will be lost if longer sentences are passed to the encoder   

![Encoder-Decoder](img/Encoder-decoder.png)


# Drawbacks

The traditional encoder-decoder architecture used before attention mechanisms suffered from several key limitations:

**`Information Bottleneck`**: The entire input sequence had to be compressed into a single fixed-length context vector, regardless of the input sequence length.  
**`Long Range Dependencies`**: As sequence length increased, the model struggled to maintain relationships between positions.  
**`Vanishing Information`**: Information from the beginning of long sequences would "fade" by the time it reached the decoder.  

These limitations were particularly problematic for machine translation tasks where sentences in different languages often have different structures and word orders.

# Attention 

The concept of `Attention` was introduced to solve the above challenges in a landmark paper *`Neural Machine Translation by Jointly Learning to Align and Translate`* by Bahdanau, Cho, and Bengio in 2014. It revolutionized the field of sequence processing by allowing neural networks to focus on specific parts of the input when generating outputs. 

![Encoder-Decoder-Attention](img/Encoder-decoder-attention.png)

:::info
* Decoder (s1) has now access to every hidden state in the above image and has context of every hidden state in the encoder stage.
* See the attention weights giving the importance for each hidden state
:::

Another way of imagining the Attention Mechanism is as below. The attention block in between has the context information of inputs and much more richer containing semantic meaning.

![Attention](img/Attention.png)

:::info
The stronger colored bands in the attention visualization represent higher attention weights
:::

The Bahdanau attention mechanism allowed  decoder to "look back" at the entire sequence of encoder hidden states when generating each output token. Rather than relying solely on a fixed context vector, the decoder could dynamically focus on relevant parts of the input sequence.



# Self Attention 

To understand how everything fits together, take a look at the entire transformer archiecture again  .  

![LLM Architecture](img/LLMarch.png)

To keep it simple, here's how the diagram will look like helping you focus on `Self Attention` mechanism


![alt text](img/Ip-2-context.png)

Detailed workflow on how contect vector is created from inout text along with the dimensions is as below


![alt text](img/Ip-2-context-details.png)




# Attention & Self Attention



# Causal Attention

